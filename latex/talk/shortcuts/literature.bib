@Article{Jin2020,
  author   = {Pengzhan Jin and Zhen Zhang and Aiqing Zhu and Yifa Tang and George Em Karniadakis},
  journal  = {Neural Networks},
  title    = "{SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems}",
  year     = {2020},
  issn     = {0893-6080},
  pages    = {166 - 179},
  volume   = {132},
  abstract = {We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions. We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models, and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.017},
  keywords = {Deep learning, Physics-informed, Dynamical systems, Hamiltonian systems, Symplectic maps, Symplectic integrators},
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608020303063},
}

@Book{hairer2006,
  author    = {Ernst Hairer and Christian Lubich and Gerhard Wanner},
  publisher = {Springer-Verlag},
  title     = {Geometric Numerical Integration},
  year      = {2006},
  doi       = {10.1007/3-540-30666-8},
  url       = {https://doi.org/10.1007%2F3-540-30666-8},
}

@InProceedings{batchnorm-ioffe15,
  author    = {Sergey Ioffe and Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Francis Bach and David Blei},
  month     = {07--09 Jul},
  pages     = {448--456},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  pdf       = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url       = {http://proceedings.mlr.press/v37/ioffe15.html},
}

@InProceedings{Santurkar2018,
 author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {2483--2493},
 publisher = {Curran Associates, Inc.},
 title = {How Does Batch Normalization Help Optimization?},
 url = {https://proceedings.neurips.cc/paper/2018/file/905056c1ac1dad141560467e0a99e1cf-Paper.pdf},
 volume = {31},
 year = {2018}
}