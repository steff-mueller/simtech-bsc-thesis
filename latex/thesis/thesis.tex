\documentclass[twoside,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,bindingoffset=0.5cm,inner=2.5cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[a4paper,bindingoffset=1cm,inner=2cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry} %,showframe
\usepackage{helvet}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\sfdefault}
% \usepackage[german,ngerman]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amstext,amssymb,bm}
\usepackage{mathtools}
\usepackage{xcolor,color}
\usepackage{pifont}
\usepackage{array}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algpseudocode,algorithm}
\usepackage{placeins}
%%% Title page
\usepackage{common/titlePageST}
%%% Appendix in TOC
\usepackage[toc,page]{appendix}
%%% Setstrech on title page
\usepackage{setspace}
%%% Tilde in URL in literature
\usepackage{url}
%%% multiple rows
\usepackage{multirow}
%%% fancy column and row seperators
\usepackage{hhline}
%%% custom items in enumerate and itemize
\usepackage{enumitem}
%%% custom format for algorithm comments
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em // #1}

\clubpenalty=5000
\widowpenalty=5000

\setlength{\emergencystretch}{2cm}
%----------------------------------------------------------------------------------------
%	abbreviation includes
%----------------------------------------------------------------------------------------
\input{common/default_abbrev}
\input{common/specific_abbrev}
%%% Clever refing
\usepackage{hyperref}
\usepackage[capitalise,noabbrev]{cleveref}

%----------------------------------------------------------------------------------------
%	References with cleveref
%----------------------------------------------------------------------------------------
\crefformat{equation}{(#2#1#3)}

%----------------------------------------------------------------------------------------
%	Theorem environments
%----------------------------------------------------------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

%----------------------------------------------------------------------------------------
%	fancyhdr
%----------------------------------------------------------------------------------------
\usepackage{fancyhdr}

%----------------------------------------------------------------------------------------
%	pgfplots
%----------------------------------------------------------------------------------------
\usepackage{pgfplots,pgfplotstable,booktabs}
\pgfplotsset{compat=1.16}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{external}
\tikzexternalize
\tikzsetexternalprefix{figures_cache/}

\pgfplotsset{
	every axis/.append style={
		ymajorgrids,
		grid style={dashed,lightgray,semithick},
	}
}

\pgfplotsset{
  	every axis plot/.append style={line width=0.7pt,},
}

% Style to select only points from #1 to #2 (inclusive)
\pgfplotsset{select coords between index/.style 2 args={
    x filter/.code={
        \ifnum\coordindex<#1\def\pgfmathresult{}\fi
        \ifnum\coordindex>#2\def\pgfmathresult{}\fi
    }
}}

\pgfplotstableset{
	every head row/.style={ before row=\toprule,after row=\midrule}, 
	every last row/.style={ after row=\bottomrule}
}
%----------------------------------------------------------------------------------------

\makeatletter
\newcommand{\theauthor}{Steffen Müller} %
\makeatother

\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}
\fancyfoot[OR,EL]{\thepage} % outer
\fancyhead[OL,ER]{\theauthor} % inner 
\fancyhead[OR,EL]{IANS -- Institute of Applied Analysis and Numerical Simulation} % outer
\fancyfoot[C]{}

% \headsep=4mm
% \footskip=4mm
\parindent=0mm
\parskip=6pt
% \renewcommand{\footskip}{3pt}

%----------------------------------------------------------------------------------------
%	Something
%----------------------------------------------------------------------------------------
\usepackage{textpos}
\setlength{\TPHorizModule}{1mm}%
\setlength{\TPVertModule}{1mm}%
% \headsep=5mm
% \footskip=5mm
\pagestyle{fancy}
\newcommand{\articleheading}[3]{
{\large #1}\\[3mm]
{\Large\bf #2}\\[3mm]
{\large #3}
}

%----------------------------------------------------------------------------------------
%	Start Document
%----------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
%----------------------------------------------------------------------------------------
%
% TITLE PAGE
%
%----------------------------------------------------------------------------------------
%------------------------------------------
\begin{titlePageST}
%------------------------------------------
\makeLogo%
{-10pt}{
\includegraphics[width=0.7\textwidth]{figures/logos/simtech.pdf}
}%
{0pt}{
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/logos/ians.pdf}
	\end{center}}%
{0pt}{
\begin{flushright}
	\vspace{-10pt}
	\includegraphics[width=0.9\textwidth]{figures/logos/unistuttgart_logo_englisch_cmyk.eps}
\end{flushright}}%
\vspace{35pt}%
%------------------------------------------
\makeHeader%
[Research Group: Numerical Mathematics] %
{Institute of Applied Analysis and Numerical Simulation} %
\vspace{50pt}%
%------------------------------------------
\makeTitle%
{Simulation Technology Degree Course} %
{Bachelor Thesis} %
\vspace{80pt}%
%------------------------------------------
\makeTitleThesis%
{Symplectic Neural Networks}
\vspace{90pt}%
%------------------------------------------
\begin{supervisorST}{3}%
\addSuper%
{First Reviewer}%
{Prof. Dr. B. Haasdonk}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\addSuper%
{Second Reviewer}%
{Prof. Dr. D. Pflüger}%
{Institute of Parallel and Distributed\\[-0.2cm]
Systems (Scientific Computing)}%
\addSuper%
{Advisor}%
{Patrick Buchfink, M.Sc.}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\end{supervisorST}%
\vspace{80pt}%
%------------------------------------------
\begin{authorST}{Submitted by}%
\addAuthorInfo{Author}{Steffen Müller}
\addAuthorInfo{Student ID}{3260643}
\addAuthorInfo{SimTech ID}{119} %
\addAuthorInfo{Submission Date}{01.12.2020} %FILLIN
\end{authorST}%
%------------------------------------------
\end{titlePageST}
%----------------------------------------------------------------------------------------
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ABSTRACT
%
%----------------------------------------------------------------------------------------
\section*{Abstract}
We use physics-based neural networks to learn and predict the dynamics 
of Hamiltonian systems from data.
We build on previous work on symplectic networks (SympNets) for
identifying Hamiltonian systems from data. After recapitulating the 
architecture of SympNets, we extend SympNets by 
proposing symplectic variants of batch normalization and convolution. We achieve 
to improve the baseline performance of SympNets via batch normalization. 
We empirically verify that SympNets with batch normalization are more robust
with respect to faster learning rates.
With convolution,
we make SympNets suitable for high-dimensional Hamiltonian ODE systems arising from
semi-discretization of Hamiltonian PDEs. In particular, we successfully 
extrapolate trajectories of the the semi-linear wave equation with SympNets in our 
numerical experiments. We show how SympNets relate to two conventional geometric 
integrators commonly used for time integration of Hamiltonian systems.
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ACKNOWLEDGEMENTS
%
%----------------------------------------------------------------------------------------
\section*{Acknowledgements}
I want to thank Patrick Buchfink for reviewing this thesis and for the numerous
fruitful and enjoyable discussions.
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% TOC
%
%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\newpage\thispagestyle{plain}\null
%----------------------------------------------------------------------------------------
%
% Begin with document content
%
%----------------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic} 
%----------------------------------------------------------------------------------------
%
% Introduction
%
%----------------------------------------------------------------------------------------
\section{Introduction}

Nowadays, neural networks are successfully used in a lot of different contexts and fields, 
for example in object detection, object classification, speech recognizition, robotics,
chemistry or medicine \cite{Goodfellow2016}.
However, most of the time the neural networks, or in general the machine learning models, 
rely on a large amount of training data to achieve the desired performance. Prior structural
knowledge is not used beneficially. In the recent field of physics-based machine
learning, the objective is to embed prior knowledge from physics into the machine 
learning model, to for example reduce the dependency on a large amount of training data, 
or to improve the quality of the results.
In the context of simulations, machine learning models can act as surrogate models,
where the training data is generated from the original simulation. Often, these 
simulations are computationally expensive to execute and therefore one can only 
generate a limited amount of training data. However, most of the time there exists 
prior knowledge about the simulated system, which could be used beneficially.

In this thesis, we focus on predicting the dynamics of a special class of dynamical
systems, so-called Hamiltonian (ODE) systems, with neural networks. The neural networks
are fed from example data of the dynamical behavior of the Hamiltonian system,
possibly from real-word measurements or simulation results.
Generally, mechanical systems resulting from physical principles are
Hamiltonian (ODE) systems as long as no dissipative elements are introduced 
\cite{leimkuhler_reich_2005}. Such conservative systems arise in a lot
of applications, for example rigid body systems, molecular systems in chemistry or
astronomicial systems \cite{leimkuhler_reich_2005}. Furthermore, high-dimensional
Hamiltonian ODE systems arise from so-called Hamiltonian PDEs upon discretization
in space \cite{leimkuhler_reich_2005}. Canonical Hamiltonian systems are fully described
by a so-called Hamiltonian, which can be interpeted as the total energy of the system.
Poincaré has shown in 1899 that these Hamiltonian
systems have an exceptional property: The flow map, which operates on the so-called phase space 
and which advances the state of a Hamiltonian system by a certain time $t$, is symplectic. 
Informally, symplecticity is a stronger occurence of volume and orientation preservation
\cite{leimkuhler_reich_2005}. This exceptional property of the flow map led to the development of 
so-called geometric numerical integrators, which are symplectic themselves. It turns out 
that these geometric integrators offer much better solutions over long time than their
non-symplectic counterparts \cite{hairer2006}. We want to learn such a flow map with 
a neural network. Motivated by the good results of the geometric integrators,
it suggests itself to intrinsically embed the symplectic property into the structure
of the neural networks.

\citet{Deco1995} proposed a neural network architecture for the related
problem of volume-preservation already in \citeyear{Deco1995}. 
There have already been several neural network-based proposals how to identify 
Hamiltonian systems from data.
For example, \citet{Greydanus2019} propose a neural network to learn the Hamiltonian
of the Hamiltonian system. After the Hamiltonian is learned, conventional numerical
integrators can be used to predict the dynamics of the Hamiltonian system.
Their approach involves computing the gradient of the neural network during training
and during prediction with the numerical integrator. In addition, their approach 
requires time derivatives for the training data.
Opposed to this indirect method, \citet{Jin2020} instead learn the flow, 
which advances the state of a Hamiltonian system by a certain time $t$ based on a
previous (initial) state, directly with a symplectic neural network (SympNet). 
SympNets are constructed in a special way such that they are guaranteed to be symplectic.
The approach by \citeauthor{Jin2020} does not require computing the gradient of the neural
network during training and prediction, making training and 
prediction more efficient. Furthermore, the networks proposed by \citeauthor{Jin2020}
only need examples from phase space instead of time derivatives for the training data.

In this thesis, we extend the SympNets by introducing
the concepts of batch normalization and convolution to SympNets,
while maintaining their symplectic property. We achieve to
improve the baseline performance of SympNets by proposing a symplectic
version of batch normalization. 
The SympNets as proposed by \citeauthor{Jin2020} are not suitable for high-dimensional 
Hamiltonian, as the parameter space would explode for
such high-dimensional systems. By bringing the concept of convolution
to SympNets, we make SympNets applicaple for high-dimensional
Hamiltonian systems arising from Hamiltonian PDEs.

\subsection{Outline}

In \cref{sec_problem_setup}, we introduce the necessary theory for Hamiltonian systems
which is required in the following sections. In particular, we give the exact
definition of a symplectic map.
In \cref{sec_architecture_of_sympnets}, after we briefly introduce basic neural network terminology,
we recapitulate the architecture of SympNets (symplectic networks)
as proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
\citeauthor{Jin2020} have also provided universal approximation theorems for SympNets.
We repeat these statements in \cref{sec_unv_approx_theorem} and briefly discuss 
reasonable nonlinear activation functions for SympNets.
In \cref{sec_conv_and_norm_for_sympnets}, we extend SympNets by introducing
convolution and batch normalization to SympNets,
while still maintaining their symplectic property.
In \cref{sec_relation_to_geometric_integrators}, we build a connection 
between SympNets and two conventional geometric integrators.
In \cref{sec_numerical_experiments}, we compare the performance of SympNets 
with and without normalization. In addition, we compare their performance to neural 
networks which do not intrinsically preserve the symplectic structure. 
Furthermore, we apply convolutional SympNets to a high-dimensional Hamiltonian system 
arising from the semi-linear wave equation.
In \cref{sec_resume}, we summarize our results and give an outlook on possible further 
topics to be explored.

In accordance with the examination regulations from 22th July 2016 of the bachelor's program 
\emph{Simulation Technology} of the University of Stuttgart, 
\cref{sec_problem_setup,sec_architecture_of_sympnets,sec_unv_approx_theorem,sec_relation_to_geometric_integrators} 
belong to the propaedeuticum. The remaining sections belong to the bachelor thesis.

\subsection{Notation}

Given some entries $v_1, \dots, v_n \in \R$, we construct a vector $v \in \R^n$ with
\begin{equation*}
	v = \begin{pmatrix}
		v_i
	\end{pmatrix}_{i=1}^n
	.
\end{equation*}
We denote the $i$-th entry of a vector $v \in \R^n$ with $v_i$ or $(v)_i$.
We write a matrix $A \in \R^{n \times k}$ consisting of the column vectors 
$v_1, \dots, v_k \in \R^n$ as $A = (v_1, \dots, v_k)$.
Given some matrices $A,B,C,D \in \R^{n_1 \times n_2}$, we write
\begin{equation*}
	\begin{pmatrix}
		A & B \\
		C & D
	\end{pmatrix} \in \R^{2n_1 \times 2n_2}
\end{equation*}
for the matrix consisting of the blocks $A,B,C$ and $D$.
The $n$-by-$n$ identity matrix is denoted by $I_n \in \R^{n \times n}$. 
If the dimension $n$ can be inferred from context we may just write $I$.
We denote the $n$-dimensional $1$-vector by $\onevec{n} \in \R^n$.

We denote the $k$-th partial derivative of a function $f: \mathbb{R}^n \to \mathbb{R},
x \mapsto f(x)$ with $\deldel{x_k} f(x)$, $k=1, \dots, n$.

Given a multi-dimensional function $f: \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$,
we denote Jacobian with
\begin{equation*}
	\jac{f}{x} \in \R^{n_2 \times n_1}
	.
\end{equation*}
If we want to emphasize that the Jacobian is evaluated at $v \in \R^{n_1}$, we write
\begin{equation*}
	\jac{f}{x} \bigg|_{x = v} \in \R^{n_2 \times n_1}
	.
\end{equation*}

For a scalar-valued function $f : \R^n \to \R$, we denote the gradient with
$\grad{f} \in \R^n$ and the Hessian matrix with $\nabla^2 f \in \R^{n \times n}$.

We write $\N$ for the set of naturnal numbers without zero, and we write $\N_0$ 
for the set of naturnal numbers with zero. We denote the set of discrete functions from
$\mathbb{Z}$ to $\R$ as $\R^{\mathbb{Z}} := \{ f : \mathbb{Z} \to \mathbb{R} \}$.

We write the Kronecker delta as
\begin{equation*}
	\delta_{ij} := \begin{cases}
		1 &: i = j \\
		0 &: \text{else}
	\end{cases}
	.
\end{equation*}

The number $d \in \N$ will have a special meaning in the context of Hamiltonian systems.
We reserve the usage of $d$ for this purpose throughout this thesis. We define 
the skew-symmetric matrix $J \in \R^{2d \times 2d}$ as
\begin{equation*}
	J := \begin{pmatrix}
		0 & I_d \\
		-I_d & 0
	\end{pmatrix}
	.
\end{equation*}

For $q,p \in \R^{d}$ and maps $f_{11}, f_{12}, f_{21}, f_{22} : \R^d \to \R^d$,
we define the block operator as
\begin{equation*}
	\begin{bmatrix}
		f_{11} & f_{12} \\
		f_{21} & f_{22}
	\end{bmatrix}
	\qpvec
	:= \begin{pmatrix}
		f_{11}(q) + f_{12}(p) \\
		f_{21}(q) + f_{22}(p)
	\end{pmatrix} \in \R^{2d}
	.
\end{equation*}
We denote the identity map with $id$.


%----------------------------------------------------------------------------------------
%
% Content
%
%----------------------------------------------------------------------------------------
\newpage
\section{Problem setup}\label{sec_problem_setup}
Let us introduce the basic theory for Hamiltonian systems and describe the problem
setup in more detail.

\begin{definition}
	A matrix $A \in \mathbb{R}^{2d \times 2d}$ is called symplectic if $A^TJA=J$.
\end{definition}

\begin{definition}
	A differentiable map $\phi : U \to \mathbb{R}^{2d}$ (where $U \subset \mathbb{R}^{2d}$ is an open set)
	is called symplectic if the Jacobian matrix $\jac{\phi}{x}$ is symplectic everywhere, i.e.
	\begin{equation*}
		\lb \jac{\phi}{x} \rb^T J \lb \jac{\phi}{x} \rb = J
		.
	\end{equation*}
\end{definition}

\begin{definition}
	A Hamiltonian (ODE) system can be written in canonical form as
	\begin{align*}
		\dot{y}(t) &= J \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
		y(t_0) &= y_0
		,
	\end{align*}
	where $y: I \subset \mathbb{R} \to \mathbb{R}^{2d},\, t \mapsto y(t) = (q(t),p(t))$ and 
	$y_0 = (q_0, p_0) \in \mathbb{R}^{2d}$ the initial value at $t_0 \in I$. 
	The function $H: \mathbb{R}^{2d} \to \mathbb{R}$ is called the Hamiltonian 
	or the total energy. The entries of $q = q(t) \in \mathbb{R}^d$ are called 
	generalized coordinates
	and the entries of $p=p(t) \in \mathbb{R}^d$ are called conjugate momenta. 
	The phase space $\mathcal{V} = \mathbb{R}^{2d}$ has even dimension for Hamiltonian systems.
\end{definition}

Note that the Hamiltonian $H$ is a first integral, i.e. the total energy is preserved, because
\begin{equation*}
	\ddt H(y(t)) = \lsb \grad{H}(y(t)) \rsb^T \dot{y}(t) = 
	\lsb \grad{H}(y(t)) \rsb^T J \grad{H}(y(t)) = 0
\end{equation*}
since $J$ is skew-symmetric.

Let $\phi_{t,H} : U \subset \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ be the flow for a 
canonical Hamiltonian system with Hamiltonian $H$ for a fixed time $t$, i.e.
\begin{equation*}
	\phi_{t,H}\begin{pmatrix}
		q_0 \\
		p_0
	\end{pmatrix}
	= \begin{pmatrix}
		q(t; q_0, p_0) \\
		p(t; q_0, p_0)
	\end{pmatrix}
	,
\end{equation*}
where $q(t; q_0, p_0)$ and $p(t; q_0, p_0)$ denote the solution of the Hamiltonian system
at time $t$ for initial values $y_0 = (q_0,p_0) \in \mathbb{R}^{2d}$. 
Poincaré has shown that the flow of a Hamiltonian system is symplectic.

\begin{theorem}(Poincaré 1899)
	Let $H: \mathbb{R}^{2d} \to \mathbb{R}$ be a twice continuously differentiable
	function on $U \subset \mathbb{R}^{2d}$. Then, for each fixed $t$, the flow
	$\phi_{t,H}$ is a symplectic map wherever it is defined.
\end{theorem}
\begin{proof}
	We refer to \citet[Theorem 2.4, p.~184]{hairer2006} 
	or to \citet[Theorem 1, p.~54]{leimkuhler_reich_2005}.
\end{proof}

Note that a Hamiltonian (ODE) system in general can be written as
\begin{align*}
	\dot{y}(t) &= S \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
	y(t_0) &= y_0
\end{align*}
with an arbitrary nondegenerate skew-symmetric matrix $S \in \mathbb{R}^{2d \times 2d}$.
However there always exists a transformation to express such a Hamiltonian ODE system in
canonical form (see \citet[Remark 3.8]{peng2016}). Thus, we restrict w.l.o.g.
to canonical systems.

Our goal is to learn the flow map $\phi_{t,H} : U \to \mathbb{R}^{2d}$ for fixed $t$ with a neural network.
To be specific, for fixed $t$, we supply the neural network with training pairs $(y_i, \phi_{t,H}(y_i))$
($y_i \in \mathbb{R}^{2d}$ and $i=1, \dots, n_{\text{train}}$) and want the neural network
to be able to predict $\phi_{t,H}(y)$ for arbitrary $y \in \mathbb{R}^{2d}$.
This leads to the idea that we structurally embed symplecticity into the neural network itself.

Symplecticity has already been successfully embedded into numerical integrators for ODEs, which
led to the development of geometric integrators, see for example \citeauthor{hairer2006}
(\cite{hairer2006}).

\section{Architecture of SympNets}\label{sec_architecture_of_sympnets}

In this section we recapitulate the architecture of SympNets (Symplectic Networks) as
proposed by \citeauthor{Jin2020} in \cite{Jin2020}. We prove
symplecticity for all layer types.

First, let us briefly introduce neural network terminology which we will use.
We refer to \cite{Goodfellow2016} for a comprehensive look at neural
network concepts.

\begin{definition}
	(Layer)
	A neural network layer with input dimension $n_1$ and output dimension $n_2$
	and parameters $\theta \in \R^{n_\theta}$ is a map 
	$\phi : \R^{n_1} \to \R^{n_2},\, x \mapsto \mathcal{L}(x;\theta) = \mathcal{L}(x)$.
\end{definition}

A common layer is the so-called (fully-connected) linear layer
$\phi(x) = Wx +b$ with parameters $W \in \R^{n_2 \times n_1},\, b \in \R^{n_2}$,
i.e. $\theta = (vec(W)^T, b^T)^T \in \R^{n_2 n_1 + n_2}$.

In our special case we will always have $n_1 = n_2 = 2d$.

\begin{definition}
	(Neural Network)
	A neural network $\Phi$ is a composition of one or multiple layers with
	compatible input and ouput dimensions:
	\begin{equation*}
		\Phi(x;\Theta) = \phi_{n_L} \lb \phi_{n_L-1} \lb \cdots
		\lb \phi_2(
		\phi_1(x;\theta_1); \theta_2 ) \cdots \rb ; \theta_{n_L-1} \rb ; \theta_{n_L} \rb
	\end{equation*}
	We denote the vector of all
	layer parameters with $\Theta = (\theta_1^T, \dots, \theta_{n_L}^T)^T$.
\end{definition}

We use neural networks in the context of supervised learning. 
In supervised learning, we want to learn a function based on some training data
$\mathcal{T} = \{(x_1, y_1) \dots, (x_{n_{\text{train}}},y_{n_{\text{train}}}) \}
\subset \R^{n_x} \times \R^{n_y}$. Here, $x_i$ refers to input
and $y_i$ refers to the expected output.

If the training data $\mathcal{T}$ does not fit into memory,
we split the training data into disjoint subsets
$\{ \mathcal{B}_i \}_{i=1}^{n_{\text{b}}}$ with
$\mathcal{T} = \mathcal{B}_1 \cup \cdots \mathcal{B}_{n_{\text{b}}}$.
We call every $\mathcal{B}_i$ a mini-batch.

For a given loss function $L$, the neural network tries to minimize the loss 
function $L$ by learning locally optimal parameters $\Theta_{\text{min}}$.
Typically, the loss function $L$ is expressed as
\begin{equation*}
	L(\mathcal{B}; \Theta) = \sum_{(x_i,y_i) \in \mathcal{B}} l(\Phi(x_i; \Theta), y_i),
\end{equation*}
where $l : \R^{n_y} \times \R^{n_y} \to \R$ gives the loss for an 
individual training sample and $\mathcal{B}$ refers to a mini-batch
or to the whole training data $\mathcal{T}$.

The neural network trains a local optimum $\Theta_{\text{min}}$ via
an iterative algorithm from the gradient descent family. For example, given
initial parameters $\Theta_0$, the standard gradient descent algorithm is defined as
\begin{equation*}
	\Theta_{i+1} = \Theta_i - \gamma \grad[\Theta]{L(\mathcal{T};\Theta_{i})}
	.
\end{equation*}
Here, $\gamma \in \R$ denotes the learning rate.
If the training data $\mathcal{T}$ is split into multiple mini-batches 
$\{ \mathcal{B}_i \}_{i=1}^{n_{\text{b}}}$, every gradient-descent step
is executed with a different mini-batch $\mathcal{B}_i$, i.e.
\begin{equation*}
	\Theta_{i+1} = \Theta_i - \gamma \grad[\Theta]{L(
		\mathcal{B}_{((i \textrm{ mod } n_{\text{b}}) + 1)};
		\Theta_{i})}
	.
\end{equation*}
In this case, the algorithm is called stochastic gradient descent, because it estimates
the gradient of the whole training data based on smaller mini-batches.
The iteration index $i \in \N$ denotes the so-called epoch.

The algorithm for computing the gradient is called backpropagation, which
is a special case of reverse-mode automatic differentation.
Basically, the backpropagatin algorithm first computes the loss value 
in a so-called forward pass
and keeps the intermediate values in a computational graph.
Afterwards, in the so-called backward pass, the chain rule is applied programmatically
on the computational graph to compute the gradient, while reusing the intermediate values
from the forward pass.

\subsection{General architecture}

Let us recapitulate the general architecture of SympNets.
We use that the composition of symplectic maps is again symplectic. So we impose that every
layer of the neural network must be symplectic, which leads to the overall neural network
to be symplectic.

\begin{definition}
	(Unit Triangular Layer) A layer $\phi : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ 
	is called a unit triangular layer with layer transform 
	$\layertf : \mathbb{R}^d \to \mathbb{R}^d,\, p \mapsto \layertf(p)$
	and bias parameter $b \in \R^{2d}$, if $\phi$ can be expressed as
	\begin{equation*}
		\phi_\up \qpvec = \uppersympop{\layertf} + b
		= \begin{pmatrix}
			q + \layertf(p) \\
			p
		\end{pmatrix} + b \quad \text{(upper unit triangular layer)}
	\end{equation*}
	or
	\begin{equation*}
		\phi_\low \qpvec = \lowersympop{\layertf} + b
		= \begin{pmatrix}
			q \\
			\layertf(q) + p
		\end{pmatrix} + b. \quad \text{(lower unit triangular layer)}
	\end{equation*}
	If we do not explicitly specify the bias parameter $b$, we assume
	that $b$ is a non-learnable constant with value $b=0$. The layer transform
	also depends on parameters $\theta \in \R^{n_\theta}$, i.e.
	$\layertf(p) = \layertf(p;\theta)$, 
	but we suppress the dependency on the parameters for shorter notation.
\end{definition}

A SympNet only consists of unit triangular layers. In order that the unit triangular layers 
$\phi_{\text{up}}$ or $\phi_{\text{low}}$ are symplectic,
we have to put additional requirements on the layer transform $\layertf$, see
\cref{jacobi_symmetric} later in this section.

This structure is a special version
of the structure proposed by \citeauthor{Deco1995} in \cite{Deco1995} 
for volume-conserving neural networks. Both SympNets and the volume-conserving neural
networks proposed by \citeauthor{Deco1995} are so-called residual neural networks:

\begin{definition}
	(Residual neural network)
	A neural network is called a residual neural network
	if it can be expressed as a composition of residual blocks
	\begin{equation*}
		R(x; \theta) = R(x) = \mathcal{F}(x; \theta) + x
	\end{equation*}
	with input $x \in \R^{n_1}$ and parameters $\theta \in \R^{n_\theta}$. 
	The map $\mathcal{F}$ represents the residual mapping.
	In other words, the neural network $\Phi$ then can be expressed as
	\begin{equation*}
		\Phi(x) = (R_n \circ \cdots \circ R_1)(x)
	\end{equation*}
	with residual blocks $R_1, \dots, R_n$.
\end{definition}

The term "residual neural network" was firstly introduced by \citeauthor{resnet2016} in
\cite{resnet2016}. A SympNet is indeed a residual neural network, because a
unit triangular layer itself forms a residual block:
\begin{equation*}
	\phi_\up \qpvec = \lb \begin{pmatrix}
		T(p) \\
		0
	\end{pmatrix} + b \rb + \qpvec
\end{equation*}

\begin{lemma}\label{jacobi_symmetric}
	An upper or lower unit triangular layer $\phi_{\text{up}}$ or $\phi_{\text{low}}$
	is symplectic if and only if the Jacobian of the layer transform
	$\layertf \in C(\mathbb{R}^d, \R^d)$ 
	is symmetric everywhere.
\end{lemma}
\begin{proof}
	We show the result for $\phi_{\text{up}}$ only. 
	The proof is analogous for $\phi_{\text{low}}$.
	We have that
	\begin{equation*}
		\jac{\phi_{\text{up}}}{(q,p)} = \begin{pmatrix}
			I & \jac{\layertf}{p} \\
			0 & I
		\end{pmatrix}
		.
	\end{equation*}
	It follows
	\begin{align*}
		\lb \jac{\phi_{\text{up}}}{(q,p)} \rb^T J \lb \jac{\phi_{\text{up}}}{(q,p)} \rb
		&= \lb \jac{\phi_{\text{up}}}{(q,p)} \rb^T \begin{pmatrix}
			0 & I \\
			-I & -\jac{\layertf}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			I & 0 \\
			\lb \jac{\layertf}{p} \rb^T & Id
		\end{pmatrix} \begin{pmatrix}
			0 & I \\
			-I & -\jac{\layertf}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			0 & I \\
			-I & \lb \jac{\layertf}{p} \rb ^T-\jac{\layertf}{p}
		\end{pmatrix} \overset{!}{=} J
		.
	\end{align*}
	Thus, $\phi_{\text{up}}$ is symplectic if and only if
	$\lb \jac{\layertf}{p} \rb ^T-\jac{\layertf}{p}=0$, 
	i.e. if and only if the Jacobian $\jac{\layertf}{p}$ is symmetric everywhere.
\end{proof}

The next corollary is useful to construct symplectic unit triangular layers.
\begin{corollary}\label{gradient_corollary}
	Let $V: \mathbb{R}^d \to \mathbb{R}, \; p \mapsto V(p)$ be a function in 
	$C^2(\R^d)$. Then the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = \grad{V}(p)
	\end{equation*}
	are symplectic. We call $V$ a potential.
\end{corollary}
\begin{proof}
	The Jacobian matrix of $\grad{V}$ corresponds to the Hessian matrix of $V$,
	i.e. $\jac{(\grad{V})}{p} =\nabla^2 V$.
	The Hessian is everywhere symmetric due to $V \in C^2(\R^d)$ (Lemma of Schwarz),
	thus the result follows with \cref{jacobi_symmetric}.
\end{proof}

\subsection{Linear layers}
In this section, we introduce symplectic linear layers. These linear layers can be interpreted 
as the symplectic analogon to the fully-connected linear layers used in a fully-connected
neural network (FNN).

\begin{definition}\label{def_linear_layer}
	(Linear layers)
	Given a symmetric matrix $S \in \R^{d \times d}$ and bias $b \in \R^{2d}$,
	we call the upper and lower unit triangular layers
	with layer transform $\layertf(p) = Sp$ the (symplectic) upper and lower 
	linear layers $\ell_\up$ and $\ell_\low$.
\end{definition}

The linear layers can be expressed with matrix-vector multiplication, for example
\begin{equation*}
	\ell_\up \qpvec = \begin{pmatrix}
		I & S \\
		0 & I
	\end{pmatrix} \qpvec + b
	.
\end{equation*}

The matrix $S$ and the bias $b$ are learnable parameters.
In practice, we parametrize the symmetric matrix $S\in \mathbb{R}^{d \times d}$
with $S = A^T + A$ via another arbitrary matrix $A\in \mathbb{R}^{d \times d}$, as
most optimization methods used for learning neural networks are designed for
unconstrained optimization problems.

The linear layers are symplectic, because the Jacobian of the layer transform $\jac{\layertf}{p} = S$
is a symmetric matrix by defintion. Alternatively, we can apply \cref{gradient_corollary} by
choosing the potential $V(p) := p^TAp$.

\begin{proof}
	For $k=1, \dots, d$, we have
	\begin{align*}
		\lb \grad{V}(p) \rb_k &= \deldel{p_k} \lb \sum_{i,j=1}^d A_{ij} p_i p_j \rb
		= \sum_{i,j=1}^d A_{ij} \deldel{p_k}(p_i) p_j + \sum_{i,j=1}^d A_{ij} p_i \deldel{p_k}(p_j) \\
		&= \sum_{j=1}^d A_{kj} p_j + \sum_{i=1}^d A_{ik} p_i = ((A+A^T)p)_k
		.
	\end{align*}
	Choose $A=\frac{1}{2}S \implies A+A^T=S$.
	Thus, symplecticity follows with \cref{gradient_corollary}.
\end{proof}

We may enhance expressivity of a linear layer by alternately composing multiple
$\ell_{\up}$ and $\ell_{\low}$. We define 
$\mathcal{L}^{n}_{\up},\, \mathcal{L}^{n}_{\low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ with

\begin{align*}
	\mathcal{L}^{n}_{\up} \qpvec &:= \begin{pmatrix}
		I && 0 / S_n \\
		S_n / 0 && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && 0 \\
		S_2 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix} + b \\
	\mathcal{L}^{n}_{\low} \qpvec &:= \begin{pmatrix}
		I && S_n / 0 \\
		0 / S_n && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && 0 \\
		S_1 && I
	\end{pmatrix} + b
	.
\end{align*}

The composed layers
$\mathcal{L}^{n}_{\up}$ and $\mathcal{L}^{n}_{\low}$ are again symplectic because they 
are a composition of the symplectic maps $\ell_{up}$ and $\ell_{low}$.

\citeauthor{jin2020unit} show in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map. In other words, 
the set of all possible $\mathcal{L}^{9}_{\up}$ is equal to the set of all symplectic linear maps.

It does not make sense to put two upper linear layers or two lower linear layers after each other,
because this reduces to a single upper or lower linear layer:

We briefly show the statement for two upper linear layers $\ell_{up,2}$ and $\ell_{up,1}$.
\begin{align*}
	\lb \ell_{up,2} \circ \ell_{up,1} \rb \qpvec &=
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\lb
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix}
	\qpvec + b_1
	\rb + b_2 \\
	&= \begin{pmatrix}
		I && S_1 + S_2 \\
		0 && I
	\end{pmatrix} \qpvec
	+ \begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix} b_1
	+ b_2 \\
	&= \begin{pmatrix}
		I && S \\
		0 && I
	\end{pmatrix} \qpvec + b
\end{align*}
with $S := S_1 + S_2$ and $b := \begin{pmatrix}
	I && S_2 \\
	0 && I
\end{pmatrix} b_1
+ b_2$.

\subsection{Activation layers}
In this section, we introduce the symplectic analogon
to activation layers in fully-connected neural networks.
In contrast to the linear layers introduced in the previous section,
the activation layers are nonlinear as long the activation
function $\activation : \R \to \R$ is nonlinear.

\begin{definition}
	(Activation layers)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \activation(p_i) \rb_{i=1}^d
	\end{equation*}
	the upper and lower activation layers $\mathcal{N}_\up$ and $\mathcal{N}_\low$.
\end{definition}
The coefficients $a \in \mathbb{R}^d$ are learnable parameters.

\begin{corollary}
	Given an activation function $\activation \in C^1(\R)$,
	the activation layers $\mathcal{N}_{up}$ and $\mathcal{N}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be an integral of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{k=1}^d a_k \mathcal{A}(p_k)
		.
	\end{equation*}

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $k=1, \dots d$ follows
	\begin{equation*}
		\lb \grad{V}(p) \rb_i = \deldel{p_i} \lb \sum_{k=1}^d a_k \mathcal{A}(p_k) \rb
		= a_i \activation(p_i) = \lb \layertf(p) \rb_i
		.
	\end{equation*}
	Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

\subsection{Gradient layers}

The so-called gradient layers are nonlinear layers with
greater expressivity than activation layers. The layer transform $\layertf$
of a gradient layer can be compared to two fully-connected linear layers with a
nonlinear activation in-between. The name "gradient layer" is
motivated by the fact that the layer transform $\layertf$ can approximate,
under certain conditions, an arbitrary $\grad{V}$ for $V \in C^1(\R^d)$, 
see \citet[Lemma 4]{Jin2020}. The gradient layers are, among others, inspired
by the symmetric layer in \citet{Ruthotto2020}.

\begin{definition}
	(Gradient layers)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \activation \lb (Kp)_j + c_j \rb \bigg)_{j=1}^n
	\end{equation*}
	the upper and lower gradient layers $\mathcal{G}_\up$ and $\mathcal{G}_\low$.
\end{definition}
Here, $K \in \mathbb{R}^{n \times d}$ and $a,c \in \mathbb{R}^n$
are learnable parameters. 
The number $n \in \mathbb{N}$ denotes the width of the gradient layer and can be chosen freely.
In practice, we choose $n \gg d$ for large expressivity.

\begin{corollary}
	Given an activation function $\sigma \in C^1$ the gradient layers $\mathcal{G}_{up}$
	and $\mathcal{G}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be an integral of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + c_j)
		.
	\end{equation*}
	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $i=1, \dots, d$ follows
	\begin{align*}
		(\grad{V(p)})_i &= \deldel{p_i} \lb \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + c_j) \rb
		= \sum_{j=1}^n a_j \activation((Kp)_j + c_j) 
		\underbrace{\deldel{p_i} \lb (Kp)_j \rb}_{=K_{ji} = K^T_{ij}}
		= (\layertf(p))_i
		.
	\end{align*}
	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}_{up}$ and $\mathcal{G}_{low}$ are symplectic.
\end{proof}

Note that activation layers are a subset of gradient layers. We can see this by choosing
$n=d$, $K=I_d$ and $c=0$.

\section{Univeral approximation theorems}\label{sec_unv_approx_theorem}

In \cite{Jin2020}, \citeauthor{Jin2020} show approximation theorems for SympNets.
We repeat the statements here and refer to \cite{Jin2020} for proofs.
We define the norm on $C^r(W, \R^n)$ for a compact set $W \subset \R^m$ as
\begin{equation*}
	\norm{f}_{C^r(W,\R^n)} := \sum_{\abs{\alpha} \leq r}
	\max_{1 \leq i \leq n} \sup_{x \in W}
	\abs{\frac{\partial^{\abs{\alpha}}}{\partial x_1^{\alpha_1} \cdots x_m^{\alpha_m}} 
	f_i(x)}
	,
\end{equation*}
where $f = (f_1, \dots, f_n)^T \in C^r(W,\R^n)$ and $\alpha \in \N_0^m$ denotes a multi-index, i.e. 
$\abs{\alpha} = \alpha_1 + \cdots + \alpha_m$.

\begin{definition}
	($r$-finite)
	Let $r \in \N_0$ be given. The function $\sigma$ is r-finite if $\sigma \in C^r(\R)$
	and $0 < \int \abs{\frac{\mathrm{d}^r}{\mathrm{d}x^r} \sigma} d\lambda < \infty$. Here,
	$\lambda$ is the Lebesgue measure on $\R$.
\end{definition}

\begin{definition}
	($r$-uniformly dense on compacta)
	Let $m,n \in \N, \, r \in \N_0$ be given, $U \subset \R^m$ an open set,
	and $S_1 \subset C^r(U,\R^n)$. Then we say $S_2$ is $r$-uniformly dense
	on compacta in $S_1$ if $S_2 \subset S_1$ and for all $f \in S_1$, compact
	$W \subset U$ and every $\epsilon > 0$, there exists $g \in S_2$ such that
	$\norm{f-g}_{C^r(W,\R^n)} < \epsilon$.
\end{definition}

\begin{definition}\label{def_la_sympnet}
	(LA-SympNet)
	We call a neural network $\Phi$ a LA-SympNet if it can be expressed as
	a composition of linear and alternating lower and upper activation layers
	\begin{equation*}
		\Phi = \mathcal{L}^n
		\circ \lb \mathcal{N}_{\text{up/low}} \circ \mathcal{L}^n \rb \circ
		\cdots
		\circ \lb \mathcal{N}_\up \circ \mathcal{L}^n \rb
		\circ \lb \mathcal{N}_\low \circ \mathcal{L}^n \rb
		.
	\end{equation*}
	Here, $\mathcal{L}^n = \mathcal{L}_{\text{low}}^n$ or $\mathcal{L}^n = \mathcal{L}_{\text{up}}^n$,
	where $n \in \N$ denotes the number of the so-called (linear) sublayers.
	The depth of the LA-SympNet is determined by the number of activation layers
	$\mathcal{N}_{\text{up/low}}$.
\end{definition}

\begin{definition}\label{def_g_sympnet}
	(G-SympNet)
	We call a neural network $\Phi$ a G-SympNet if it can be expressed as a composition
	of alternating lower and upper gradient layers
	\begin{equation*}
		\Phi = \mathcal{G}_{\text{up/low}} \cdots \circ \mathcal{G}_\up \circ \mathcal{G}_\low,
	\end{equation*}
	where all gradient layers share the same width $n \in \N$.
	The depth of the G-SympNet is determined by the number of gradient layers
	$\mathcal{G}_{\text{up/low}}$.
\end{definition}

Let us denote the set of all symplectic maps in $C^r(U, \R^{2d})$ for an open set $U \subset \R^{2d}$ as
\begin{equation*}
	\mathcal{S}\mathcal{P}^r(U) = \left\{ 
		\phi \in C^r(U, \R^{2d}) : \phi \, \text{ is a symplectic map}
	\right\}
\end{equation*}

\begin{theorem}
	(Approximation theorem for LA-SympNets)
	For all $r \in \N$ and open sets $U \subset \R^{2d}$, the set of all LA-SympNets
	is $r$-uniformly dense on compacta in $\mathcal{S}\mathcal{P}^r(U)$ if the activation function
	$\activation$ is $r$-finite.
\end{theorem}

\begin{theorem}
	(Approximation theorem for G-SympNets)
	For all $r \in \N$ and open sets $U \subset \R^{2d}$, the set of all G-SympNets
	is $r$-uniformly dense on compacta in $\mathcal{S}\mathcal{P}^r(U)$ if the activation function
	$\activation$ is $r$-finite.
\end{theorem}

The approximation theorems do not give any information
about the required size of the SympNet, or how long it takes to learn a SympNet,
but they state that for every smooth symplectic map, there exists a LA-SympNet or G-SympNet
which approximates the smooth symplectic map arbitrarily well on a compact set,
under certain conditions.

\citeauthor{Jin2020} have already shown in \cite[Lemma 1]{Jin2020} that the
sigmoid activation function $\sigma(x) = \frac{1}{1+e^{-x}}$
is $r$-finite for $r \in \N$. This implies that the tanh activation function,
$\mathrm{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$,
also is $r$-finite, because with $\mathrm{tanh}(x) = 2\sigma(2x) - 1$, it follows for $r \in \N$
that $\mathrm{tanh} \in C^r(\R)$ and
\begin{equation*}
	\frac{\mathrm{d}^r}{\mathrm{d}x^r} \mathrm{tanh}(x) = 
	2^{r+1} \frac{\mathrm{d}^r}{\mathrm{d}x^r} \sigma(2x)
	.
\end{equation*}
Thus we have
\begin{equation*}
	0 < \int_{- \infty}^{\infty} \abs{\frac{\mathrm{d}^r}{\mathrm{d}x^r} \mathrm{tanh}(x)} dx
	= 2^{r+1} \int_{- \infty}^{\infty} \abs{\frac{\mathrm{d}^r}{\mathrm{d}x^r} \sigma(2x)} dx
	= 2^{r+1} \int_{- \infty}^{\infty} \abs{\frac{\mathrm{d}^r}{\mathrm{d}x^r} \sigma(\tilde{x})} d\tilde{x}
	< \infty
	,
\end{equation*}
since the sigmoid function $\sigma$ is $r$-finite. This is the motivation why we conduct
our numerical experiments with the sigmoid and tanh activation function. In addition to
these two activation functions, we conduct the experiments with the ELU activation function defined as
\begin{equation*}
	\mathrm{ELU}(x) = \begin{cases}
		x &: x > 0 \\
		a (\exp(x) - 1) &: x \leq 0
	\end{cases}
\end{equation*}
The ELU activation function is continously differentiable for $a=1$,
which makes it a reasonable choice in our context. However, 
the ELU activation function is not $r$-finite for all $r \in \N_0$, since
it is only continously differentiable once and the integral 
$\int \abs{\frac{\mathrm{d}^r}{\mathrm{d}x^r} \mathrm{ELU}} d\lambda$
diverges for $r=0$ and $r=1$. Universal approximation theorems for SympNets
for non-$r$-finite activation functions is an open topic to be explored.

\section{Convolution and Normalization for SympNets}\label{sec_conv_and_norm_for_sympnets}

In this section we introduce new extensions to SympNets. In particular, we bring the concept
of convolution to SympNets and we propose a possibility how to embed normalization into SympNets while
maintaining symplecticity.

\subsection{Introduction to Convolution}

Convolution in neural networks has been successfully used in a variety of 
applications, because it enables parameter sharing and leads to 
sparse-connectivity compared to for example a fully-connected layer \cite{Goodfellow2016}.
Basically, in convolutional neural networks, matrix multiplication is 
replaced by convolution, which also is a linear operation.

We introduce convolution in a rather abstract way in order to simplify the following proofs
for symplecticity.
General convolution is defined on infinite-dimensional function spaces.
Neural networks implement a finite-dimensional version, as the operation has to be
computable.

Given two discrete functions $f,g \in \mathbb{R}^\mathbb{Z}$ convolution is defined as 
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau - a)
	.
\end{equation*}
However, most popular neural network libraries actually do not implement real convolution
in convolution layers. Instead, they implement the so called cross-correlation, but call it convolution. 
Cross-correlation is a flipped convolution and is defined as
\begin{equation*}
	\star: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f \star g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau + a)
	.
\end{equation*}
Because most popular neural network libraries implement cross correlation and call it convolution,
we stick to the same convention and use cross-correlation from now on.
Still, the choice is arbitrary, as the following statements could be constructed analogously with convolution.

We work with finite-dimensional vectors in neural networks. Therefore, we define a bijective mapping between
vectors in $\mathbb{R}^n$ and functions in $\mathbb{R}^{\mathbb{Z}}$ 
via zero-continuation on $\mathbb{Z}$:
\begin{align*}
	\mathcal{I}_n &: \mathbb{R}^n \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb\mathcal{I}_n(x)\rb(\tau) := \sum_{a=1}^{n} x_a \delta_{\tau a} \\
	%
	\mathcal{I}_n^{\text{inv}} &: \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^n,
	\quad \mathcal{I}_n^{-1}(f) := \lb f(\tau) \rb_{\tau=1}^n
\end{align*}

Additionially, we define a shift operator $\mathcal{S}_\zeta$, 
which shifts a discrete function $f \in \mathbb{R}^\mathbb{Z}$ by $\zeta \in \mathbb{Z}$ positions
in the right direction:
\begin{equation*}
	\mathcal{S}_\zeta : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb \mathcal{S}_\zeta(f) \rb (\tau) := f(\tau - \zeta)
\end{equation*}

\begin{definition}\label{def_cross_corr}
	(Valid cross-correlation)
	We define the valid cross-correlation of a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$ and 
	a finite-dimensional input $x \in \mathbb{R}^{n_x}$ with $n_x \geq n_k$ as
	\begin{equation*}
		\star_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_x-n_k+1},
		\quad \star_{\text{v}}(k,x) := \mathcal{I}_{n_x-n_k+1}^{-1} (
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) \star \mathcal{I}_{n_x}(x)
		)
		.
	\end{equation*}
\end{definition}
In other words, the valid cross-correlation $\star_{\text{v}}$ evaluates the cross-correlation 
only at positions where the finite-dimensional kernel $k$ and the 
finite-dimensional input $x$ fully overlap and puts the result into a finite-dimensional vector.

The kernel $k$ will be a parameter which the neural network should learn.
Note that in this context it does not make a difference if we use convolution or cross-correlation, because
the neural network would just learn a flipped version of the kernel $k$ in the opposite case.

We define a stride operator $\mathcal{D}_s$ for a stride number $s \in \mathbb{N}$ with
\begin{equation*}
	\mathcal{D}_s : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (\mathcal{D}_s)(\tau) := f(s \tau)
	.
\end{equation*}
The stride operators skips $s$ entries of $f \in \mathbb{R}^\mathbb{Z}$.

\begin{definition}\label{def_cross_corr_stride}
	(Valid cross-correlation with stride)
	Given a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$, a finite-dimensional
	input $x \in \mathbb{R}^{n_x}$, stride number $s \in \mathbb{N}$ and output dimension
	\begin{equation*}
		n_{\text{out}} = \left\lfloor \frac{n_x - n_k}{s} \right\rfloor + 1
	\end{equation*}
	the valid-cross correlation with stride is defined as
	\begin{equation*}
		\star_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_{\text{out}}},
		\quad \star_{\text{v}}(k,x) := 
		(\mathcal{I}_{n_{\text{out}}}^{-1} \circ D_s)
		(
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) \star \mathcal{I}_{n_x}(x)
		)
		.
	\end{equation*}
\end{definition}

Note that \cref{def_cross_corr_stride} equals \cref{def_cross_corr} for stride number $s=1$.

\subsection{Convolution layers}

The valid cross correlation (without stride) has the output dimension $n_x - n_k + 1$.
Thus, the valid cross-correlation $\star_{\text{v}}$ decreases the dimension of the input $x$.
However, we want to incorporate the valid cross-correlation as the layer transform $\layertf$ of a
unit triangular layer. The layer transform $\layertf$ must keep the dimension $d$.
Therefore we apply a so-called padding operation on the layer transform input $p \in \R^d$ 
before passing it to the valid
cross-correlation. The padding operation increases the dimension of $p$ in order
that the composition of padding and cross-correlation keeps the overall dimension.
Let $c \in \mathbb{N}$ denote the number by which the padding operator increases 
the input dimension, such that dimension is $n_x = d+c$. 
In order that the composition of padding and cross-correlation keeps the dimension, it must hold
\begin{equation*}
	n_x - n_k + 1 = \underbrace{(d+c)}_{\substack{
		\text{increased dim.} \\
		\text{by } c \text{ via padding}
	}} - \, n_k + 1 = d \iff c = n_k-1
	.
\end{equation*}
For symmetry reasons, we pad the same number of values at the beginning and 
the end of the input vector $p$. This means that the number of padded values $c$ has to be even,
i.e. we can write $c=2m$ for a $m \in \mathbb{N}_0$.
Consequently, the equation above implies that the kernel size $n_k$ has to be odd, i.e. 
$n_k = 2m+1$. We define two different padding operators,
which both increase the input vector dimension by $2m$.

\begin{definition}
	(Constant padding)
	Given the padding values $l,r \in \mathbb{R}^m$, we define constant padding 
	$c_{\text{pad},m}$ as
	\begin{equation*}
		c_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad c_{\text{pad},m}(p) = c_{\text{pad},m}(p;l,r) := \lb l^T, p^T, r^T \rb^T
		.
	\end{equation*}
\end{definition}
The constant padding $c_{\text{pad},m}$ is an affine linear map in $p$, because we can write
\begin{equation}\label{cpad_affine}
	c_{\text{pad},m}(p;l,r) = c_{\text{pad}}(p;0_m,0_m)
	+ c_{\text{pad},m}(0_d;l,r)
\end{equation}
and $c_{\text{pad},m}(p;0_m,0_m)$ is linear regarding $p$.

\begin{definition}
	(Symmetric padding)
	We define symmetric padding as
	\begin{equation*}
		s_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad s_{\text{pad},m}(p) := (
			\underbrace{p_m, p_{m-1} \dots, p_1}_{m \text{ values}}, \,
			\underbrace{p_1, p_2 \dots, p_d}_{d \text{ values}}, \,
			\underbrace{p_d, p_{d-1} \dots, p_{d-m+1}}_{m \text{ values}}
		)^T
		.
	\end{equation*}
\end{definition}
The symmetric padding $s_{\text{pad},m}$ is a linear map. We call the operation symmetric padding,
because the outermost left and right inner vector entries are reflected at the beginning and the end
of the vector.

Given an odd kernel $k \in \mathbb{R}^{2m+1}$, we set
\begin{equation*}
	\hat{k} = \hat{k}(k) := \mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k))
	.
\end{equation*}
Then $\hat{k}(-m) = k_1, \, \dots,\, \hat{k}(m) = k_{2m+1}$ and
$\hat{k}(\tau)=0$ for $\abs{\tau} > m$. With $\hat{k}$, we can express the valid cross-correlation as
\begin{equation*}
	\star_{\text{v}}(k,x) = \mathcal{I}_{n_x-2m}^{-1} (
		\mathcal{S}_{m}(\hat{k}) \star \mathcal{I}_{n_x}(x)
	)
	,
\end{equation*}
because
\begin{equation*}
	\mathcal{S}_{m}(\hat{k}) 
	= \mathcal{S}_{m}(\mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k)))
	= \mathcal{S}_{m+(-m-1)}(\mathcal{I}_{2m+1}(k))
	= \mathcal{S}_{-1}(\mathcal{I}_{2m+1}(k))
	.
\end{equation*}

\begin{definition}
	(Symmetric kernel)
	We call an odd kernel $k \in \mathbb{R}^{2m+1}$ symmetric if
	\begin{equation*}
		\hat{k}(\tau) = \hat{k}(-\tau)
	\end{equation*}
	for all $\tau \in \mathbb{Z}$.
\end{definition}

Let us now define symplectic convolution layers by combining padding and valid 
cross-convolution.
\begin{definition}\label{def_conv_layer}
	(Convolution layers)
	Given a symmetric kernel $k \in \mathbb{R}^{2m+1}$, padding
	$\chi_{\text{pad},m} = c_{\text{pad},m}$ or $\chi_{\text{pad},m} = s_{\text{pad},m}$,
	we call the upper and lower unit triangular layers with bias $b \in \R^{2d}$ and layer transform
	\begin{equation*}
		\layertf(p) := \star_{\text{v}}(k,\chi_{\text{pad},m}(p))
	\end{equation*}
	the (symplectic) convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$.
\end{definition}
The symmetric kernel $k \in \mathbb{R}^{2m+1}$ is a learnable parameter.
If $\chi_{\text{pad},m} = c_{\text{pad},m}$, the padding values $l,r \in \R^m$ can either
be learnable parameters or constant. The convolution layers can be used without any bias ($b=0$), a
constant non-learnable bias ($b \neq 0 =$ const), or with a learnable bias parameter 
($b$ is a learnable parameter). We stick to $b=0$ in our experiments.

With $\hat{\chi}_{\text{pad},m}(p) := \mathcal{I}_{d+2m}(\chi_{\text{pad},m}(p))$,
we can write the layer transform as
\begin{equation}\label{eq_conv_layer_transform}
	\layertf(p) =
	\mathcal{I}_{d}^{-1} (
		\mathcal{S}_{m}(\hat{k}) \star \hat{\chi}_{\text{pad},m}(p)
	)
	.
\end{equation}

\begin{lemma}\label{jac_linear_map}
	Let $f: \mathbb{R}^d \to \mathbb{R}^d$ be a linear map. Then the Jacobian matrix
	$\jac{f}{x}$ is constant, i.e.
	\begin{equation*}
		\jac{f}{x} \bigg|_{x = v} = \jac{f}{x} \bigg|_{x = w} \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
	and for the Jacobian-vector product holds
	\begin{equation*}
		\lb \jac{f}{x} \bigg|_{x = v} \rb w = f(w) \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
\end{lemma}
\begin{proof}
	$f$ is linear $\implies$ There exists a matrix representation $f(x) = Ax$ with
	$A \in \mathbb{R}^{d \times d}$ \\
	$\implies \jac{f}{x} \big|_{x=v} = A = \text{const.} \quad \forall v \in \mathbb{R}^d$
	$\implies \lb \jac{f}{x} \big|_{x=v} \rb w = Aw = f(w) \quad \forall v,w \in \mathbb{R}^d$
\end{proof}
As the Jacobian matrix for a linear map is constant, we omit the evaluation point, i.e.
$\jac{f}{x} \big|_{x=v} = \jac{f}{x}$.

\begin{theorem}\label{thm_conv_const_pad_symplectic}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $\chi_{\text{pad},m} = c_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	We have to show that the Jacobian of the layer transform $\layertf(p)$
	is symmetric (\cref{jacobi_symmetric}).
	Because of \cref{cpad_affine}, it suffices to show the case $l,r=0$ (the Jacobian
	of a constant has only zero-valued entries). For $l,r=0$, the layer transform 
	$\layertf(p)$ 
	is a linear map, because it is a composition of linear maps only.

	Define the bilinear form
	\begin{equation}\label{eq_bilinear_proof_conv}
		b : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R},
		\quad b(v,w) := \ip{v}{\lb \jac{\layertf}{p} \rb w}
		.
	\end{equation}
	To show symmetry of the Jacobian, we show that 
	$b(e_i, e_j) = b(e_j, e_i)$ for all $i,j=1,\dots,d$:
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{\lb \jac{\layertf}{p} \rb e_j}
		= \ip{e_i}{\activation_{\mathcal{C}}(e_j)} \quad \text{(\cref{jac_linear_map})} \\
		&\stackrel{\cref{eq_conv_layer_transform}}{=} \ip{e_i}{\mathcal{I}_{d}^{-1} (
			\mathcal{S}_{m}(\hat{k}) \star \hat{c}_{\text{pad},m}(e_j)
		)} \\
		&= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				\underbrace{(\hat{c}_{\text{pad},m}(e_j))(\tau+a)}_{
					= \delta_{(\tau+a) (j+m)}
				}
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((j-i+m)-m) = \hat{k}(j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\begin{theorem}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $\chi_{\text{pad},m} = s_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	With $\chi_{\text{pad},m} = s_{\text{pad},m}$, the layer transform 
	$\layertf$ is linear, because it is a composition of linear maps. Thus,
	we proof the statement the same way as \cref{thm_conv_const_pad_symplectic} above.
	For $\tau,a \in \mathbb{Z}$ we have
	\begin{equation*}
		\hat{s}_{\text{pad},m}(e_j)(\tau + a) = 
		\delta_{(\tau + a) (m-j+1)} + \delta_{(\tau + a) (j+m)} + \delta_{(\tau + a) (2d+m-j+1)}
		.
	\end{equation*}
	Consequently, for $\chi_{\text{pad},m} = s_{\text{pad},m}$ and $i,j=1, \dots, d$, 
	the bilinear form \cref{eq_bilinear_proof_conv} becomes
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				(\hat{s}_{\text{pad},m}(e_j))(\tau+a)
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((m-j+1-i)-m) + \hat{k}((j+m-i)-m) + \hat{k}((2d+m-j+1-i)-m) \\
		&= \hat{k}(1-j-i) + \hat{k}(j-i) + \hat{k}(1+2d-j-i)
		.
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\subsubsection*{Parametrization of a symmetric kernel}

Let $\{b_1, b_2, \dots b_m \} \subset \mathbb{R}^{2m+1}$ be a basis of the space 
$\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.
The symmetric kernel $k$ is then parametrized by the coefficients $\beta \in \mathbb{R}^m$ via
\begin{equation*}
	k = (b_1, b_2, \dots, b_m) \beta = \beta_1 b_1 + \beta_2 b_2 + \dots + \beta_m b_m
	.
\end{equation*}

One choice is the canonical basis with basis vectors $b_1, \dots, b_m \in \mathbb{R}^{2m+1}$ given by
\begin{equation*}
	(b_i)_{j+m+1} = \hat{k}(b_i)(j) = \begin{cases}
		1 &: \abs{j} = i-1 \\
		0 &:else
	\end{cases} 
	\quad (j=-m, \dots, m)
\end{equation*}

Another possible basis choice inspired by finite differences is:
\begin{align*}
	b_1^{FD} = (0, \dots, 0,& 1,0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_2^{FD} = (0, \dots, 0, 1,-&2,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_3^{FD} = (0, 0, \dots, 0, 1,-4,& 6,-4,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1} \\
	&\vdots
\end{align*}
In general, the entries for a basis vector $b_i^{FD} \in \mathbb{R}^{2m+1}$ $(1 \leq i \leq m)$ 
originate from Pascal's triangle.
\begin{equation*}
	\lb b_i^{FD} \rb_{j+m+1} = \hat{k}(b_i^{FD})(j) = \begin{dcases}
		(-1)^j \binom{2(i-1)}{j+i-1} &: \abs{j} < i \\
		0 &: \text{else}
	\end{dcases}
	\quad (j=-m, \dots, m)
\end{equation*}

The symmetry of the kernel basis vectors $b_i^{FD}$ follows with the definition of the binomial coefficient:
\begin{align*}
	\binom{2(i-1)}{j+i-1} &= \frac{2(i-1)!}{(j+i-1)!(2(i-1)-(j+i-1))!} \\
	&= \frac{2(i-1)!}{(j+i-1)!(i-j-1)!}
\end{align*}
Thus we have
\begin{equation*}
	\binom{2(i-1)}{(-j)+i-1} = \binom{2(i-1)}{j+i-1}
\end{equation*}
and for $i=1, \dots, m$ and $j= 0, \dots, m$
\begin{align*}
	\hat{k}(b_i^{FD})(-j) &= \begin{dcases}
		(-1)^{-j} \binom{2(i-1)}{(-j)+i-1} : \abs{-j} < i \\
		0 : else
	\end{dcases} \\
	&= \begin{dcases}
		(-1)^{j} \binom{2(i-1)}{j+i-1} : \abs{j} < i \\
		0 : else
	\end{dcases} \\
	&= \hat{k}(b_i^{FD})(j)
	.
\end{align*}
So $\{ b_i^{FD} \}_{i=1}^m$ is a valid basis of
the space $\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.

\citet{Ruthotto2020} discuss similar parametrizations for
convolution kernels, albeit in a more general PDE context. They
suggest for example to parametrize the kernel by taking a linear combination
of basis kernels refering to reaction, convection and diffusion terms.
In our numerical experiments, it turns out that
parametrization plays an important role how well a neural network learns.

\subsection{Convolution Gradient Layers}\label{sec_conv_gradient_layer}

In this section, we propose convolution for gradient layers.
\citet{Ruthotto2020} discuss convolution for a similar residual layer in a more 
general PDE context.

For a fixed kernel $k$, valid cross-correlation is a linear map regarding the second argument. 
Therefore there exists a representation matrix $A_{k}$ with 
$\star_{\text{v}}(k,p) = A_{k}p$. Transposed valid cross-correlation 
is defined as the linear map associated with the transpose $A_{k}^T$ of $A_{k}$.
We denote the transposed valid cross-correlation by $\star_{\text{v}}^T$.

Again, most popular neural network libraries say convolution, but actually implement
valid cross-correlation. In this case, transposed convolution actually refers to
transposed valid cross-correlation.

For large $d \in \mathbb{N}$, it can make sense to use convolution inside Gradient layers 
instead of a full matrix $K$. Let us repeat the layer transfrom $\layertf$ for Gradient layers:
\begin{equation*}
	\layertf(p) = K^T \bigg( a_j \activation \lb (Kp)_j + c_j \rb \bigg)_{j=1}^n
\end{equation*}
A gradient layer with width $n \in \mathbb{N}$ can be implemented by 
using valid cross-correlation (with stride) for $K^T \in \mathbb{R}^{d \times n}$ and the
corresponding transposed valid cross-correlation (with stride) for $K \in \mathbb{R}^{n \times d}$.
The choice of transpose is intentional, because we want to upscale the input $p \in \mathbb{R}^d$ ($n >> d$). 
Cross-correlation without padding decreases the dimension. Therefore we first apply 
transposed cross-correlation on $p$, because the transpose increases the dimension.
A large kernel size $n_k$ and a large stride number $s$ allow for significant upscaling.
We emphasize that the kernel $k$ has not to be symmetric in this setting. 
It is reasonable that the parametrization for $a \in \R^n$ and $c \in \R^n$ also respects
the convolution in a certain way.
One possibility to parametrize $a \in \R^n$ with a parameter $\tilde{a} \in \R^{n_k}$ is
\begin{equation*}
	c = \star_{\text{v}}^T(\tilde{c},1_d)
	.
\end{equation*}
The vector $c \in \R^n$ can be parametrized with a parameter $\tilde{c} \in \R^{n_k}$ analogously.
To summarize, the convolution gradient layers then can be defined as:
\begin{definition}
	(Convolution gradient layers)
	Given a kernel size $n_k \in \N$, a stride number $s \in \N$,
	kernel parameters $k,\tilde{a},\tilde{c} \in \mathbb{R}^{n_k}$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{equation*}
		T(p) = \star_{\text{v}} \lb k, \, \lb a_j \activation \lb \lb \star_{\text{v}}^T(k,p) \rb_j + c_j \rb \rb_{j=1}^n \rb 
		\quad
		\text{with } c = \star_{\text{v}}^T(\tilde{c},1_d) 
		\text{ and } a = \star_{\text{v}}^T(\tilde{a},1_d)
	\end{equation*}
	the upper and lower convolution gradient layers 
	$\mathcal{G}_\up^{\text{conv}}$ and $\mathcal{G}_\low^{\text{conv}}$.
	The width $n \in \N$ corresponds to the output dimension of the transposed valid cross-correlation $\star_{\text{v}}^T$
	and thus depends on the kernel size $n_k$ and the stride number $s$.
\end{definition}

The convolution gradient layers are symplectic by construction,
because they are a special parametrization of gradient layers.

\subsection{Normalization}

Batch normalization is a standard method to accelerate training initially proposed by
\citeauthor{batchnorm-ioffe15} in \cite{batchnorm-ioffe15}.
One motivation for batch normalization comes from that the sigmoid activation function saturates 
for large absolute values, which in turn results in vanishing gradients. The motivation of batch
normalization then is to prevent getting into the saturating regime of the sigmoid function
by normalizing the input. In a more general remark, \citet{Santurkar2018} argue that
batch normalization smooths the optimization landscape.
We introduce a possibility to incorporate batch normalization
into activation and gradient layers while maintaining their symplecticity.

Given a mini-batch input $\mathcal{B} = \{  x_1, x_2, \dots, x_{n_{\mathcal{B}}} \} \subset \mathbb{R}^{n}$ 
with size $n_{\mathcal{B}}$, we define the batch normalization transformation as
\begin{equation*}
	\eta_{\gamma, \beta} : \mathbb{R}^n \to \mathbb{R}^n,\quad
	\eta_{\gamma, \beta}(x) 
	:= \lb \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} 
	\lb x_j - \lb \mu_\mathcal{B} \rb_j \rb + \beta_j \rb_{j=1}^n
	.
\end{equation*}
where $n$ is the input dimension of the input $x \in \mathbb{R}^n$ and $\gamma, \beta \in \mathbb{R}^{n}$ 
are learnable parameters.
The scalar $\epsilon \in \mathbb{R}$ is a small positive value to avoid division by zero.
$\sigma^2_\mathcal{B} \in \mathbb{R}^{n}$ refers to the mini-batch variance and
$\mu_\mathcal{B} \in \mathbb{R}^{n}$ refers to the mini-batch mean. The batch normalization results 
in zero mean and unit variance of the whole mini-batch input.

The learnable parameters $\gamma, \beta \in \mathbb{R}^{n}$ allow the neural network to modify
the normalization during training if necessary. The batch normalization transform is able to 
represent the identity transform by setting appropriate $\gamma, \beta$.

The mean $\mu_\mathcal{B} \in \R^n$ and variance $\sigma^2_\mathcal{B} \in \R^n$ are estimated by:
\begin{align*}
	\mu_\mathcal{B} &= \frac{1}{n_{\mathcal{B}}} \sum_{i=1}^{n_{\mathcal{B}}} x_i \\
	\sigma^2_\mathcal{B} &= \frac{1}{n_{\mathcal{B}}} 
	\sum_{i=1}^{n_{\mathcal{B}}} (x_i - \mu_\mathcal{B})^2
\end{align*}

During training the mini-batch variance $\sigma^2_\mathcal{B} \in \mathbb{R}^{n}$ and
the mini-batch $\mu_\mathcal{B} \in \mathbb{R}^{n}$ are continously updated in the forward pass.
To be precise, for a new mini-batch input
$\mathcal{B} = \{ x_1, x_2, \dots, x_{n_{\mathcal{B}}} \} \subset \R^n$, 
the mean $\mu_\mathcal{B}$ and variance $\sigma^2_\mathcal{B}$
are updated based on $\mathcal{B}$, before $\eta_{\gamma, \beta}(x_k)$ for
$x_k \in \mathcal{B}$ is evaluated (see Algorithm 1).
When training has finished the neural network does not update $\sigma_\mathcal{B}^2$ 
and $\mu_\mathcal{B}$ anymore. Instead, the neural network remembers the last value from training.

Note that the batch normalization transformation may be incorporated into a layer
deep inside a neural network. If this is the case, the mini-batch input $\mathcal{B}$ 
refers to the collective output of the previous layer.

\begin{algorithm}\label{algo_batch_norm}
	\caption{Batch normalization transform}
	\textbf{Input:} mini batch $\mathcal{B} = \{x_1, x_2, \dots, x_{n_\mathcal{B}}\} \subset \R^n$ 
	and $x_k \in \mathcal{B}$ \\
	\textbf{Output:} $\eta_{\gamma, \beta}(x_k) \in \R^n$
	\setstretch{1.5}
	\begin{algorithmic}
		\If{training\_mode} \Comment{Update mean and variance if training, otherwise
		keep previous values}
			\State $\mu_\mathcal{B} \gets \frac{1}{n_\mathcal{B}} \sum_{i=1}^{n_\mathcal{B}} x_i$
			\State $\sigma^2_\mathcal{B} \gets \frac{1}{n_\mathcal{B}} \sum_{i=1}^{n_\mathcal{B}} 
			(x_i - \mu_\mathcal{B})^2$
		\EndIf
		\State \Return 
		$\lb \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} 
		\lb (x_k)_j - \lb \mu_\mathcal{B} \rb_j \rb + \beta_j \rb_{j=1}^n$
	\end{algorithmic}
\end{algorithm}

The batch normalization transform can be implemented in a straightforward way with modern
neural network libraries, as during training the Jacobian $\jac{\eta_{\gamma, \beta}}{(\gamma, \beta)}$ 
is obtained via automatic differentiation.

If the training data is small enough, so that splitting the data into multiple mini batches
is not necessary, the mean and variance are calculated for the whole training data set. This is the
case for our numerical experiments, as we work with very small training data sets.

\subsubsection{Normalized gradient layers}

In this section, we incorporate the batch normalization transform into gradient layers.

\begin{definition}\label{def_norm_gradient_layers_1}
	(Normalized gradient layers - Variant 1)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \activation
		\lb \overline{p}_j \rb \bigg)_{j=1}^n
		\quad \text{with } \, \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	the upper and lower normalized gradient layers 
	$\mathcal{G}_\up^{\eta}$ and $\mathcal{G}_\low^{\eta}$ (variant 1).
\end{definition}

\begin{definition}\label{def_norm_gradient_layers_2}
	(Normalized gradient layers - Variant 2)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon}
		\activation \lb \overline{p}_j \rb \bigg)_{j=1}^n
		\quad \text{with } \, \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	the upper and lower normalized gradient layers 
	$\mathcal{G}_\up^{\eta}$ and $\mathcal{G}_\low^{\eta}$ (variant 2).
	The variance $\sigma^2_\mathcal{B} \in \R^n$ refers to the mini-batch variance of $Kp+c$.
\end{definition}

Here, $K \in \mathbb{R}^{n \times d}$ and $a,c, \gamma, \beta \in \mathbb{R}^n$
are learnable parameters.
It turns out in our numerical experiments that variant 1 outperforms
variant 2 or is equally good as variant 2 in almost all cases.

\begin{corollary}\label{cor_norm_gradient_layers_symp}
	Given an activation function $\sigma \in C^1(\R)$,
	the normalized gradient layers $\mathcal{G}^{\eta}_{up}$ and $\mathcal{G}^{\eta}_{down}$
	(variant 1 and variant 2) are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. For arbitrary $\alpha \in \R^n$, we define the potential
	\begin{equation*}
		V(p) := \sum_{j=1}^n \alpha_j a_j \mathcal{A}(\overline{p}_j)
		\quad \text{with } \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
		.
	\end{equation*}
	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $i=1, \dots, d$ follows
	\begin{align*}
		(\grad{V(p)})_i &= \deldel{p_i} \lb \sum_{j=1}^n \alpha_j a_j \mathcal{A}(\hat{p}_j) \rb
		= \sum_{j=1}^n \alpha_j a_j \activation(\overline{p}_j) \deldel{p_i} \overline{p}_j
		= \sum_{j=1}^n \alpha_j a_j \activation(\overline{p}_j)
		\frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} \underbrace{K_{ji}}_{=K^T_{ij}} \\
		%
		\implies
		\grad{V(p)}
		&= K^T \bigg( \alpha_j a_j \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon}
		\activation \lb \overline{p}_j \rb \bigg)_{j=1}^n
		.
	\end{align*}
	If we choose $\alpha_j = 1$, it follows $\grad{V(p)} = \layertf(p)$ for variant 2.
	If we choose $\alpha_j = \frac{(\sigma^2_\mathcal{B})_j + \epsilon}{\gamma_j}$,
	it follows $\grad{V(p)} = \layertf(p)$ for variant 1.
	With \cref{gradient_corollary} follows that both normalized Gradient layer variants
	$\mathcal{G}_{up}^\eta$ and $\mathcal{G}_{low}^\eta$ are symplectic.
\end{proof}

\subsubsection{Normalized convolution gradient layers}\label{sec_norm_conv_gradient_layer}

Analogously to the convolution gradient layers introduced in \cref{sec_conv_gradient_layer},
we can implement a normalized convolution gradient layer (variant 1 and variant 2) 
with width $n \in \mathbb{N}$
using valid cross-correlation (with stride) for $K^T \in \mathbb{R}^{d \times n}$ and the
corresponding transposed valid cross-correlation (with stride) for $K \in \mathbb{R}^{n \times d}$.
The layer transform $\layertf$ for the normalized version (variant 1) of the convolution gradient
layers then can be written as
\begin{equation*}
	T(p) = \star_{\text{v}} \lb k, \, \lb a_j \activation \lb \overline{p}_j \rb \rb_{j=1}^n \rb 
	\quad
	\text{with } \overline{p} = 
	\eta_{\gamma, \beta} \lb \underbrace{\star_{\text{v}}^T(k,p) + c}_{:= \tilde{p}} \rb
	\text{, }\, c = \star_{\text{v}}^T(\tilde{c},1_d) 
	\text{ and } a = \star_{\text{v}}^T(\tilde{a},1_d)
	.
\end{equation*}
The variant 2 follows analogously by inserting the factor 
$\frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon}$ at the respective place.
It is reasonable that the normalization respects the convolution in a certain way.
This also involves calculating the mean and variance in a way that the convolution is respected.

In our numerical experiments, we only use normalized convolution gradient layers with equal 
kernel size $n_k$ and stride number $s$, i.e. $n_k = s$. For this special configuration,
the output dimension of the transposed cross-correlation is $n_k d$ and
the vector $\tilde{p} \in \R^{n_k d}$, which is the input for the
batch normalization transform, can be expressed as
\begin{equation*}
	\tilde{p}_i = \lb \star_{\text{v}}^T(k,p) + \star_{\text{v}}^T(\tilde{c},1_d) \rb_i 
	= \begin{cases}
		k_i p_1 + \tilde{c}_i &: 1 \leq i \leq n_k \\
		k_{i - n_k} p_2 + \tilde{c}_{i - n_k} &: n_k+1 \leq i \leq 2n_k \\
		k_{i - 2n_k} p_3 + \tilde{c}_{i - 2n_k} &: 2n_k+1 \leq i \leq 3n_k \\
		\dots
	\end{cases} \quad (1 \leq i \leq n_k d)
	.
\end{equation*}
So the transposed cross-correlation results in a upscaled vector $\tilde{p} \in \R^{n_k d}$ with blocks of size $n_k$.
Therefore, we implement the batch normalization transform 
$\overline{p} = \eta_{\gamma, \beta}(\tilde{p})$ in such a way that $\tilde{p}_i$ is
normalized the same as $\tilde{p}_{i+n_k}$ for all $i = 1, \dots, n_k (d-1)$. In particular, this means that
$\lb \mu_\mathcal{B} \rb_i = \lb \mu_\mathcal{B} \rb_{i+n_k}$, 
$\lb \sigma^2_\mathcal{B} \rb_i = \lb \sigma^2_\mathcal{B} \rb_{i+n_k}$, 
$\gamma_i = \gamma_{i+n_k}$ and $\beta_i = \beta_{i+n_k}$ for $i = 1, \dots, n_k (d-1)$
and that the parameters $\gamma$ and $\beta$ can be parametrized with vectors
$\tilde{\gamma} \in \R^{n_k}$ and $\tilde{\beta} \in \R^{n_k}$. All respective entries of $\tilde{p} \in \R^{n_k d}$ 
belonging together are used to compute the mean and variance.
As before, the mean and variance are computed over all training samples in the mini-batch $\mathcal{B}$.
Similar normalization schemes can be constructed for other configurations of the 
kernel size $n_k$ and stride value $s$.

\subsubsection{Normalized activation layers}

\begin{definition}\label{def_norm_activation_layer_1}
	(Normalized activation layers - Variant 1)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \activation(\hat{p}_i) \rb_{i=1}^d
		\quad \text{with } \hat{p} = \eta_{\gamma, \beta} \lb p \rb \in \R^d
	\end{equation*}
	the normalized upper and lower activation layers $\mathcal{N}_\up^\eta$ and $\mathcal{N}_\low^\eta$
	(variant 1).
\end{definition}

\begin{definition}\label{def_norm_activation_layer_2}
	(Normalized activation layers - Variant 2)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \frac{\gamma_i}{(\sigma^2_\mathcal{B})_i + \epsilon} 
		\activation(\hat{p}_i) \rb_{i=1}^d
		\quad \text{with } \hat{p} = \eta_{\gamma, \beta} \lb p \rb \in \R^d
	\end{equation*}
	the normalized upper and lower activation layers $\mathcal{N}_\up^\eta$ and $\mathcal{N}_\low^\eta$
	(variant 2).
	The variance $\sigma^2_\mathcal{B} \in \R^d$ refers to the mini-batch variance of $p$.
\end{definition}

The coefficients $a \in \mathbb{R}^d$ are learnable parameters.

\begin{corollary}
	Given an activation function $\sigma \in C^1(\R)$,
	the normalized activation layers $\mathcal{N}^{\eta}_{up}$ and $\mathcal{N}^{\eta}_{low}$
	(variant 1 and variant 2) are symplectic.
\end{corollary}
\begin{proof}
	Symplecticity follows because the normalized activation layers are a special case
	of normalized gradient layers (choose $n=d$, $K=I_d$ and $c=0$ for normalized gradient layers). 
	We have already shown that both normalized gradient layer variants are symplectic in
	\cref{cor_norm_gradient_layers_symp}.
\end{proof}

\section{Relation to geometric integrators}\label{sec_relation_to_geometric_integrators}

The symplectic Euler and Störmer-Verlet schemes are two geometric integrators. Geometric integrators
are numerical integrators for ODE systems, which preserve a geometric property. In our context,
this geometric property is symplecticity. Precisely, if we denote the numerical integrator with
$\phi^{\text{h}}_{t,H} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$, the map $\phi^{\text{h}}_{t,H}$ is
symplectic.

The two variants of the symplectic Euler scheme are given by:
\begin{equation*}
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H} (p_{n+1}, q_n) \\
			q_{n+1} &= q_n + h \grad[p]{H}(p_{n+1}, q_n)	
	\end{split}
	\quad\quad \text{or} \quad\quad
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H}(p_n, q_{n+1}) \\
			q_{n+1} &= q_n + h \grad[p]{H}{p}(p_n, q_{n+1})	
	\end{split}
\end{equation*}

The $p$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	p_{n+1/2} &= p_n - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_n) \\[7pt]
	q_{n+1} &= q_n + \frac{h}{2} \lb \grad[p]{H}(p_{n+1/2}, q_n) + \grad[p]{H}(p_{n+1/2}, q_{n+1}) \rb \\[7pt]
	p_{n+1} &= p_{n+1/2} - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_{n+1})
\end{align*}
and the $q$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	q_{n+1/2} &= q_n + \frac{h}{2} \grad[p]{H}(p_n, q_{n+1/2}) \\[7pt]
	p_{n+1} &= q_n - \frac{h}{2} \lb \grad[q]{H}(p_n, q_{n+1/2}) + \grad[q]{H}(p_{n+1}, q_{n+1/2}) \rb \\[7pt]
	q_{n+1} &= q_{n+1/2} + \frac{h}{2} \grad[p]{H}(p_{n+1}, q_{n+1/2})
	.
\end{align*}
We refer to \citet[p.~189 and p.~190]{hairer2006} for details.

If the Hamiltonian $H$ is separable, i.e. $H(q,p) = U(q) + V(p)$, the symplectic Euler and Störmer-Verlet
schemes become explicit. Then given $\grad[p]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $p \mapsto \grad[p]{H}(p)$ 
and $\grad[q]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $q \mapsto \grad[q]{H}(q)$,
the left variant of the symplectic Euler scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix} \begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix} 
\end{equation*}
and the right variant of the symplectic Euler scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
	.
\end{equation*}

Similarly, the $p$-staggered Störmer-Verlet scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & 0 \\
		-\frac{h}{2} \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{bmatrix}
		id & 0 \\
		-\frac{h}{2} \grad[q]{H} & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
and the $q$-staggered Störmer-Verlet scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & \frac{h}{2} \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & \frac{h}{2} \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
	.
\end{equation*}

To conclude, the explicit Euler and Störmer-Verlet schemes can be expressed with
the same unit triangular structure we use for SympNets. This means that a SympNet is able
to reproduce both schemes exactly or approximately, provided appropriate layer choices.

Symplecticity for the explicit Euler and Störmer-Verlet schemes follows directly 
from \cref{gradient_corollary} and the fact that the composition of symplectic maps is again symplectic.

\section{Numerical experiments}\label{sec_numerical_experiments}

In this section, we compare the performance of different SympNet architectures 
for predicting flows of Hamiltonian systems.
Additionially, we compare SympNets to network architectures which do not intrinsically
conserve the symplectic property.
After the neural networks have been trained with a certain training set,
we use them as numerical integrators
to predict a trajectory for a given initial value $y_0$ of a Hamiltonian ODE.
A single evaluation of the neural network then represents one time iteration.

In all our experiments, we use the mean-square error (MSE) loss defined by
\begin{equation*}
	L(\mathcal{B}, \Theta) = \sum_{(x_i, y_i) \in \mathcal{B}} \norm{\Phi(x_i; \Theta) - y_i}^2_2
	,
\end{equation*}
where $\mathcal{B}$ refers to a mini-batch and $\Theta$ refers to the neural network parameters.
We work with very small training data. Thus, we do not split the training data into mini-batches.
We initialize all bias parameters with $0$. All other parameters are initialized randomly from
the normal distribution with mean $0$ and standard deviation $0.01$.
For training, we use the AMSgrad variant of the Adam optimizer \cite{amsgrad2018}.
The standard Adam optimizer led to large loss spikes at later epochs and often
failed to converge to stable parameter values. This phenomenon
was greatly reduced by switching to the AMSgrad variant. The Adam alogrithm (and the AMSgrad variant)
computes individual adaptive learning rates for different parameters, based on a baseline
learning rate hyperparameter $\gamma \in \R$.

In all our experiments the training and test data are subsets of $\mathcal{V} \times \mathcal{V}$,
where $\mathcal{V} = \R^{2d}$ denotes the phase space of the Hamiltonian system. In particular,
given some phase space samples $x_1, \dots, x_n \in \mathcal{V}$, we generate data samples 
$\mathcal{T} \subset \mathcal{V} \times \mathcal{V}$
with the $q$-staggered Störmer-Verlet integrator $\phi^{\text{h}}_{t,H}$
for a fixed time $t \in \R$ and the respective Hamiltonian $H$:
\begin{equation*}
	\mathcal{T} = \{ (x_i, \phi^{\text{h}}_{t,H}(x_i)) \}_{i=1}^{n_{\text{train}}}
\end{equation*}
The data samples $\mathcal{T}$ are split into a training data set $\mathcal{T}_{\text{train}}$
and $\mathcal{T}_{\text{test}}$, i.e. $\mathcal{T}_{\text{train}} \cup \mathcal{T}_{\text{test}} = \mathcal{T}$
and $\mathcal{T}_{\text{train}} \cap \mathcal{T}_{\text{test}} = \emptyset$. 
We denote the size of the training data set with $n_{\text{train}} \in \N$ and the size of
the test data set with $n_{\text{test}} \in \N$.
The training data
$\mathcal{T}_{\text{train}}$ and test data $\mathcal{T}_{\text{test}}$ are used to 
calculate the training and test loss.

\subsection{Low-dimensional systems}

In this section, we deal with low-dimensional Hamiltonian systems with $d=1$.
The neural network architectures used in the experiments with low-dimensional systems
are listed in \cref{table_low_dim_arch}.
\begin{table}
	\centering
	\begin{tabular}{lp{8cm}c}
		\toprule Architecture & Layers & Parameters \\
		\midrule L-SympNet & Nine alternating upper and lower linear layers,
		with bias in last layer, see \cref{def_linear_layer}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $11$ \\
		%
		FNN & 
		A fully-connected network with overall input and output dimension $2$. The network has one hidden
		(fully-connected) linear layers with input and output dimension $50$.
		Nonlinear activation layers are placed between all linear layers.
		& $2802$ \\
		%
		LA-SympNet & LA-SympNet with depth $5$ and $4$ sublayers, see \cref{def_la_sympnet}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $41$ \\
		%
		N1-LA-SympNet & 
		Same as LA-SympNet, but with normalized activation layers (variant 1, 
		see \cref{def_norm_activation_layer_1}) instead of activation layers.
		& $51$ \\
		%
		N2-LA-SympNet & 
		Same as LA-SympNet, but with normalized activation layers (variant 2, 
		see \cref{def_norm_activation_layer_2}) instead of activation layers.
		& $51$ \\
		%
		G-SympNet & G-SympNet with depth $4$ and width $30$, see \cref{def_g_sympnet}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $360$ \\
		%
		N1-G-SympNet & 
		Same as G-SympNet, but with normalized Gradient layers 
		(variant 1, see \cref{def_norm_gradient_layers_1}) instead of Gradient layers.
		& $600$ \\
		%
		N2-G-SympNet & 
		Same as G-SympNet, but with normalized Gradient layers 
		(variant 2, see \cref{def_norm_gradient_layers_2}) instead of Gradient layers. 
		& $600$ \\
		%
		LARGE-FNN &
		A fully-connected network with overall input and output dimension $2$. The network has two hidden
		(fully-connected) linear layers with input and output dimension $70$.
		Nonlinear activation layers are placed between all linear layers.
		& $10292$ \\
		%
		LARGE-LA-SympNet & LA-SympNet with depth $40$ and $9$ sublayers, see \cref{def_la_sympnet}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $491$ \\
		%
		LARGE-N1-LA-SympNet & 
		Same as LARGE-LA-SympNet, but with normalized activation layers (variant 1, 
		see \cref{def_norm_activation_layer_1}) instead of activation layers.
		& $571$ \\
		%
		LARGE-N2-LA-SympNet & 
		Same as LARGE-LA-SympNet, but with normalized activation layers (variant 2, 
		see \cref{def_norm_activation_layer_2}) instead of activation layers.
		& $571$ \\ \bottomrule
	\end{tabular}
	\caption{
	Table of all neural network architectures for the
	low-dimensional experiments. The activation function used inside the activation
	and gradient layers is either sigmoid, tanh or ELU and is explicitly stated
	in the respective experiment.
	}\label{table_low_dim_arch}
\end{table}

For all low-dimensional experiments, the training and test data are generated
with the fixed time $t = 0.1$
from phase space samples uniformly sampled from a compact set $\mathcal{D} \subset \R^2$.

\subsubsection{Harmonic Oscillator}

For $q,p \in \R$, the Hamiltonian for the Harmonic Oscillator is given by
\begin{equation*}
	H(q,p) = \frac{p^2}{2m} + \frac{1}{2} kq^2
	.
\end{equation*}
For the experiment, we choose $m=1$ and $k=1$.

The flow for the Harmonic Oscillator is linear, because the resulting ODE
is linear. Thus, the ODE can be solved via the variation of constants formula,
which results in a linear flow.
As we have mentioned,
\citeauthor{jin2020unit} have shown in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map.
Consequently, a SympNet consisting out of nine alternating 
upper and lower linear layers should be able to learn the flow for the Harmonic Oscillator.
Indeed, \cref{fig_harm_osc_loss} shows that such a linear SympNet successfully 
learns the flow for the Harmonic Oscillator with a very low test loss.

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{axis}[
		ymode=log,
		no markers,
		title={Test loss (Harmonic Oscillator)},
		xlabel={Epoch}, xmin=0, xmax=500,
		ylabel={Test loss},
		legend entries={
			L-SympNet
		}
	]

	\addplot[
		color=orange
	] table[x=epoch,y=loss,col sep=comma] {../data/harmonic_oscillator/l-sympnet/sigmoid/test_loss.csv};
		
	\end{axis}
\end{tikzpicture}
\caption{Test loss for Harmonic Oscillator ($500$ epochs). The L-SympNet consists out of nine alternating upper and 
lower linear layers. The training and test data are generated from random phase space samples in 
$\mathcal{D} = [-2,2] \times [-2,2]$. The training data size is $n_{\text{train}} = 40$
and the test data size is $n_{\text{test}}=400$.
The learning rate is set to $\gamma = 0.01$.}\label{fig_harm_osc_loss}
\end{figure}

\subsubsection{Simple Pendulum}

For $q,p \in \R$, the Hamiltonian for the Simple Pendulum is given by
\begin{equation*}
	H(q,p) = \frac{p^2}{2ml^2} + mgl (1-\cos(q))
	.
\end{equation*}
For the experiment, we choose $m=1, g=1$ and $l=1$. The flow of the Simple Pendulum is nonlinear.
Therefore, the flow cannot be represented by linear models, which makes the Simple Pendulum
a good experiment to look into. The generalized coordinate $q$ denotes the angle of the pendulum.

We conduct two different experiments with different
phase space samples originating from two different sets $\mathcal{D}$:
\begin{align*}
	\mathcal{D}_{\text{swing}} &= [-\frac{\pi}{2}, \frac{\pi}{2}] \times [-\sqrt{2}, \sqrt{2}]
	\quad (\text{swinging}) \\
	\mathcal{D}_{\text{swing+rotate}} &= [-20, 20] \times [-2.5, 2.5]
	\quad (\text{swinging and rotating})
\end{align*}
Physically, $\mathcal{D}_{\text{swing}}$ mostly contains phase spaces states where
the pendulum is swinging, in contrast to $\mathcal{D}_{\text{swing+rotate}}$.
We choose $n_{\text{train}} = 40$ and $n_{\text{test}} = 400$ for all experiments involving
the Simple Pendulum. The test data is sampled from the same compact set $\mathcal{D}$ as the training data.

\cref{fig_pend_exp1_loss} and \cref{{table_pend_exp1_loss}} show the training and test loss for the different architectures
and activation functions with training and test data generated from $\mathcal{D}_{\text{swing}}$ and learning rate $\gamma = 0.01$.
Note that incorporating normalization results in better training and test losses, and may also stabilize
the learning process. The SympNets do not tend to overfit.
The N1-G-SympNet with tanh activation function can even compete with the LA-SympNets, whereas
without normalization the G-SympNet is not able to do so.
In contrast, the fully-connected networks (FNN) tend to overfit, because they have a high test loss (right column) 
compared to the training loss (left column).
\cref{fig_pend_exp1_phase_plots} and \cref{fig_pend_exp1_time} show phase plots, the total energy over time
and the predicted position over time for the two best-performing architectures and the FNN.
Note that no architecture is able to successfully predict the phase flow for a initial value outside the training data,
i.e. $(q_0, p_0) \notin \mathcal{D}_{\text{swing}}$.
The total energy is maintained with the SympNets, in contrast to the FNN.

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[	
		group style={
			group size=2 by 3,
			y descriptions at=edge left,
			horizontal sep=0.1cm,
			vertical sep=2.2cm,
		},
		xlabel=Epoch, ylabel=Loss,
		width=\axisdefaultwidth, height=7cm,
		no markers,
		ymax=1e-1, ymin=5e-10, ymode=log,
		legend style={nodes={scale=0.75, transform shape}},
		legend entries={
			FNN,
			LA-SympNet,
			N1-LA-SympNet,
			N2-LA-SympNet,
			G-SympNet,
			N1-G-SympNet,
			N2-G-SympNet
		}
	]
		\nextgroupplot[title={Training loss (sigmoid activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/sigmoid/loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=purple, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/sigmoid/loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/sigmoid/loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/sigmoid/loss.csv};

		\nextgroupplot[title={Test loss (sigmoid activation)},]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/sigmoid/test_loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=purple, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/sigmoid/test_loss.csv};

		\nextgroupplot[title={Training loss (tanh activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/tanh/loss.csv};

			\addplot[
				color=purple, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/tanh/loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/tanh/loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/tanh/loss.csv};

		\nextgroupplot[title={Test loss (tanh activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/test_loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=purple, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/tanh/test_loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/test_loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/tanh/test_loss.csv};

		\nextgroupplot[title={Training loss (ELU activation)}]

			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/elu/loss.csv};
		
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/elu/loss.csv};
		
			\addplot[
				color=purple, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/elu/loss.csv};
		
			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/elu/loss.csv};
		
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/elu/loss.csv};
		
			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/elu/loss.csv};
		
			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/elu/loss.csv};

		\nextgroupplot[title={Test loss (ELU activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/elu/test_loss.csv};
		
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=purple, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/elu/test_loss.csv};
		
	\end{groupplot}
\end{tikzpicture}
\caption{Training and test loss for simple pendulum with training and test data 
generated from $\mathcal{D}_{\text{swing}}$.
(First row) sigmoid activation (Second row) tanh activation (Third row) ELU activation.
Normalization leads to improved performance.}\label{fig_pend_exp1_loss}
\end{figure}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss (learning rate $\gamma = 0.01$)}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data/simple_pendulum_swing/test_loss_summary.csv}

	\caption{Test losses for Simple Pendulum 
	with training and test data generated from $\mathcal{D}_{\text{swing}}$ after $10^5$ epochs. 
	}\label{table_pend_exp1_loss}
\end{table}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 1,
				horizontal sep=2cm
			},
			width=7.7cm,
			xlabel=$q$, ylabel=$p$,
			xmajorgrids
		]
			
		\nextgroupplot[
			title={Phase plot for $q_0 = \frac{\pi}{2},\, p_0 = 0$},
			legend entries={
				FNN (tanh),
				N1-LA-SympNet (tanh),
				N1-G-SympNet (tanh),
				Störmer-Verlet
			},
			legend columns=4,
			legend to name=leg_ped_swing_phase
		]
			\addplot[
				color=green
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=purple
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=olive
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/swinging_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/phase_plot.csv};
	
		\nextgroupplot[title={Phase plot for $q_0 = \pi,\, p_0=1$}, ymax=6]
			\addplot[
				color=green
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/rotating_case/phase_plot.csv};

			\addplot[
				color=purple
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/rotating_case/phase_plot.csv};
	
			\addplot[
				color=olive
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/rotating_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/exact/rotating_case/phase_plot.csv};
	
		\end{groupplot}
	\end{tikzpicture}
	\\[5pt]
	\ref{leg_ped_swing_phase}
	\caption{Phase plots for simple pendulum. Training and test data generated from $\mathcal{D}_{\text{swing}}$.
	(Left) The phase plots of the SympNets and the exact solution are indistinguishable.
	(Right) The neural networks fail to generalize for a $(q_0, p_0) \notin \mathcal{D}_{\text{swing}}$.}
	\label{fig_pend_exp1_phase_plots}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 1,
				horizontal sep=2cm
			},
			no markers,
			xlabel={t},
			width=7.7cm,
			legend style={nodes={scale=0.75, transform shape}},
			legend entries={
				Störmer-Verlet,
				FNN (tanh),
				N1-LA-SympNet (tanh),
				N1-G-SympNet (tanh)
			}
		]

		\nextgroupplot[title={Total energy $H$}, ylabel={$H(t)$}, xmin=0, xmax=100]
			\addplot[
				color=darkgray
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/total_energy.csv};

			\addplot[
				color=green
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/swinging_case/total_energy.csv};

			\addplot[
				color=purple,densely dashed
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/swinging_case/total_energy.csv};

			\addplot[
				color=olive,densely dashed
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/swinging_case/total_energy.csv};

		\nextgroupplot[title={Predicted position $q$}, ylabel={$q(t)$}, xmin=91, xmax=97, legend pos=north west]
			\addplot[
				color=darkgray
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/q.csv};

			\addplot[
				color=green
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/swinging_case/q.csv};

			\addplot[
				color=purple,densely dashed
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/swinging_case/q.csv};

			\addplot[
				color=olive,densely dashed
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/swinging_case/q.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{Total energy and position over time for simple pendulum for $q_0 = \frac{\pi}{2},\, p_0 = 0$.
	Training and test data generated from $\mathcal{D}_{\text{swing}}$.
	The total energy is maintained with the SympNets, in contrast to the FNN.
	The prediction of the position over long time is better with SympNets than with the FNN.}
	\label{fig_pend_exp1_time}
\end{figure}

\cref{fig_pend_exp2_loss} shows the test losses if the training and test data
is generated from $\mathcal{D}_{\text{swing+rotate}}$ instead.
We perform the experiment with learning rate $\gamma = 0.01$ and faster learning rate $\gamma = 1.0$.
\cref{table_pend_exp2_loss} shows the final test loss for the first learning rate $\gamma = 0.01$.
\cref{table_pend_exp2_loss_fast_lr} shows the final test loss for the faster learning rate $\gamma = 1.0$.
Compared to to the previous case with training data from $\mathcal{D}_{\text{swing}}$,
the chosen activation function has a greater influence on model performance.
The LARGE-LA-SympNet and the normalized variants LARGE-N1-LA-SympNet and LARGE-N2-LA-SympNet completely fail to learn the
flow if the training data is generated from $\mathcal{D}_{\text{swing+rotate}}$, 
although their depth has been increased such that they have the same amount of parameters 
as the G-SympNets (see \cref{table_low_dim_arch}). So the in general good performance of the 
LA-SympNets in the previous case with training data from $\mathcal{D}_{\text{swing}}$ 
does not transfer to this case with training data from $\mathcal{D}_{\text{swing+rotate}}$.
For training data from $\mathcal{D}_{\text{swing}}$, the SympNets
do not work well with the ELU activation function, whereas the FNN with ELU activation still achieves 
a comparatively low loss for the slower learning rate $\gamma = 0.01$ (see the right plot in the
first row of \cref{fig_pend_exp2_loss}).

For training and test data generated from $\mathcal{D}_{\text{swing+rotate}}$, 
normalization does not lead to a significant change in prediction performance, compared to
what we observed with $\mathcal{D}_{\text{swing}}$. For $\mathcal{D}_{\text{swing+rotate}}$,
the G-SympNet with sigmoid activation performs best overall
(see left plot in first row of \cref{fig_pend_exp2_loss}). Still, normalization slightly improves the
performance for the tanh and ELU activation (see 
mid and right plot in first row of \cref{fig_pend_exp2_loss}).
Furthermore, normalization shows more robustness with respect to faster learning rates
(see second row in \cref{fig_pend_exp2_loss}). The G-SympNet without normalization fails to converge
for the faster learning rate $\gamma = 1.0$, in contrast to the N1-G-SympNet and N2-G-SympNet.

The generalized coordinate $q$ is an angular coordinate for the simple pendulum. 
This is the reason why $\lim_{t \to \infty} q(t) = \infty$
in the rotating case. This circumstance makes learning the neural network hard, especially if the
goal is to have a good prediction over a long time.
The Störmer-Verlet scheme for the Simple Pendulum contains the sine function.
Consequently, the neural networks have to learn a representation for multiple periods of 
the sine function, opposed to the swinging case where it suffices to learn the sine function 
on $[-\pi/2, \pi/2]$. This could be an explanation why the good performance 
of the LA-SympNets for the swinging case does not transfer to the rotating case, because
learning a sine function with the structure of LA-SympNets is much more challenging as with the
structure of the G-SympNets. With G-SympNets, a single gradient layer is able to represent
the sine function on a compact set (see \citet[Lemma 4]{Jin2020}), whereas with the LA-SympNets 
the linear layers need to work together with the nonlinear activation layers to do so.
Furthermore, intuitively, the large values of $q$ in the rotating case could mask the swinging case, 
where $\abs{q} \leq \pi/2$. This might be an explanation for why normalization does not lead to a 
similar performance improvement for the swinging plus rotating case,
as with the swinging-only case. This issue needs further investigation.
We comment this issue in the Outlook (\cref{sec_outlook}).

For almost all experiments, the first variant of the normalization layers (N1) performs better or equally good
to the second variant of the normalization layers (N2). There are only two cases where the N2 normalization variant
has a slightly better final test loss than the N1 normalization variant 
(see \cref{fig_pend_exp2_loss}, right plot in first row and mid plot in second row).

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[
		group style={
			group size=3 by 2,
			y descriptions at=edge left,
			horizontal sep=0.1cm,
			vertical sep=2.2cm,
		},
		xlabel=Epoch, ylabel={Test loss},
		width=6.3cm, height=7cm,
		no markers,
		ymax=1e-2, ymin=5e-7, ymode=log
	]
	
		\nextgroupplot[
			title={Sigmoid ($\gamma = 0.01$)},
			legend entries={
				LARGE-FNN,
				LARGE-LA-SympNet,
				LARGE-N1-LA-SympNet,
				LARGE-N2-LA-SympNet,
				G-SympNet,
				N1-G-SympNet,
				N2-G-SympNet
			},
			legend columns=4,
			legend to name=grouplegend
		]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=purple,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n1-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=orange,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n2-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/sigmoid/test_loss.csv};
	
		\nextgroupplot[title={tanh ($\gamma = 0.01$)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/tanh/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=purple,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n1-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=orange,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n2-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/tanh/test_loss.csv};
	
		\nextgroupplot[title={ELU ($\gamma = 0.01$)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/elu/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=purple,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n1-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=orange,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n2-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=olive,,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/elu/test_loss.csv};

		%%% FROM HERE: fast learning rate
		\nextgroupplot[title={Sigmoid ($\gamma = 1.0$)}]
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n1-g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n2-g-sympnet/sigmoid/test_loss.csv};
	
		\nextgroupplot[title={tanh ($\gamma = 1.0$)}]	
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n1-g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n2-g-sympnet/tanh/test_loss.csv};
	
		\nextgroupplot[title={ELU ($\gamma = 1.0$)}]
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n1-g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n2-g-sympnet/elu/test_loss.csv};
				
	\end{groupplot}
\end{tikzpicture}
\\[5pt]
\ref{grouplegend}
\caption{Test loss for simple pendulum with training and test data from $\mathcal{D}_{\text{swing+rotate}}$.
(First row) Learning rate $\gamma = 0.01$. (Second row) Faster learning rate $\gamma = 1.0$.
Test losses bigger than $10^{-2}$ and architectures that diverged during training are not displayed.
}\label{fig_pend_exp2_loss}
\end{figure}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss (learning rate $\gamma = 0.01$)}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data/simple_pendulum_swing_rot/test_loss_summary.csv}

	\caption{Test losses for Simple Pendulum with training and test data
	generated from $\mathcal{D}_{\text{swing+rotate}}$ after $3 \cdot 10^5$ epochs.}
	\label{table_pend_exp2_loss}
\end{table}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss (learning rate $\gamma = 1.0$)}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data/simple_pendulum_swing_rot_fast_lr/test_loss_summary.csv}

	\caption{Test losses for simple pendulum after $3 \cdot 10^5$ epochs
	with training and test data generated from $\mathcal{D}_{\text{swing+rotate}}$
	and faster learning rate $\gamma = 1.0$.
	NaN means that the test loss diverged. Only G-SympNets with normalization
	are able to handle the faster learning rate.}
	\label{table_pend_exp2_loss_fast_lr}
\end{table}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 1,
				horizontal sep=2cm
			},
			width=7.7cm,
			xlabel=$q$, ylabel=$p$,
			xmajorgrids
		]
			
		\nextgroupplot[
			title={Phase plot for $q_0 = \frac{\pi}{2},\, p_0 = 0$},
			legend entries={
				LARGE-FNN (sigmoid),
				LARGE-LA-SympNet (tanh),
				G-SympNet (sigmoid),
				Störmer-Verlet
			},
			legend columns=4,
			legend to name=leg_ped_swing_rot_phase
		]
			\addplot[
				color=green,thin
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/swinging_case/phase_plot.csv};
	
			\addplot[
				color=red
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=blue
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/swinging_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/phase_plot.csv};
	
		\nextgroupplot[title={Phase plot for $q_0 = \pi,\, p_0=1$}]
			\addplot[
				color=green
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/rotating_case/phase_plot.csv};

			\addplot[
				color=red
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/tanh/rotating_case/phase_plot.csv};
	
			\addplot[
				color=blue
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/rotating_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/phase_plot.csv};
	
		\end{groupplot}
	\end{tikzpicture}
	\\[5pt]
	\ref{leg_ped_swing_rot_phase}
	\caption{Phase plots for simple pendulum. Training and test data uniformly sampled from $\mathcal{D}_{\text{swing+rotate}}$.
	(Left) Only the G-SympNet is indistinguishable from the exact phase plot. The FNN does not have a closed orbit.
	(Right) The LA-SympNet fails to predict the phase plot for $q_0 = \pi,\, p_0=1$.}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 2,
				horizontal sep=2cm,
				vertical sep=2cm,
			},
			no markers,
			xlabel={t},
			width=7cm,
			legend style={nodes={scale=0.75, transform shape}},
			legend entries={
				Störmer-Verlet,
				LARGE-FNN (sigmoid),
				G-SympNet (sigmoid)
			}
		]

		%% Swinging case
		\nextgroupplot[title={Total energy $H$ for $q_0 = \frac{\pi}{2},\, p_0 = 0$}, ylabel={$H(t)$}, xmin=0, xmax=100]
			\addplot[
				color=darkgray
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/total_energy.csv};

			\addplot[
				color=green
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/swinging_case/total_energy.csv};

			\addplot[
				color=blue
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/swinging_case/total_energy.csv};

		\nextgroupplot[title={Predicted position $q$ for $q_0 = \frac{\pi}{2},\, p_0 = 0$}, ylabel={$q(t)$}, xmin=91, xmax=97, legend pos=north west]
			\addplot[
				color=darkgray
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/q.csv};

			\addplot[
				color=green
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/swinging_case/q.csv};

			\addplot[
				color=blue
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/swinging_case/q.csv};

		%% Rotating case	
		\nextgroupplot[title={Total energy $H$ for $q_0 = \pi,\, p_0=1$}, ylabel={$H(t)$}, xmin=0, xmax=10]
			\addplot[
				color=darkgray
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/total_energy.csv};

			\addplot[
				color=green
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/rotating_case/total_energy.csv};

			\addplot[
				color=blue
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/rotating_case/total_energy.csv};

		\nextgroupplot[title={Predicted position $q$ for $q_0 = \pi,\, p_0=1$}, ylabel={$q(t)$}, xmin=8, xmax=9, legend pos=north west]
			\addplot[
				color=darkgray
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/q.csv};

			\addplot[
				color=green
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/rotating_case/q.csv};

			\addplot[
				color=blue
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/rotating_case/q.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{
	Training and test data generated from $\mathcal{D}_{\text{swing+rotate}}$.
	The learning rate is set to $\gamma = 0.01$.
	Total energy and position over time for simple pendulum for $q_0 = \frac{\pi}{2},\, p_0 = 0$.
	(first row) and $q_0 = \pi,\, p_0=1$ (second row).
	The total energy is better maintained with the SympNet for $q_0 = \frac{\pi}{2},\, p_0 = 0$.
	The prediction of the position over long time is better with the SympNet than with the FNN
	in both cases.}
\end{figure}

\FloatBarrier
\subsection{High-dimensional systems}

In this section, we work with a high-dimensional Hamiltonian system with $d \gg 1$
arising from a semi-discretized partial differential equation.
Namely,
the one-dimensional semi-linear wave equation with constant speed $c \in \R$, nonlinear term 
$g : \R \to \R$, domain length $l \in \R$ and domain $\Omega = [-l/2, l/2]$
\begin{equation*}
	\frac{\partial^2}{\partial t^2} u(t,x) = 
	c^2 \frac{\partial^2}{\partial x^2} u(t,x) - g(u(t,x)) 
	\quad \forall x \in \Omega, 
	\, t \in I \subset \R
\end{equation*}
with initial conditions
\begin{align*}
	u(t_0,x) &= u_0(x) \\
	\deldelt u(t_0,x) &= w_0(x) \quad \forall x \in \Omega
\end{align*}
and Dirichlet boundary conditions
\begin{equation*}
	u(t,-l/2) = u_{\text{left}}, \, u(t,l/2) = u_{\text{right}} \quad \forall t \in I \subset \R
\end{equation*}
can be transformed into a Hamiltonian ODE system upon discretization with finite differences
(see \cite{2006ham_pde} and \cite{peng2016} for details). 
Here, $u(x,t) \in \R$ denotes the unknown time-dependent displacement field on the domain $\Omega$.

Given an equally-spaced grid $\{ x_i \}_{n=0}^{n+1} \subset \Omega$, $x_0 = -l/2$, $x_{n+1} = l/2$,
with grid width $\Delta x$,
initial values $y_0 = (\bar{q}_0^T, \bar{p}_0^T)^T \in \R^{2n}$, $\bar{q}_0 = \lb u_0(x_i) \rb_{i=1}^n \in \R^n$, 
$\bar{p}_0 = \lb w_0(x_i) \rb_{i=1}^n \in \R^n$,
the resulting Hamiltonian system is given by
\begin{align*}
	\dot{y}(t) &= \frac{J_{2n}}{\Delta x} \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
	y(t_0) &= y_0
\end{align*}
with Hamiltonian
\begin{equation*}
	H(q,p) = \sum_{i=1}^n \Delta x \lb 
	\frac{1}{2} p_i^2 + \frac{c^2 (q_{i+1} - q_i)^2}{4 \Delta x^2} 
	+ \frac{c^2 (q_i - q_{i-1})^2}{4 \Delta x^2} + G(q_i)
	\rb
	\quad (q,p \in \R^n)
	,
\end{equation*}
where $G'(u) = \int_0^u g(v) dv$, $q_0 := u_{\text{left}}$ and $q_{n+1} := u_{\text{right}}$.
The generalized coordinates $q_i(t)$ then refer to $u(t,x_i)$ and 
the conjugate momenta $p_i(t)$ then refer to $\deldelt u(t,x_i)$.
The resulting Hamiltonian system has a large phase space dimension $d=n \gg 1$.
It suggests itself to use convolution layers, since otherwise
the parameter space would explode. It is also physically reasonable
to use convolution layers, because the original PDE describes the system locally.
Thus it makes sense to share parameters on the domain.
The neural network architectures used in the experiments
are listed in \cref{table_high_dim_arch}. With these architectures,
we implicitly assume that the PDE coefficients do not depend on the position variable $x$.
We also assume that the grid points are equidistant. Both conditions are met
in our case with the wave equation. If one or both conditions do not apply, sharing the same
parameters over the whole domain would not work anymore without 
adding further logic, because the current position $x$ then would influence
local system behavior.

In the nonlinear case, we also use that the nonlinearity $g$ 
only depends on $u(t,x)$ at a single point $x$, and not
on values on a neighborhood around $x$. 
This motivates why we set the kernel size and stride number
to the same value for the convolution gradient layers in 
the CG-SympNets (see \cref{table_high_dim_arch}).
If the kernel size is equal to the stride number, the (transposed) cross-correlation
has no overlap, which implies that the grid points do not influence each other.
So the intuition behind the 
proposed CG-Sympnet architecture is that the (linear) convolution layers learn the
linear part, while the nonlinear convolution gradient layers learn the nonlinear part.

In our experiments with the wave equation, the phase spaces samples
used to generate the training and test data originate from the trajectory
starting from $y_0$, i.e. $x_i = y_{i-1} = \phi^{\text{h}}_{t,H}(x_{i-1})$ for
$i = 1, \dots, n_{\text{train}} + n_{\text{test}}$. The data is generated
for the fixed time $t = 0.01$ in all experiments. 
The first $n_{\text{train}}$ samples go to the training data, the remaining samples
to the test data. In that sense, the test loss is an indicator how well 
the model extrapolates in time. The training size
$n_{\text{train}} \in \N$ is chosen such that it contains all data up to
total accumulated time $T=1$, i.e. $n_{\text{train}} = 100$. The test data
contains the trajectory samples for total accumulated time $T=1$ to $T=10$.
All experiments are conducted with the learning rate $\gamma = 0.1$.

\begin{table}
	\centering
	\begin{tabular}{lp{8cm}c}
		\toprule Architecture & Layers & Parameters \\
		\midrule CNN & 
		A standard purely linear convolutional neural network with four $1$-dimensional
		convolution layers with kernel size $3$ and constant zero-padding to keep the input dimension. 
		& $12$ \\
		%
		C-SympNet & A purely linear SympNet consisting out of four alternating upper and lower
		(symplectic) convolution layers with kernel size $3$ and constant zero-padding,
		see \cref{def_conv_layer}. 
		& $8$ \\
		%
		LARGE-CNN &
		A standard convolutional neural network consisting of four consecutive blocks of
		the following layers: $1$-dimensional convolution layer with kernel size $3$
		and symmetric padding, $1$-dimensional transposed convolution layer
		with kernel size $100$ and stride $100$, nonlinear activation layer
		and $1$-dimensional convolution layer with kernel size $100$ and stride $100$. 
		& $809$ \\
		%
		CG-SympNet & 
		A SympNet consisting of four consecutive blocks of
		the following two layers: upper (or lower) $1$-dimensional symplectic convolution layer with
		kernel size $3$ and symmetric padding (see \cref{def_conv_layer}) 
		followed by a upper (or lower) convolution gradient layer
		with kernel size $100$ and stride $100$ (see \cref{sec_conv_gradient_layer}). 
		The four blocks as a whole alternate between
		the upper and lower variants, i.e. a single block in itself contains either only the upper or only
		the lower variants. 
		& $1208$ \\
		%
		N1-CG-SympNet & Same as CG-SympNet, but with normalized convolution gradient layers
		(variant 1, see \cref{sec_norm_conv_gradient_layer}) instead of convolution gradient layers.
		& $2008$ \\
		%
		N2-CG-SympNet & Same as CG-SympNet, but with normalized convolution gradient layers
		(variant 2, see \cref{sec_norm_conv_gradient_layer}) instead of convolution gradient layers.
		& $2008$ \\ \bottomrule
	\end{tabular}
	\caption{Table of all neural network architectures for the
	high-dimensional experiments. The activation function used inside the nonlinear activation layers
	and the convolution gradient layers is either sigmoid, tanh or ELU and is explicitly stated.
	If not stated otherwise, we use the FD basis to parametrize the symmetric kernels 
	inside the symplectic convolution layers.}\label{table_high_dim_arch}
\end{table}

\subsubsection{Linear wave equation}

The first experiment is conducted with the linear wave equation, i.e.
$G(u) = g(u) =0$. We set $c=1, l=1$ and use $100$ grid points for the discretization.
The initial values are set to a bump function
\begin{align*}
	u_0(x) &= \begin{cases}
	\frac{1}{2} \lb 
	\exp \lb - \lb \frac{6x}{w} \rb^2 \rb - \exp \lb - \lb \frac{6w/2}{w} \rb^2 \rb 
	\rb & : \abs{x} \leq w \\
	0 & : \text{else}
	\end{cases} \\
	w_0(x) &= 0
\end{align*}
with $w = l/4$, see first plot in \cref{fig_linear_wave_eq_displacement}.
The Dirichlet boundary values are set to $u_{\text{left}} = u_{\text{right}} = 0$.
So this corresponds to the linear wave equation transport problem with fixed ends.
As with the Harmonic Oscillator, the resulting Hamiltonian ODE is linear and thus the
flow is linear. Therefore, a linear model as the CNN
and C-Sympnet architectures (see \cref{table_high_dim_arch}) should be able to represent the flow.
The Dirichlet boundary values are encoded in the convolution layers of the
C-SympNet via constant zero-padding.

The test losses for the CNN and C-SympNet architectures are depicted in
\cref{fig_loss_linear_wave_eq}. We parametrize the symmetric kernels of C-SympNet
with the canonical basis and the finite-difference inspired (FD) basis. As it turns out,
the C-SympNet with FD basis has a much lower test loss and converges much faster
than the C-SympNet with the canonical basis.
The naive non-physically informed linear CNN architecture performs worst as expected. 
The CNN architecture is naive, because each convolution
layer of the CNN architecture uses the same kernel for the whole input $(q,p)$.

The extrapolation performance of all architectures can be seen in
\cref{fig_linear_wave_eq_displacement} (second and third plot). The C-SympNet with FD basis
is indistinguishable from the Störmer-Verlet solution.The extrapolation performance of the C-SympNet
with the canonical basis is still quite good, although the test loss
is much larger than the test loss for the C-SympNet with the FD basis.
The CNN solution gets unstable after a few time iterations.

The FD basis gives interpretability in terms of finite differences in our context,
which might make it a favorable choice in our context.

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{axis}[
		ymode=log,
		no markers,
		title={Test loss (Linear wave equation)},
		xlabel={Epoch}, xmin=0, xmax=500,
		ylabel={Test loss},
		legend style={nodes={scale=0.75, transform shape}},
		legend cell align=left,
		legend entries={
			CNN,
			C-SympNet (canonical basis),
			C-SympNet (FD basis)
		}
	]

	\addplot[
		color=green
	] table[x=epoch,y=loss,col sep=comma] {../data-wave/transport/linear_cnn/sigmoid/test_loss.csv};

	\addplot[
		color=blue
	] table[x=epoch,y=loss,col sep=comma] {../data-wave/transport/linear_canonical/sigmoid/test_loss.csv};

	\addplot[
		color=purple
	] table[x=epoch,y=loss,col sep=comma] {../data-wave/transport/linear_fd/sigmoid/test_loss.csv};
		
	\end{axis}
\end{tikzpicture}
\caption{Test losses for the linear wave equation
experiment after $500$ epochs.}\label{fig_loss_linear_wave_eq}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=1 by 3,
				vertical sep=2.2cm,
			},
			no markers,
			width=0.7*\linewidth, height=\axisdefaultheight,
			xlabel={$x$}, xmin=-0.5, xmax=0.5,
			ymin=-0.3, ymax=0.55, restrict y to domain=-2:2,
			legend style={
				nodes={scale=0.75, transform shape},
				legend cell align=left,
				%legend pos=outer north east
			},
			legend entries={
				CNN,
				C-SympNet (canonical basis),
				C-SympNet (FD basis),
				Störmer-Verlet,
			}
		]

		\nextgroupplot[
			title={Displacement $q(t=0,x)$}, ylabel={$q(t=0,x)$},
			legend entries={
				Initial value
			}
		]
			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/exact/q_t0.csv};

		\nextgroupplot[title={Displacement $q(t=3,x)$}, ylabel={$q(t=3,x)$}]
			\addplot[
				color=green
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_cnn/sigmoid/q_t3.csv};
		
			\addplot[
				color=blue
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_canonical/sigmoid/q_t3.csv};
		
			\addplot[
				color=purple
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_fd/sigmoid/q_t3.csv};

			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/exact/q_t3.csv};
	
		\nextgroupplot[title={Displacement $q(t=9,x)$}, ylabel={$q(t=9,x)$}]
			\addplot[
				color=green
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_cnn/sigmoid/q_t9.csv};
		
			\addplot[
				color=blue
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_canonical/sigmoid/q_t9.csv};
		
			\addplot[
				color=purple
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_fd/sigmoid/q_t9.csv};

			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/transport/exact/q_t9.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{Displacement $q(t,x)$ for $t=0$ (initial values) and $t=3,9$ (extrapolation) for the
	linear wave equation. The C-SympNet with FD basis is indistinguishable
	from the Störmer-Verlet solution.}\label{fig_linear_wave_eq_displacement}
\end{figure}

\subsubsection{Sine-Gordon}

The Sine-Gordon equation is a 
semi-linear wave equation with $G(u) = 1 - \cos(u)$, $g(u) = \sin(u)$ and $c=1$.
One can show that the Sine-Gordon equation has the solution
\begin{equation*}
	u_{\text{sol}}(t,x) = 4 \arctan \lsb \exp \lb \frac{x-x_0-vt}{\sqrt{1-v^2}} \rb \rsb 
	.
\end{equation*}

For our experiment, we set the initial values
\begin{align*}
	u_0(x) &= u_{\text{sol}}(0,x) \\
	w_0(x) &= \deldelt u_{\text{sol}}(0,x)
\end{align*}
with $v=0.2$ and the Dirichlet boundary values to $u_{\text{left}} = 0, \, u_{\text{right}} = 2\pi$.
Furthermore, we set $l=50$ and use $2000$ grid points for the discretization.

For this experiment, the flow is nonlinear and thus a purely linear model will not suffice.
The LARGE-CNN and the CG-SympNets are nonlinear models.
The Dirichlet boundary values $u_{\text{left}}$ and $u_{\text{right}}$ could again
be embedded into the CG-SympNets via constant padding, 
either by specifying the padding values $u_{\text{left}}$ and $u_{\text{right}}$ before training
or by making the padding values learnable during training.
Instead, we use symmetric padding this time, because for the discretized solution it holds
$u_{\text{left}} = q_0 \approx q_1$ and $q_{n} \approx q_{n+1} = u_{\text{right}}$
on the time domain in which we are interested.

The training and test losses for the LARGE-CNN and CG-SympNets for different activation functions
are shown in \cref{fig_losses_sine_gordon} and \cref{table_losses_sine_gordon}.
Again, the non-physically-informed naive LARGE-CNN performs worst as expected, 
because it applies the same kernels on the whole input vector $(q,p)$. 
In this experiment, N1 normalization always outperforms N2 normalization by a great margin.
The N1-CG-SympNet and the CG-SympNet (without normalization) perform nearly the same.
The best performing architecture is the N1-CG-SympNet
with ELU activation, see \cref{table_losses_sine_gordon}. Actually, N1 normalization
only improved the performance for the ELU activation, while it led to a small 
decrease of performance for the sigmoid and tanh activation.

The extrapolation performance of the two best performing SympNet architectures and the LARGE-CNN
can be seen at \cref{fig_sine_gordon_displacement}. The LARGE-CNN gets unstable after a few time iterations,
whereas the CG-SympNet and the N1-CG-SympNet still give a reasonable solution at $t=9$.

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[	
			group style={
				group size=2 by 3,
				y descriptions at=edge left,
				horizontal sep=0.7cm,
				vertical sep=2.2cm,
			},
			xlabel=Epoch, ylabel=Loss,
			width=0.95*\axisdefaultwidth, height=7cm,
			no markers,
			ymax=1e-1, ymin=5e-10, ymode=log,
			xmin=0, xmax=2000,
			legend style={nodes={scale=0.75, transform shape}},
			legend entries={
				LARGE-CNN,
				CG-SympNet,
				N1-CG-SympNet,
				N2-CG-SympNet
			}
		]
			\nextgroupplot[title={Training loss (sigmoid activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/sigmoid/loss.csv};
	
				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/sigmoid/loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/sigmoid/loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/sigmoid/loss.csv};
	
	
			\nextgroupplot[title={Test loss (sigmoid activation)},]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/sigmoid/test_loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/sigmoid/test_loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/sigmoid/test_loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/sigmoid/test_loss.csv};
	
			\nextgroupplot[title={Training loss (tanh activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/tanh/loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/tanh/loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/tanh/loss.csv};
	
			\nextgroupplot[title={Test loss (tanh activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/tanh/test_loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/test_loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/tanh/test_loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/tanh/test_loss.csv};
	
			\nextgroupplot[title={Training loss (ELU activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/elu/loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/elu/loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/elu/loss.csv};
	
			\nextgroupplot[title={Test loss (ELU activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/elu/test_loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/elu/test_loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/test_loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/elu/test_loss.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{Training and test losses for the Sine-Gordon experiment
	(First row) sigmoid activation (Second row) tanh activation
	(Third row) ELU activation}\label{fig_losses_sine_gordon}
\end{figure}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data-wave/sine_gordon/test_loss_summary.csv}

	\caption{Test losses for Sine-Gordon experiment after $2000$ epochs.}
	\label{table_losses_sine_gordon}
\end{table}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=1 by 3,
				vertical sep=2.2cm,
			},
			no markers,
			width=0.9*\linewidth, height=\axisdefaultheight,
			title={Displacement $q(t=9,x)$ (Sine Gordon1)},
			xlabel={$x$}, xmin=-25, xmax=25,
			ymin=-1, ymax=10, restrict y to domain=-5:10,
			legend style={
				nodes={scale=0.75, transform shape},
				legend cell align=left,
				%legend pos=outer north east
			},
			legend entries={
				LARGE-CNN,
				CG-SympNet (tanh),
				N1-CG-SympNet (ELU),
				Störmer-Verlet,
			}
		]

		\nextgroupplot[
			title={Displacement $q(t=0,x)$ (Sine Gordon)}, ylabel={$q(t=0,x)$},
			legend entries={
				Initial value
			}
		]
			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/exact/q_t0.csv};

		\nextgroupplot[title={Displacement $q(t=3,x)$ (Sine Gordon)}, ylabel={$q(t=3,x)$}]
			\addplot[
				color=green
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/cnn/elu/q_t3.csv};
		
			\addplot[
				color=red
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/q_t3.csv};
		
			\addplot[
				color=blue
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/q_t3.csv};

			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/exact/q_t3.csv};
		
		\nextgroupplot[title={Displacement $q(t=9,x)$ (Sine Gordon)}, ylabel={$q(t=9,x)$}]
			\addplot[
				color=green
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/cnn/elu/q_t9.csv};
		
			\addplot[
				color=red
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/q_t9.csv};
		
			\addplot[
				color=blue
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/q_t9.csv};

			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/exact/q_t9.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{Displacement $q(t,x)$ for $t=0$ (initial values) and $t=3,9$ (extrapolation) for the
	Sine-Gordon experiment.}\label{fig_sine_gordon_displacement}
\end{figure}

%----------------------------------------------------------------------------------------
%
% Resume
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\section{R\'esum\'e}\label{sec_resume}
\subsection{Summary and conclusion}

The main contribution of our work is to transfer the concepts of
convolution and batch normalization to SympNets, while preserving symplecticity.
Adding convolution makes SympNets applicaple for high-dimensional Hamiltonian systems,
which typically arise from semi-discretized partial differential equations. 

We have successfully reproduced the good performance of the LA-SympNets 
observed by \citeauthor{Jin2020} for the simple pendulum swinging case.
Furthermore, we managed to
make the G-SympNet competitive with the LA-SympNet for the simple pendulum swinging case
by proposing a method to add batch normalization to SympNets while preserving symplecticity.
We have seen that the good performance of the LA-SympNet does not
transfer to the rotating case of the simple pendulum, at least if no further logic
is added to prevent the angular coordinate to increase monotonically.
We have seen that SympNets preserve the total energy in the simple pendulum swinging case and
offer good long-term predictions compared to models which do not preserve the symplectic
structure.

\citeauthor{batchnorm-ioffe15} observed that batch normalization leads to more robustness
with respect to faster learning rates. We reproduced this observation with our special
batch normalization variant for SympNets.
We empirically noted that embedding physical knowledge into the structure of the model
(like in our case symplecticity) reduces overfitting.

We applied SympNets with convolution to the semi-linear wave
equation and successfully extrapolated single trajectories of the resulting Hamiltonian ODE.

We conclude that in our case
it is a worthwile undertaking to structurally embed a priori knowledge of a system
into a machine learning model, because it improves prediction performance
and preserves potentially important properties of the original system, 
like in our case the preservation of total energy.

\subsection{Outlook}\label{sec_outlook}

As already noted by \citeauthor{Jin2020}, the unit triangular structure lends itself
to be used in a recurrent neural network setting. For example, the neural network could 
form a loop by connecting the last layer of the neural network to the first layer again.
When unfolding the loop, this can be interpreted as parameter 
sharing between several consecutive blocks of the neural network. 
If we interpret a single block as a numerical integrator, this is equivalent to applying
the same numerical integrator several times, i.e. the problem to predict the flow for
time $t$ is divided in $t/n$ smaller problems, where $n \in \N$ denotes the number how often
the loop connection is executed. Thus, this might improve model performance. This
direction could be explored further and tested empirically.

As also \citeauthor{Jin2020} already noted, a unit triangular layer is always invertible, 
which means that every SympNet is reversible. So storing the intermediate values
in the forward pass before computing the gradient is actually not necessary. This
paves the way for memory-efficient implementations of SympNets.

The performance of the SympNets for the simple pendulum rotating case has not been as good
as for the swinging case. We have already noted that the angular coordinate $q$
of the simple pendulum monotonically increases when the pendulum rotates,
which might explain this observation. A simple fix worth trying might be
to take the angular coordinate $q$ modulo $2\pi$, or incorporate other appropriate
coordinate transformations. Another possible approach could be to divide the
phase space into multiple domains with a different (sub-)neural network
responsible for each domain, and implement transition maps between the different
domains. The simple pendulum phase plot is $2\pi$-periodic on the $q$-axis.
The universal approximation theorems however only apply for compact sets.
It is an open research topic how to improve the extrapolation performance
of neural networks for periodic functions \cite{Ziyin2020}.

We have only worked with one-dimensional convolution so far. It could be investigated
how to generalize our proposal for two and three dimensions. This would add
even more application areas, as it would allow to predict the
flow for two and three dimensional systems in space. Besides, it could be investigated
how to allow discretizations with non-equidistant grids.

So far, there only exists universal approximation theorems for SympNets 
for $r$-finite activation functions.
Universal approximation theorems for other activation functions, for example
the continously differentiable ELU activation function, is still an open topic to be explored.

We believe that the idea to intrinsically embed a priori knowledge about a system
into a machine learning model is very promising and should be further explored
also in other contexts.

%----------------------------------------------------------------------------------------
%
% Appendices
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\section{Implementation}

The symplectic neural network was implemented in Python via the PyTorch neural network
library \cite{PyTorch2019}. We used the PyTorch Tensorboard extension to get 
live monitoring of the neural networks during training and for experiment tracking 
and comparison (\url{https://pytorch.org/docs/stable/tensorboard.html}). 
We used Docker containers for a reproducible
development and execution environment. The source code has been managed with Git
version control and hosted in GitLab. We used the Visual Studio Code Remote SSH extension
for comfortable editing of source code files on
a remote cluster from the local development machine
(\url{https://code.visualstudio.com/docs/remote/ssh}). We refer to the supplemented
source code for instructions how to run the experiments.

\newpage~\newpage
\section{Declaration of authorship}

\vspace{3cm}

\begin{table}[h!]
\centering
\begin{tabular}{|p{13cm}|}
\hline\\
	I hereby certify
	\begin{enumerate}
		\item that this thesis has been composed by me and is based on my own work, unless stated otherwise,
		\item that all direct or indirect sources used are acknowledged as references and all extracts from work of others, either verbatim or in spirit, are stated as such,
		\item that neither the thesis itself nor parts of this thesis have been part of another examination procedure,
		\item that neither the thesis itself nor parts of this thesis have been published and
		\item that all copies of this thesis, either digital or printed, coincide.
	\end{enumerate}
	Therewith, this declaration of authorship is in accordance with the examination regulations from 22th July 2016 of the bachelor's program \emph{Simulation Technology} of the University of Stuttgart.\\\\
\hline
\end{tabular}
\end{table}

\vspace{4cm}
\hrulefill\\
Name
\hspace{7cm}
Date, City, Signature
\end{appendices}
%----------------------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\addcontentsline{toc}{section}{References}
\bibliographystyle{abbrvnat}
\bibliography{../../literature/references.bib}

\end{document}
