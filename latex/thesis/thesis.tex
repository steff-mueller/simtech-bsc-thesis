\documentclass[twoside,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,bindingoffset=0.5cm,inner=2.5cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[a4paper,bindingoffset=1cm,inner=2cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry} %,showframe
\usepackage{helvet}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\sfdefault}
% \usepackage[german,ngerman]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amstext,amssymb,bm}
\usepackage{mathtools}
\usepackage{xcolor,color}
\usepackage{pifont}
\usepackage{array}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algpseudocode,algorithm}
%%% pgfplots for plotting
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
%%% Title page
\usepackage{common/titlePageST}
%%% Appendix in TOC
\usepackage[toc,page]{appendix}
%%% Setstrech on title page
\usepackage{setspace}
%%% Tilde in URL in literature
\usepackage{url}
%%% multiple rows
\usepackage{multirow}
%%% fancy column and row seperators
\usepackage{hhline}
%%% custom items in enumerate and itemize
\usepackage{enumitem}
%%% custom format for algorithm comments
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em // #1}

\clubpenalty=5000
\widowpenalty=5000

\setlength{\emergencystretch}{2cm}
%----------------------------------------------------------------------------------------
%	abbreviation includes
%----------------------------------------------------------------------------------------
\input{common/default_abbrev}
\input{common/specific_abbrev}
%%% Clever refing
\usepackage[capitalise,noabbrev]{cleveref}

%----------------------------------------------------------------------------------------
%	References with cleveref
%----------------------------------------------------------------------------------------
\crefformat{equation}{(#2#1#3)}

%----------------------------------------------------------------------------------------
%	Theorem environments
%----------------------------------------------------------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

%----------------------------------------------------------------------------------------
%	fancyhdr
%----------------------------------------------------------------------------------------
\usepackage{fancyhdr}

%----------------------------------------------------------------------------------------
%	pgfplots
%----------------------------------------------------------------------------------------
% Style to select only points from #1 to #2 (inclusive)
\pgfplotsset{select coords between index/.style 2 args={
    x filter/.code={
        \ifnum\coordindex<#1\def\pgfmathresult{}\fi
        \ifnum\coordindex>#2\def\pgfmathresult{}\fi
    }
}}

\makeatletter
\newcommand{\theauthor}{Steffen Müller} %
\makeatother

\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}
\fancyfoot[OL,ER]{University of Stuttgart} % inner
\fancyfoot[OR,EL]{\thepage} % outer
\fancyhead[OL,ER]{\theauthor} % inner 
\fancyhead[OR,EL]{IANS -- Institute of Applied Analysis and Numerical Simulation} % outer
\fancyfoot[C]{}

% \headsep=4mm
% \footskip=4mm
\parindent=0mm
\parskip=6pt
% \renewcommand{\footskip}{3pt}

%----------------------------------------------------------------------------------------
%	Something
%----------------------------------------------------------------------------------------
\usepackage{textpos}
\setlength{\TPHorizModule}{1mm}%
\setlength{\TPVertModule}{1mm}%
% \headsep=5mm
% \footskip=5mm
\pagestyle{fancy}
\newcommand{\articleheading}[3]{
{\large #1}\\[3mm]
{\Large\bf #2}\\[3mm]
{\large #3}
}

%----------------------------------------------------------------------------------------
%	Start Document
%----------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
%----------------------------------------------------------------------------------------
%
% TITLE PAGE
%
%----------------------------------------------------------------------------------------
%------------------------------------------
\begin{titlePageST}
%------------------------------------------
\makeLogo%
{-10pt}{
\includegraphics[width=0.7\textwidth]{figures/logos/simtech.pdf}
}%
{0pt}{
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/logos/ians.pdf}
	\end{center}}%
{0pt}{
\begin{flushright}
	\vspace{-10pt}
	\includegraphics[width=0.9\textwidth]{figures/logos/unistuttgart_logo_englisch_cmyk.eps}
\end{flushright}}%
\vspace{35pt}%
%------------------------------------------
\makeHeader%
[Research Group: Numerical Mathematics] %
{Institute of Applied Analysis and Numerical Simulation} %
\vspace{50pt}%
%------------------------------------------
\makeTitle%
{Simulation Technology Degree Course} %
{Bachelor Thesis} %
\vspace{80pt}%
%------------------------------------------
\makeTitleThesis%
{Symplectic Neural Networks}
\vspace{90pt}%
%------------------------------------------
\begin{supervisorST}{3}%
\addSuper%
{First Reviewer}%
{Prof. Dr. B. Haasdonk}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\addSuper%
{Second Reviewer}%
{Prof. Dr. D. Pflüger}%
{Institute of Parallel and Distributed\\[-0.2cm]
Systems (Scientific Computing)}%
\addSuper%
{Advisor}%
{Patrick Buchfink, M.Sc.}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\end{supervisorST}%
\vspace{80pt}%
%------------------------------------------
\begin{authorST}{Submitted by}%
\addAuthorInfo{Author}{Steffen Müller}
\addAuthorInfo{Student ID}{3260643}
\addAuthorInfo{SimTech ID}{119} %
\addAuthorInfo{Submission Date}{...} %FILLIN
\end{authorST}%
%------------------------------------------
\end{titlePageST}
%----------------------------------------------------------------------------------------
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ABSTRACT
%
%----------------------------------------------------------------------------------------
\section*{Abstract}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ACKNOWLEDGEMENTS
%
%----------------------------------------------------------------------------------------
\section*{Acknowledgements}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% TOC
%
%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\newpage\thispagestyle{plain}\null
%----------------------------------------------------------------------------------------
%
% Begin with document content
%
%----------------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic} 
%----------------------------------------------------------------------------------------
%
% Introduction
%
%----------------------------------------------------------------------------------------
\section{Introduction}

\subsection{Outline}

\subsection{Notation}

Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be an arbitrary function and $x \in \mathbb{R}^n$
a vector. When we write $f(x)$ we mean the element-wise application of $f$ 
on the vector $x \in \mathbb{R}^n$, i.e. $f(x) = \lb
	f(x_1), ..., f(x_n)
\rb^T$.

The $d$-by-$d$ identity matrix is denoted by $I_d$. If the dimension $d$ can be inferred
from context we may just write $I$.

$\onevec{n}$ denotes the $n$-dimensional $1$-vector.

\todo{Introduce block matrix notation for general functions}

We denote the $k$-th partial derivative of a function $f: \mathbb{R}^n \to \mathbb{R},
x \mapsto f(x)$ with $\deldel{x_k} f(x)$.

If $f: \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$ is a multi-dimensional function

\todo{"partial derivative" of a vector, Jacobian matrices}

\todo{Gradient}

\todo{Describe special meaning of dimension $d$}

\begin{equation*}
	J := \begin{pmatrix}
		0 & I_d \\
		-I_d & 0
	\end{pmatrix}
\end{equation*}

vector $v \in \R^n$
\begin{equation*}
	v = \begin{pmatrix}
		v_i
	\end{pmatrix}_{i=1}^n
\end{equation*}


%----------------------------------------------------------------------------------------
%
% Content
%
%----------------------------------------------------------------------------------------
\newpage
\section{Problem setup}

\begin{definition}
	A matrix $A \in \mathbb{R}^{2d \times 2d}$ is called symplectic if $A^TJA=J$.
\end{definition}

\begin{definition}
	A differentiable map $\phi : U \to \mathbb{R}^{2d}$ (where $U \subset \mathbb{R}^{2d}$ is an open set)
	is called symplectic if the Jacobian matrix $\jac{\phi}{x}$ is everywhere symplectic, i.e.
	\begin{equation*}
		\lb \jac{\phi}{x} \rb^T J \lb \jac{\phi}{x} \rb = J
	\end{equation*}
\end{definition}

\begin{definition}
	A Hamiltonian (ODE) system can be written in canonical form as
	\begin{align*}
		\dot{y}(t) &= J \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
		y(t_0) &= y_0
	\end{align*}
	where $y: I \subset \mathbb{R} \to \mathbb{R}^{2d},\, t \mapsto y(t) = (q(t),p(t))$ and 
	$y_0 = (q_0, p_0) \in \mathbb{R}^{2d}$ the initial value for $t_0 \in \mathbb{R}$. 
	The function $H: \mathbb{R}^{2d} \to \mathbb{R}$ is called the Hamiltonian 
	or the total energy. $q = q(t) \in \mathbb{R}^d$ are called generalized coordinates
	and $p=p(t) \in \mathbb{R}^d$ are called conjugate momenta. 
	The phase space $\mathbb{R}^{2d}$ has even dimension for Hamiltonian systems.
\end{definition}

Note that the Hamiltonian $H$ is a first integral, i.e. the total energy is preserved, because
\begin{equation*}
	\ddt H(y(t)) = \lsb \grad{H}(y(t)) \rsb^T \dot{y}(t) = 
	\lsb \grad{H}(y(t)) \rsb^T J \grad{H}(y(t)) = 0
\end{equation*}
since $J$ skew-symmetric.

Let $\phi_{t,H} : U \subset \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ be the flow for a 
canonical Hamiltonian system with Hamiltonian $H$ for a fixed time step $t$, i.e.
\begin{equation*}
	\phi_{t,H}\begin{pmatrix}
		q_0 \\
		p_0
	\end{pmatrix}
	= \begin{pmatrix}
		q(t; q_0, p_0) \\
		p(t; q_0, p_0)
	\end{pmatrix}
\end{equation*}
where $q(t; q_0, p_0)$ and $p(t; q_0, p_0)$ denote the solution of the Hamiltonian system
at time $t$ for initial conditions $y_0 = (q_0,p_0) \in \mathbb{R}^{2d}$. 
Poincaré has shown that the flow of a Hamiltonian system is symplectic.

\begin{theorem}(Poincaré 1899)
	Let $H: \mathbb{R}^{2d} \to \mathbb{R}$ be a twice continuously differentiable
	function on $U \subset \mathbb{R}^{2d}$. Then, for each fixed $t$, the flow
	$\phi_{t,H}$ is a symplectic map wherever it is defined.
\end{theorem}
\begin{proof}
	We refer to \citet[Theorem 2.4, p.~184]{hairer2006} 
	or to \citet[Theorem 1, p.~54]{leimkuhler_reich_2005}.
\end{proof}

Note that a Hamiltonian (ODE) system in general can be written as
\begin{align*}
	\dot{y(t)} &= S \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
	y(t_0) &= y_0
\end{align*}
with an arbitrary nondegenerate skew-symmetric matrix $S \in \mathbb{R}^{2d \times 2d}$.
However there always exists a transformation to express such a Hamiltonian ODE system in
canonical form (see \citet[Remark 3.8]{peng2016}).

Our goal is to learn the flow map $\phi_{t,H} : U \to \mathbb{R}^{2d}$ for fixed $t$ with a neural network.
To be specific, for fixed $t$, we supply the neural network with training pairs $(y_i, \phi_{t,H}(y_i))$
($y_i \in \mathbb{R}^{2d}$ and $i=1, \dots, n_{\text{train}}$) and want the neural network
to be able to predict $\phi_{t,H}(y)$ for arbitrary $y \in \mathbb{R}^{2d}$.
This leads to the idea that we embed symplecticity into the neural network itself structurally.

Symplecticity has already been successfully embedded into numerical integrators for ODEs, which
led to the development of geometric integrators.

\section{Architecture of SympNets}

In this section we recapitulate the architecture of SympNets (Symplectic Networks) as
proposed by \citeauthor{Jin2020} in \cite{Jin2020}. We proof
symplecticity for all layer types.

\todo{Briefly introduce neural network terminology:}

\todo{Layer}

\todo{Forward pass}

\todo{Mini-batch}

\todo{Training, backpropagation, learnable parameters}

\subsection{General architecture}

We use that the composition of symplectic maps is again symplectic. So we impose that every
layer of the neural network must be symplectic, which leads to the overall neural network
to be symplectic.

\begin{definition}
	(Unit Triangular Layer) A layer $\mathcal{L} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ 
	is called a unit triangular layer with layer transform 
	$\hat{\sigma}_{\mathcal{L}} : \mathbb{R}^d \to \mathbb{R}^d,\, p \mapsto \layertf(p)$
	and bias $b \in \R^{2d}$,
	if $\mathcal{L}$ can be expressed as
	\begin{equation*}
		\mathcal{L} \qpvec = \uppersympop{\hat{\activation}_L} + b
		= \begin{pmatrix}
			q + \hat{\sigma}_{\mathcal{L}}(p) \\
			p
		\end{pmatrix} + b \quad \text{(upper unit triangular layer)}
	\end{equation*}
	or
	\begin{equation*}
		\mathcal{L} \qpvec = \lowersympop{\hat{\activation}_L} + b
		= \begin{pmatrix}
			q \\
			\hat{\sigma}_{\mathcal{L}}(q) + p
		\end{pmatrix} + b \quad \text{(lower unit triangular layer)}
	\end{equation*}

	If we want to emphasize if $\mathcal{L}$ is an upper or lower
	unit triangular layer, we write $\mathcal{L}_{\text{up}}$ or $\mathcal{L}_{\text{low}}$.
	If we do not expllicitly specify the bias $b$, we assume $b=0=\text{const}$.
\end{definition}

This structure was initially proposed by \citeauthor{Deco1995} in \cite{Deco1995} 
\todo{Have a deeper look at \cite{Deco1995}} for volume-conserving neural networks.
\todo{Relation to Residual Networks / ResNet}

A SympNet only consists of unit triangular layers. In order that the unit triangular layers 
$\mathcal{L}_{\text{up}}$ or $\mathcal{L}_{\text{low}}$ are symplectic 
we have to demand additional requirements to the layer transform $\hat{\sigma}_{\mathcal{L}}$.

\begin{lemma}\label{jacobi_symmetric}
	An upper or lower unit triangular layer $\mathcal{L}_{\text{up}}$ or $\mathcal{L}_{\text{low}}$
	is symplectic if and only if the Jacobian of the layer transform
	$\hat{\sigma}_{\mathcal{L}} \in \mathbb{C}(\mathbb{R}^d, \R^d)$ 
	is everywhere symmetric.
\end{lemma}
\begin{proof}
	We show the result for $\mathcal{L}_{\text{up}}$ only. 
	The proof is analogous for $\mathcal{L}_{\text{low}}$.

	\begin{equation*}
		\jac{\mathcal{L}_{\text{up}}}{(q,p)} = \begin{pmatrix}
			I & \jac{\hat{\sigma}_{\mathcal{L}}}{p} \\
			0 & I
		\end{pmatrix}
	\end{equation*}
	It follows
	\begin{align*}
		\lb \jac{\mathcal{L}_{\text{up}}}{(q,p)} \rb^T J \lb \jac{\mathcal{L}_{\text{up}}}{(q,p)} \rb
		&= \lb \jac{\mathcal{L}_{\text{up}}}{(q,p)} \rb^T \begin{pmatrix}
			0 & I \\
			-I & -\jac{\hat{\sigma}_{\mathcal{L}}}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			I & 0 \\
			\lb \jac{\hat{\sigma}_{\mathcal{L}}}{p} \rb^T & Id
		\end{pmatrix} \begin{pmatrix}
			0 & I \\
			-I & -\jac{\hat{\sigma}_{\mathcal{L}}}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			0 & I \\
			-I & \lb \jac{\hat{\sigma}_{\mathcal{L}}}{p} \rb ^T-\jac{\hat{\sigma}_{\mathcal{L}}}{p}
		\end{pmatrix} \overset{!}{=} J
	\end{align*}
	Thus, $\mathcal{L}_{\text{up}}$ is symplectic if and only if
	$\lb \jac{\hat{\sigma}_{\mathcal{L}}}{p} \rb ^T-\jac{\hat{\sigma}_{\mathcal{L}}}{p}=0$, 
	i.e. if and only if the Jacobian $\jac{\hat{\sigma}_{\mathcal{L}}}{p}$ is everywhere symmetric.
\end{proof}

The next corollary is useful to construct symplectic unit triangular layers.
\begin{corollary}\label{gradient_corollary}
	Let $V: \mathbb{R}^d \to \mathbb{R}, \; p \mapsto V(p)$ be a function in 
	$\mathbb{C}^2(\R^d)$. Then the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = \grad{V}(p)
	\end{equation*}
	are symplectic. We call $V$ a potential.
\end{corollary}
\begin{proof}
	The Jacobian matrix of $\grad{V}$ corresponds to the Hessian matrix of $V$,
	i.e. $\jac{(\grad{V})}{p} = HD$. \todo{Notation for Hessian}
	The Hessian is everywhere symmetric, thus the result follows with \cref{jacobi_symmetric}.
\end{proof}

\subsection{Linear layers}

\begin{definition}
	(Linear layers)
	Given a symmetric matrix $S \in \R^{d \times d}$ and bias $b \in \R^{2d}$,
	we call the upper and lower unit triangular layers
	with layer transform $\layertf(p) = Sp$ the (symplectic) linear layers $\ell_\up$ and $\ell_\low$.
\end{definition}

The linear layers can be expressed with matrix-vector multiplication, for example
\begin{equation*}
	\ell_\up \qpvec = \begin{pmatrix}
		I & S \\
		0 & I
	\end{pmatrix} \qpvec + b
\end{equation*}

The matrix $S$ and the bias $b$ are learnable parameters.
In practice, we parametrize the symmetric matrix $S\in \mathbb{R}^{d \times d}$
with $S = A^T + A$ via another arbitrary matrix $A\in \mathbb{R}^{d \times d}$, as
most optimization methods used for learning neural networks are designed for
unconstrained optimization problems.

The linear layers are symplectic, because the Jacobian of the layer transform $\jac{\layertf}{p} = S$
is a symmetric matrix by defintion. Alternatively, we can apply \cref{gradient_corollary} by
choosing the potential $V(p) := p^TAp$.

\begin{proof}
	For $k=1, \dots, d$ we have
	\begin{align*}
		\lb \grad{V}(p) \rb_k &= \deldel{p_k} \lb \sum_{i,j=1}^d A_{ij} p_i p_j \rb
		= \sum_{i,j=1}^d A_{ij} \deldel{p_k}(p_i) p_j + \sum_{i,j=1}^d A_{ij} p_i \deldel{p_k}(p_j) \\
		&= \sum_{j=1}^d A_{kj} p_j + \sum_{i=1}^d A_{ik} p_i = ((A+A^T)p)_k \\
	\end{align*}
	Choose $A=\frac{1}{2}S \implies A+A^T=S \implies$
	Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

We may enhance expressivity of a linear layer by alternately composing multiple
$\ell_{up}$ and $\ell_{low}$. We define 
$\mathcal{L}^{n}_{up},\, \mathcal{L}^{n}_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ with

\begin{align*}
	\mathcal{L}^{n}_{up} \qpvec &:= \begin{pmatrix}
		I && 0 / S_n \\
		S_n / 0 && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && 0 \\
		S_2 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix} + b \\
	\mathcal{L}^{n}_{low} \qpvec &:= \begin{pmatrix}
		I && S_n / 0 \\
		0 / S_n && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && 0 \\
		S_1 && I
	\end{pmatrix} + b
\end{align*}

$\mathcal{L}^{n}_{up}$ and $\mathcal{L}^{n}_{low}$ are again symplectic because they 
are a composition of the symplectic maps $\ell_{up}$ and $\ell_{low}$.

\citeauthor{jin2020unit} show in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map. In other words, 
the set of all possible $\mathcal{L}^{9}_{up}$ is equal to the set of all symplectic linear maps.

It does not make sense to put two upper linear layers or two lower linear layers after each other,
because this reduces to a single upper or lower linear layer:

\begin{proof}
	We show the statement for two upper linear layers $\ell_{up,2}$ and $\ell_{up,1}$.
	\begin{align*}
		\lb \ell_{up,2} \circ \ell_{up,1} \rb \qpvec &=
		\begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix}
		\lb
		\begin{pmatrix}
			I && S_1 \\x
			0 && I
		\end{pmatrix}
		\qpvec + b_1
		\rb + b_2 \\
		&= \begin{pmatrix}
			I && S_1 + S_2 \\
			0 && I
		\end{pmatrix} \qpvec
		+ \begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix} b_1
		+ b_2 \\
		&= \begin{pmatrix}
			I && S \\
			0 && I
		\end{pmatrix} \qpvec + b
	\end{align*}
	with $S := S_1 + S_2$ and $b := \begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix} b_1
	+ b_2$.
\end{proof}

\subsection{Activation layers}

\begin{definition}
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \activation(p_i) \rb_{i=1}^d
	\end{equation*}
	the upper and lower activation layers $\mathcal{N}_\up$ and $\mathcal{N}_\low$.
\end{definition}
The coefficients $a \in \mathbb{R}^d$ are learnable parameters.

\begin{corollary}
	Given an activation function $\activation \in \mathbb{C}^1(\R)$
	the activation layers $\mathcal{N}_{up}$ and $\mathcal{N}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be an antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{k=1}^d a_k \mathcal{A}(p_k)
	\end{equation*}

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $k=1, \dots d$
	\begin{equation*}
		\lb \grad{V}(p) \rb_i = \deldel{p_i} \lb \sum_{k=1}^d a_k \mathcal{A}(p_k) \rb
		= a_i \activation(p_i) = \lb \layertf(p) \rb_i
	\end{equation*}
	Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

\subsection{Gradient layers}

\begin{definition}
	(Gradient layers)
	Given $K \in \mathbb{R}^{n \times d}$, $a,\hat{b} \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \activation \lb (Kp)_j + \hat{b} \rb \bigg)_{j=1}^n
	\end{equation*}
	the upper and lower gradient layers $\mathcal{G}_\up$ and $\mathcal{G}_\low$.
\end{definition}
$K \in \mathbb{R}^{n \times d}$ and $a,b \in \mathbb{R}^n$
are learnable parameters. 
$n \in \mathbb{N}$ denotes the width of the gradient layer and can be chosen freely.
In practice, we choose $n >> d$ for large expressivity.

\begin{corollary}
	Given an activation function $\sigma \in C^1$ the gradient layers $\mathcal{G}_{up}$
	and $\mathcal{G}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + \hat{b})
	\end{equation*}
	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $i=1, \dots, d$
	\begin{align*}
		(\grad{V(p)})_i &= \deldel{p_i} \lb \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + \hat{b}) \rb
		= \sum_{j=1}^n a_j \activation((Kp)_k + \hat{b}) 
		\underbrace{\deldel{p_i} \lb (Kp)_k \rb}_{=K_{ji}} \\
		&= \lb K^T \bigg( a_j \activation \lb (Kp)_j + \hat{b} \rb \bigg)_{j=1}^n \rb_i
		= (\layertf(p))_i
	\end{align*}
	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}_{up}$ and $\mathcal{G}_{low}$ are symplectic.
\end{proof}

Note that activation layers are a subset of gradient layers. We can see this by choosing
$n=d$, $K=I_d$ and $b=0$.

\section{Convolution and Normalization for SympNets}

In this section we introduce extensions to SympNets. In particular, we bring the concept
of convolution to SympNets and we propose a possibility how to embed normalization into SympNets while
maintaining symplecticity.

\subsection{Introduction to Convolution}

\todo{Motivate convolution layers (weight sharing, sparse-connectivity)}

We introduce convolution in a rather abstract way in order to simplify the following proofs.
General convolution is defined on infinite-dimensional function spaces.
Neural networks implement a finite-dimensional version, as the operation has to be
computable.

Given two discrete functions $f,g \in \mathbb{R}^\mathbb{Z}$ convolution is defined as 
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau - a)
\end{equation*}
However, most popular neural network libraries actually do not implement real convolution
in convolution layers. Instead, they implement the so called cross-correlation, but call it convolution. 
Cross-correlation is a flipped convolution and is defined as
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau + a)
\end{equation*}
Our goal is to replace $f$ with a finite-dimensional kernel $k$. The neural network should learn the kernel $k$.
Note that in this context it does not make a difference if we use convolution or cross-correlation, because
the neural network would just learn a flipped version of the kernel $k$ in the opposite case.
Because most popular neural network libraries implement cross correlation and call it convolution,
we stick to the same convention and mean cross-correlation if we write $*$.
Still, the choice is arbitrary, as the following statements could be constructed analogously with convolution.
\todo{cite Goodfellow?}

We work with finite-dimensional vectors in neural networks. Therefore, we define a bijective mapping between
finite-dimensionial vectors in $\mathbb{R}^n$ and functions in $\mathbb{R}^{\mathbb{Z}}$ 
via zero continuation on $\mathbb{Z}$:
\begin{align*}
	\mathcal{I}_n &: \mathbb{R}^k \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb\mathcal{I}_n(x)\rb(\tau) := \sum_{a=1}^{n} x_a \delta_{\tau a} \\
	%
	\mathcal{I}_n^{-1} &: \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^k,
	\quad \mathcal{I}_n^{-1}(f) := \lb f(\tau) \rb_{\tau=1}^n
\end{align*}

Additionially, we define a shift operator $\mathcal{S}_\zeta$, 
which shifts a discrete function $f \in \mathbb{R}^\mathbb{Z}$ by $\zeta \in \mathbb{Z}$ positions
in the right direction
\begin{equation*}
	\mathcal{S}_\zeta : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb \mathcal{S}_\zeta(f) \rb (\tau) := f(\tau - \zeta)
\end{equation*}

\begin{definition}
	(Valid cross-correlation)
	We define the valid cross-correlation of a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$ and 
	a finite-dimensional input $x \in \mathbb{R}^{n_x}$ with $n_x \geq n_k$ as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_x-n_k+1},
		\quad *_{\text{v}}(k,x) := \mathcal{I}_{n_x-n_k+1}^{-1} (
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}
In other words, the valid cross-correlation $*_{\text{v}}$ evaluates the cross-correlation 
only at positions where the finite-dimensional kernel $k$ and the 
finite-dimensional input $x$ fully overlap and puts the result into a finite-dimensional vector.

\todo{Add illustration}

We define a stride operator $\mathcal{D}_s$ for a stride number $s \in \mathbb{N}$ with
\begin{equation*}
	\mathcal{D}_s : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (\mathcal{D}_s)(\tau) := f(s \tau)
\end{equation*}
The stride operators skips $s$ entries of $f \in \mathbb{R}^\mathbb{Z}$.

\begin{definition}
	(Valid cross-correlation with stride)
	Given a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$, a finite-dimensional
	input $x \in \mathbb{R}^{n_x}$, stride number $s \in \mathbb{N}$ and output dimension
	\begin{equation*}
		n_{\text{out}} = \left\lfloor \frac{n_x - n_k}{s} \right\rfloor + 1
	\end{equation*}
	the valid-cross correlation with stride is defined as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_{\text{out}}},
		\quad *_{\text{v}}(k,x) := 
		(\mathcal{I}_{n_{\text{out}}}^{-1} \circ D_s)
		(
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}

\todo{Add illustration}

\subsection{Convolution layers}

The valid cross correlation (without stride) has the output dimension $n_x - n_k + 1$.
Thus, the valid cross-correlation $*_{\text{v}}$ decreases the dimension of the input $x$.
However, we want to incorporate the valid cross-correlation as the layer transform of a
unit triangular layer. The layer transform must keep the dimension.
Therefore we apply a padding operation on $x$ before passing it to the valid
cross-correlation. The padding operation increases the dimension of $x$ in order
that the composition of padding and cross-correlation keeps the overall dimension.
More specifically, let $d \in \mathbb{N}$ denote the input dimension of the layer transform.
Let $c \in \mathbb{N}$ denote the number by which the padding operator increases 
the input dimension. In order that the composition of padding and cross-correlation
keeps the dimension, it must hold
\begin{equation*}
	n_x - n_k + 1 = \underbrace{(d+c)}_{\substack{
		\text{increased dim.} \\
		\text{by } c \text{ via padding}
	}} - \, n_k + 1 = d \iff c = n_k-1
\end{equation*}
For symmetry reasons, we pad the same number of values at the beginning and 
the end of the input vector. This means that the number of padding values $c$ is even. 
Consequently, the equation above implies that the kernel size $n_k$ is odd, i.e. we can write
$n_k = 2m+1$ for a $m \in \mathbb{N}_0$, and $c=2m$. We define two different padding operators,
which both increase the input vector dimension by $2m$.

\begin{definition}
	(Constant padding)
	Given the padding values $l,r \in \mathbb{R}^m$, we define constant padding 
	$c_{\text{pad},m}$ as
	\begin{equation*}
		c_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad c_{\text{pad},m}(p) = c_{\text{pad},m}(p;l,r) := \lb l^T, p^T, r^T \rb^T
	\end{equation*}
\end{definition}
The constant padding $c_{\text{pad},m}$ is an affine linear map, because we can write
\begin{equation}\label{cpad_affine}
	c_{\text{pad},m}(p;l,r) = c_{\text{pad}}(p;0_m,0_m)
	+ c_{\text{pad},m}(0_d;l,r)
\end{equation}
and $c_{\text{pad},m}(p;0_m,0_m)$ is linear regarding $p$.

\begin{definition}
	(Symmetric padding)
	We define symmetric padding as
	\begin{equation*}
		s_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad s_{\text{pad},m}(p) := (
			\underbrace{p_m, p_{m-1} \dots, p_1}_{m \text{ values}}, \,
			\underbrace{p_1, p_2 \dots, p_d}_{d \text{ values}}, \,
			\underbrace{p_d, p_{d-1} \dots, p_{d-m+1}}_{m \text{ values}}
		)^T
	\end{equation*}
\end{definition}
The symmetric padding $s_{\text{pad},m}$ is a linear map.

Given an odd kernel $k \in \mathbb{R}^{2m+1}$ we set
\begin{equation*}
	\hat{k} = \hat{k}(k) := \mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}
Then $\hat{k}(-m) = k_1, \, \dots,\, \hat{k}(m) = k_{2m+1}$ and
$\hat{k}(\tau)=0$ for $\abs{\tau} > m$. With $\hat{k}$, we can express the valid cross-correlation as
\begin{equation*}
	*_{\text{v}}(k,x) = \mathcal{I}_{n_x-2m}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \mathcal{I}_{n_x}(x)
	)
\end{equation*}
because
\begin{equation*}
	\mathcal{S}_{m}(\hat{k}) 
	= \mathcal{S}_{m}(\mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k)))
	= \mathcal{S}_{m+(-m-1)}(\mathcal{I}_{2m+1}(k))
	= \mathcal{S}_{-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}

\begin{definition}
	(Symmetric kernel)
	We call an odd kernel $k \in \mathbb{R}^{2m+1}$ symmetric if
	\begin{equation*}
		\hat{k}(\tau) = \hat{k}(-\tau)
	\end{equation*}
	for all $\tau \in \mathbb{Z}$.
\end{definition}

Let us now define symplectic convolution layers by combining padding and valid 
cross-convolution.
\begin{definition}
	(Convolution layers)
	Given a symmetric kernel $k \in \mathbb{R}^{2m+1}$ and padding
	$(\bullet)_{\text{pad},m} = c_{\text{pad},m}$ or $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$,
	we call the upper and lower unit triangular layers with bias $b \in \R^{2d}$ and layer transform
	\begin{equation*}
		\activation_{\mathcal{C}}(p) := *_{\text{v}}(k,((\bullet)_{\text{pad}})(p))
	\end{equation*}
	the convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$.
\end{definition}
The symmetric kernel $k \in \mathbb{R}^{2m+1}$ is a learnable parameter.
We can set a constant bias (for example $b=0$) or also make the bias learnable.
If $(\bullet)_{\text{pad},m} = c_{\text{pad},m}$, the padding values $l,r \in \R^m$ can either
be constant or learnable.

With $\hat{(\bullet)}_{\text{pad},m}(p) := \mathcal{I}_{d+2m}((\bullet)_{\text{pad},m}(p))$,
we can write the layer transform as
\begin{equation}\label{eq_conv_layer_transform}
	\activation_{\mathcal{C}}(p) =
	\mathcal{I}_{d}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \hat{(\bullet)}_{\text{pad}}(p)
	)
\end{equation}

\begin{lemma}\label{jac_linear_map}
	Let $f: \mathbb{R}^d \to \mathbb{R}^d$ be a linear map. Then the Jacobian matrix
	$\jac{f}{x}$ is constant, i.e.
	\begin{equation*}
		\jac{f}{x} \bigg|_{x = v} = \jac{f}{x} \bigg|_{x = w} \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
	and for the Jacobian-vector product holds
	\begin{equation*}
		\lb \jac{f}{x} \bigg|_{x = v} \rb w = f(w) \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
\end{lemma}
\begin{proof}
	$f$ is linear $\implies$ There exists a matrix representation $f(x) = Ax$ with
	$A \in \mathbb{R}^{d \times d}$ \\
	$\implies \jac{f}{x} \big|_{x=v} = A = \text{const.} \quad \forall v \in \mathbb{R}^d$
	$\implies \lb \jac{f}{x} \big|_{x=v} \rb w = Aw = f(w) \quad \forall v,w \in \mathbb{R}^d$
\end{proof}
As the Jacobian matrix for a linear map is constant, we omit the evaluation point, i.e.
$\jac{f}{x} \big|_{x=v} = \jac{f}{x}$.

\begin{theorem}\label{thm_conv_const_pad_symplectic}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $(\bullet)_{\text{pad},m} = c_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	We have to show that the Jacobian of the layer transform $\activation_{\mathcal{C}}(p)$
	is symmetric (\cref{jacobi_symmetric}).
	Because of \cref{cpad_affine}, it suffices to show the case $l,r=0$ (the Jacobian
	of a constant has only zero-valued entries). For $l,r=0$, the layer transform 
	$\activation_{\mathcal{C}}(p)$ 
	is a linear map, because it is a composition of linear maps only.

	Define the bilinear form
	\begin{equation}\label{eq_bilinear_proof_conv}
		b : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R},
		\quad b(v,w) := \ip{v}{\lb \jac{\activation_{\mathcal{C}}}{p} \rb w}
	\end{equation}
	To show symmetry of the Jacobian, we show that 
	$b(e_i, e_j) = b(e_j, e_i)$ for all $i,j=1,\dots,d$.
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{\lb \jac{\activation_{\mathcal{C}}}{p} \rb e_j}
		= \ip{e_i}{\activation_{\mathcal{C}}(e_j)} \quad \text{(\cref{jac_linear_map})} \\
		&\stackrel{\cref{eq_conv_layer_transform}}{=} \ip{e_i}{\mathcal{I}_{d}^{-1} (
			\mathcal{S}_{m}(\hat{k}) * \hat{c}_{\text{pad}}(e_j)
		)} \\
		&= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				\underbrace{(\hat{c}_{\text{pad}}(e_j))(\tau+a)}_{
					= \delta_{(\tau+a) (j+m)}
				}
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((j-i+m)-m) = \hat{k}(j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\begin{theorem}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	With $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$, the layer transform 
	$\activation_{\mathcal{C}}$ is linear, because it is a composition of linear maps. Thus,
	we proof the statement the same way as \cref{thm_conv_const_pad_symplectic} above.
	For $\tau,a \in \mathbb{Z}$ we have
	\begin{equation*}
		\hat{s}_{\text{pad},m}(e_j)(\tau + a) = 
		\delta_{(\tau + a) (m-j+1)} + \delta_{(\tau + a) (j+m)} + \delta_{(\tau + a) (2d+m-j+1)}
	\end{equation*}
	Consequently, for $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$ and $i,j=1, \dots, d$, 
	the bilinear form \cref{eq_bilinear_proof_conv} becomes
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				(\hat{s}_{\text{pad}}(e_j))(\tau+a)
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((m-j+1-i)-m) + \hat{k}((j+m-i)-m) + \hat{k}((2d+m-j+1-i)-m) \\
		&= \hat{k}(1-j-i) + \hat{k}(j-i) + \hat{k}(1+2d-j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\subsubsection*{Parametrization of a symmetric kernel}

Let ${b_1, b_2, \dots b_m} \in \mathbb{R}^{2m+1}$ be a basis of the space 
$\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.
The symmetric kernel $k$ is then parametrized by the coefficients $\beta \in \mathbb{R}^m$ via
\begin{equation*}
	k = (b_1, b_2, \dots, b_m) \beta = \beta_1 b_1 + \beta_2 b_2 + \dots + \beta_m b_m
\end{equation*}

The basis vectors $b_1, \dots, b_m \in \mathbb{R}^{2m+1}$ for the canonical basis are given by
\begin{equation*}
	(b_i)_{j+m+1} = \hat{k}(b_i)(j) = \begin{cases}
		1 &: \abs{j} = i-1 \\
		0 &:else
	\end{cases} 
	\quad (j=-m, \dots, m)
\end{equation*}

Another possible basis choice inspired by finite differences is
\todo{Cite paper with similar idea?}
\begin{align*}
	b_1^{FD} = (0, \dots, 0,& 1,0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_2^{FD} = (0, \dots, 0, 1,-&2,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_3^{FD} = (0, 0, \dots, 0, 1,-4,& 6,-4,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1} \\
	&\vdots
\end{align*}
The entries for a basis vector $b_i^{FD} \in \mathbb{R}^{2m+1}$ $(1 \leq i \leq m)$ 
originate from Pascal's triangle.
\begin{equation*}
	\lb b_i^{FD} \rb_{j+m+1} = \hat{k}(b_i^{FD})(j) = \begin{dcases}
		(-1)^j \binom{2(i-1)}{j+i-1} &: \abs{j} < i \\
		0 &: else
	\end{dcases}
	\quad (j=-m, \dots, m)
\end{equation*}

\todo{Move this to Appendix?}
The symmetry of the kernel basis vectors $b_i^{FD}$ follows with the definition of the binomial coefficient.
\begin{align*}
	\binom{2(i-1)}{j+i-1} &= \frac{2(i-1)!}{(j+i-1)!(2(i-1)-(j+i-1))!} \\
	&= \frac{2(i-1)!}{(j+i-1)!(i-j-1)!}
\end{align*}
Thus we have
\begin{equation*}
	\binom{2(i-1)}{(-j)+i-1} = \binom{2(i-1)}{j+i-1}
\end{equation*}
and for $i=1, \dots, m$ and $j= 0, \dots, m$
\begin{align*}
	\hat{k}(b_i^{FD})(-j) &= \begin{dcases}
		(-1)^{-j} \binom{2(i-1)}{(-j)+i-1} : \abs{-j} < i \\
		0 : else
	\end{dcases} \\
	&= \begin{dcases}
		(-1)^{j} \binom{2(i-1)}{j+i-1} : \abs{j} < i \\
		0 : else
	\end{dcases} \\
	&= \hat{k}(b_i^{FD})(j) 
\end{align*}
So $b_i^{FD}$ $(i=1, \dots, m)$ form a valid basis of
the space $\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.

It turns out in our numerical experiments that
parametrization plays an important role how well a neural network learns.

\subsection{Convolution Gradient Layers}

Valid cross-correlation is a linear map. Therefore
there exists a representation matrix $A_{*_{\text{v}}}$ with $*_{\text{v}}(p) = A_{*_{\text{v}}}p$.
Transposed valid cross-correlation 
is defined as the linear map associated with the transpose $A_{*_{\text{v}}}^T$ of $A_{*_{\text{v}}}$.

Again, most popular neural network libraries say convolution, but actually implement
valid cross-correlation. In this case transposed convolution actually refers to
transposed valid cross-correlation.

For large $d \in \mathbb{N}$, it can make sense to use convolution inside Gradient layers 
instead of a full matrix $W$.

\begin{equation*}
	\mathcal{G}_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}_{up} \qpvec := \uppersympop{\hat{\activation}_g} := \begin{pmatrix}
		q + K^T diag(a) \activation(Kp + b) \\
		p
	\end{pmatrix}
\end{equation*}

In particular, a Gradient layer with width $n \in \mathbb{N}$ can be implemented by 
using valid cross-correlation (with stride) for $K^T \in \mathbb{R}^{d \times n}$ and the
corresponding transposed valid cross-correlation (with stride) for $K \in \mathbb{R}^{n \times d}$.
The choice of transpose is intentional, because we want to upscale the input $p \in \mathbb{R}^d$ ($n >> d$). 
Cross-correlation without padding decreases the dimension. Therefore we first apply 
transposed cross-correlation on $p$, because the transpose increases the dimension.
A large kernel size $s_k$ and a large stride value $k$ allow for significant upscaling.
We emphasize that the kernel $k$ has not to be symmetric in this setting.
\todo{Cite similar idea}

\subsection{Normalization}

Batch normalization is a standard method to accelerate training initially proposed by
\citeauthor{batchnorm-ioffe15} in \cite{batchnorm-ioffe15}. 
We introduce a possibility to incorporate batch normalization
into activation and gradient layers while maintaining symplecticity.
\todo{Cite similar idea}

\todo{Sigmoid activation function saturates for large values. Vanishing gradient.
Therefore normalize input before applying activation function.}

Let us define the batch normalization transformation
\begin{equation*}
	\eta_{\gamma, \beta} : \mathbb{R}^n \to \mathbb{R}^n,\quad
	\eta_{\gamma, \beta}(x) 
	:= diag(\gamma)diag \lb \lb \frac{1}{\sqrt{\lb \sigma^2_\mathcal{B} \rb_i + \epsilon}} \rb_{i=1}^n \rb 
	(x-\mu_\mathcal{B}) + \beta
\end{equation*}
where $n$ is the input dimension of the input $x \in \mathbb{R}^n$ and $\gamma, \beta \in \mathbb{R}^{d}$ 
are learnable parameters.
The scalar $\epsilon \in \mathbb{R}$ is a small positive value to avoid division by zero.
$\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ refers to the mini-batch variance and
$\mu_\mathcal{B} \in \mathbb{R}^{d}$ refers to the mini-batch mean.

The learnable parameters $\gamma, \beta \in \mathbb{R}^{d}$ allow the neural network to modify
the normalization during training if necessary. The batch normalization transform is able to 
represent the identity transform by setting appropriate $\gamma, \beta$.

Given a mini-batch $\mathcal{B}$ with size $k$ and input training data 
$x_1, x_2, \dots, x_k \in \mathbb{R}^{n}$, the
mean and variance are estimated by
\begin{align*}
	\mu_\mathcal{B} &= \frac{1}{k} \sum_{i=1}^{k} x_i \\
	\sigma^2_\mathcal{B} &= \frac{1}{k} \sum_{i=1}^{k} (x_i - \mu_\mathcal{B})^2
\end{align*}

During training the mini-batch variance $\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ and
the mini-batch $\mu_\mathcal{B} \in \mathbb{R}^{d}$ are continously updated in the forward pass.
To be precise, for a new mini-batch $\mathcal{B} = \{ x_1, x_2, \dots, x_k \}$, 
the mean $\mu_\mathcal{B}$ and variance $\sigma^2_\mathcal{B}$
are updated based on $\mathcal{B}$, before $\eta_{\gamma, \beta}(x_j)$ for
$x_j \in \mathcal{B}$ is evaluated (see Algorithm 1).
When training has finished the neural network does not update $\sigma^2$ and $\mu_\mathcal{B}$ anymore.
Instead, the neural network remembers the last value from training.

Note that the batch normalization transformation may be incorporated into a layer
deep inside a neural network. 
If this is the case, the mini-batch $\mathcal{B}$ refers to the collective output of the previous layer.

\begin{algorithm}\label{algo_batch_norm}
	\caption{Batch normalization transform}
	\textbf{Input:} $x_j$ and mini batch $\mathcal{B} = \{x_1, x_2, \dots, x_{k}\}$  \\
	\textbf{Output:} $\eta_{\gamma, \beta}(p_j)$
	\setstretch{1.5}
	\begin{algorithmic}
		\If{training\_mode} \Comment{Update mean and variane if training, otherwise
		keep previous values}
			\State $\mu_\mathcal{B} \gets \frac{1}{k} \sum_{i=1}^{k} x_i$
			\State $\sigma^2_\mathcal{B} \gets \frac{1}{k} \sum_{i=1}^{k} 
			(x_i - \mu_\mathcal{B})^2$
		\EndIf
		\State \Return 
		$diag(\gamma)diag 
		\lb \lb \frac{1}{\sqrt{\lb \sigma^2_\mathcal{B} \rb_i + \epsilon}} \rb_{i=1}^n \rb
		(x_j-\mu_\mathcal{B}) + \beta$
	\end{algorithmic}
\end{algorithm}

The batch normalization transform can be implemented in a straightforward way with modern
neural network libraries, as during training the Jacobian $\jac{\eta_{\gamma, \beta}}{(\gamma, \beta)}$ 
is obtained via automatic differentiation.

If the training data is small enough, so that splitting the data into multiple mini batches
is not necessary, the mean and variance are calculated for the whole training data set. This is the
case for our numerical experiments, as we work with very small training data sets.

\subsubsection{Normalized gradient layers}

\begin{alignat*}{2}
	\mathcal{G}^{\eta}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}^{\eta}_{up} \qpvec &:= \uppersympop{\hat{\activation}^{\eta}_g} := \begin{pmatrix}
		q + \hat{\activation}^{\eta}_g(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{G}^{\eta}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}^{\eta}_{low} \qpvec &:= \lowersympop{\hat{\activation}^{\eta}_g} := \begin{pmatrix}
		q \\
		\hat{\activation}^{\eta}_g(q) + p
	\end{pmatrix}
\end{alignat*}
with $\hat{\activation}^{\eta}_g : \mathbb{R}^d \to \mathbb{R}^d$ defined as
\begin{equation*}
	\hat{\activation}^{\eta}_g(p) := 
	K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
	diag(a) \activation(\eta_{\gamma, \beta}(Kp+b))
\end{equation*}
where $K \in \mathbb{R}^{n \times d}$ and $a,b, \gamma, \beta \in \mathbb{R}^n$
are learnable parameters. $\activation : \mathbb{R} \to \mathbb{R}$ 
is an activation function, which is applied element-wise.
$n \in \mathbb{N}$ denotes the width of the gradient layer.
$\sigma^2_\mathcal{B}$ refers to the mini-batch variance of $Kp+b$ for $\mathcal{G}^{\eta}_{up}$
or $Kq+b$ for $\mathcal{G}^{\eta}_{low}$.

\todo{Add special note for convolution gradient layers, respect convolution when normalizing.}

\todo{$diag(\gamma)$ and $\beta$ redundant, but numerical experiments better result
with $diag(\gamma)$ and ease of implementation. Double-check?
Having $diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb$ greatly improves
learning.}

\todo{Maybe add illustration for normalized gradient layer}

\begin{corollary}
	Given an activation function $\sigma \in C^1$
	the normalized gradient layers $\mathcal{G}^{\eta}_{up}$ and $\mathcal{G}^{\eta}_{down}$
	are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$ and
	\begin{equation*}
		V(p) := \onevec{n}^Tdiag(a)\mathcal{A}(\eta_{\gamma, \beta}(Kp+b))
	\end{equation*}

	We have $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{equation*}
		\jac{BN_{\gamma, \beta}(p)}{p} = 
		diag(\gamma)diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb
	\end{equation*}

	Then
	\begin{align*}
		\grad{V(p)} &= \left(\jac{V}{p}(p)\right)^T \\
		&= \left( 1_n^T diag(a) diag \lb \activation(\eta_{\gamma, \beta}(Kp+b))\rb
		\jac{BN_{\gamma, \beta}(p)}{p} K \right)^T \\
		&= \left( 1_n^T diag(a) diag \lb \activation(\eta_{\gamma, \beta}(Kp+b))\rb
		diag(\gamma) diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb K \right)^T \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag\left(\activation(\eta_{\gamma, \beta}(Kp+b))\right) diag(a) 1_n \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag(a) diag\left(\activation(\eta_{\gamma, \beta}(Kp+b))\right) 1_n \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag(a) \activation(\eta_{\gamma, \beta}(Kp+b)) = \hat{\activation}^{BN}_g(p)
	\end{align*}

	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}^{\eta}_{up}$ and $\mathcal{G}^{\eta}_{low}$ are symplectic.
\end{proof}

\subsubsection{Normalized activation layers}

\begin{alignat*}{2}
	\mathcal{N}^{\eta}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{N}^{\eta}_{up} \qpvec &:= \uppersympop{\hat{\activation}^{\eta}_a} := \begin{pmatrix}
		q + \hat{\activation}^{\eta}_a(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{N}^{\eta}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{N}^{\eta}_{low} \qpvec &:= \lowersympop{\hat{\activation}^{\eta}_a} := \begin{pmatrix}
		q \\
		\hat{\activation}^{\eta}_a(q) + p
	\end{pmatrix}
\end{alignat*}
with $\hat{\activation}^{\eta}_a : \mathbb{R}^d \to \mathbb{R}^d$ defined as
\begin{equation*}
	\hat{\activation}^{\eta}_a(p) := 
	diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
	diag(a) \activation(\eta_{\gamma, \beta}(p))
\end{equation*}
where $a, \gamma, \beta \in \mathbb{R}^d$ are learnable parameters and 
$\activation : \mathbb{R} \to \mathbb{R}$ an activation function,
which is applied element-wise. $\sigma^2_\mathcal{B}$ refers to the mini-batch variance of $p$
for $\mathcal{N}^{\eta}_{up}$ or $q$ for $\mathcal{N}^{\eta}_{low}$.

\begin{corollary}
	Given an activation function $\sigma \in C^1$
	the normalized activation layers $\mathcal{N}^{\eta}_{up}$ and $\mathcal{N}^{\eta}_{low}$
	are symplectic.
\end{corollary}
\begin{proof}
	Symplecticity follows because an activation layer is a gradient layer
	(choose $n=d$, $K=I_d$ and $b=0$). We have already shown that a gradient layer is symplectic.
\end{proof}

\section{Relation to geometric integrators}

The symplectic Euler and Störmer-Verlet schemes are two geometric integrators. Geometric integrators
are numerical integrators for ODE systems, which preserve a geometric property. In our context,
this geometric property is symplecticity. Precisely, if we denote the numerical integrator with
$\phi^{\text{h}}_{t,H} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$, the map $\phi^{\text{h}}_{t,H}$ is
symplectic.

The two variants of the symplectic Euler scheme are given by
\begin{equation*}
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H} (p_{n+1}, q_n) \\
			q_{n+1} &= q_n + h \grad[p]{H}(p_{n+1}, q_n)	
	\end{split}
	\quad\quad \text{or} \quad\quad
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H}(p_n, q_{n+1}) \\
			q_{n+1} &= q_n + h \grad[p]{H}{p}(p_n, q_{n+1})	
	\end{split}
\end{equation*}

The $p$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	p_{n+1/2} &= p_n - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_n) \\[7pt]
	q_{n+1} &= q_n + \frac{h}{2} \lb \grad[p]{H}(p_{n+1/2}, q_n) + \grad[p]{H}(p_{n+1/2}, q_{n+1}) \rb \\[7pt]
	p_{n+1} &= p_{n+1/2} - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_{n+1})
\end{align*}
and the $q$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	q_{n+1/2} &= q_n + \frac{h}{2} \grad[p]{H}(p_n, q_{n+1/2}) \\[7pt]
	p_{n+1} &= q_n - \frac{h}{2} \lb \grad[q]{H}(p_n, q_{n+1/2}) + \grad[q]{H}(p_{n+1}, q_{n+1/2}) \rb \\[7pt]
	q_{n+1} &= q_{n+1/2} + \frac{h}{2} \grad[p]{H}(p_{n+1}, q_{n+1/2})
\end{align*}
We refer to \citet[p.~189 and p.~190]{hairer2006} for details.

If the Hamiltonian $H$ is separable, i.e. $H(q,p) = U(q) + T(p)$, the sympletic Euler and Störmer-Verlet
schemes become explicit. Then given $\grad[p]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $p \mapsto \grad[p]{H}(p)$ 
and $\grad[q]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $q \mapsto \grad[q]{H}(q)$,
the left variant of the symplectic Euler scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix} \begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix} 
\end{equation*}
and the right variant of the symplectic Euler scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

Similarly, the $p$-staggered Störmer-Verlet scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & 0 \\
		-\frac{h}{2} \grad[q]{H}
	\end{bmatrix}
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-\frac{h}{2} \grad[q]{H}
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
and the $q$-staggered Störmer-Verlet scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & \frac{h}{2} \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{bmatrix}
		I & \frac{h}{2} \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

To conclude, the explicit Euler and Störmer-Verlet schemes can be expressed with
the same unit triangular structure we use for SympNets. \todo{Recurrent networks?}

Symplecticity for the explicit Euler and Störmer-Verlet schemes follows directly 
from \cref{gradient_corollary} and the fact that the composition of symplectic maps is again symplectic.

\section{Numerical experiments}

\subsection{Low-dimensional systems}

\subsubsection{Harmonic Oscillator}

\subsubsection{Simple Pendulum}

Training data from swinging and rotating domain

\begin{tikzpicture}
	\begin{axis}[
		title={Phase plot (swinging case)},
		xlabel={q},
		ylabel={p},
		no markers, smooth
	]

	\addplot[
		color=red
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/phase_plot.csv};

	%\addplot[
	%	color=blue
	%] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/la-sympnet/swinging_case/phase_plot.csv};

	\addplot[
		color=green
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/swinging_case/phase_plot.csv};

	%\addplot[
	%	color=blue,
	%	dashed
	%] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/normalized-la-sympnet/swinging_case/phase_plot.csv};

	\addplot[
		color=green,
		dashed,
		select coords between index={0}{72}
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/normalized-g-sympnet/swinging_case/phase_plot.csv};
		
	\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
	\begin{axis}[
		title={Phase plot (rotating case)},
		xlabel={q},
		ylabel={p},
	]

	\addplot[
		color=red,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/phase_plot.csv};

	\addplot[
		color=blue,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/la-sympnet/rotating_case/phase_plot.csv};

	\addplot[
		color=green,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/rotating_case/phase_plot.csv};

	\addplot[
		color=blue,
		dashed,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/normalized-la-sympnet/rotating_case/phase_plot.csv};

	\addplot[
		color=green,
		dashed,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/normalized-g-sympnet/rotating_case/phase_plot.csv};
		
	\end{axis}
\end{tikzpicture}

\begin{tikzpicture}
	\begin{axis}[
		no markers, smooth,
		title={Training loss},
		xlabel={Epoch},
		ylabel={Loss},
		ymax=1e-3,
		ymin=0,
		restrict y to domain=0:1e-3
	]

	\addplot[
		color=blue
	] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/loss.csv};

	\addplot[
		color=red
	] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/normalized-g-sympnet/loss.csv};
		
	\end{axis}
\end{tikzpicture}

\subsection{High-dimensional systems}

\todo{Describe Hamiltonian PDEs. A PDE describes system locally, thus it makes sense to share parameters, for
example in the form of CNNs etc.}

Assumptions:
\begin{itemize}
	\item Original PDE has constant coefficients \todo{Or rather should not depend on $x$ itself?}
	\item Equidistant grid points
\end{itemize}

\todo{Explain why we need assumptions.}

\subsubsection{Linear wave equation}

\todo{The choice of basis for the symmetric convolution kernels is critical (at least for fast convergence)!}

\subsubsection{Sine-Gordon}

\todo{Bias important if non-zero Dirichlet boundaries, but only at borders! // Padding modes}

\todo{Activation layer with sin works well if $a \in \mathbb{R}$, i.e. activation function is applied
isotropic.}

\todo{Maybe compare different optimizers (SGD, Adam, AdamW, amsgrad, ...)}

\todo{Activation layers: Differentiate between $a \in \mathbb{R}$ and $a \in \mathbb{R}^d$}

%----------------------------------------------------------------------------------------
%
% Resume
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\section{R\'esum\'e}
\subsection{Summary and conclusion}

\subsection{Outlook}

%----------------------------------------------------------------------------------------
%
% Appendices
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\section{First Appendix Section}

\newpage~\newpage
\section{Declaration of authorship}

\vspace{3cm}

\begin{table}[h!]
\centering
\begin{tabular}{|p{13cm}|}
\hline\\
	\todo{Change to bachelor thesis}
	I hereby certify
	\begin{enumerate}
		\item that this thesis has been composed by me and is based on my own work, unless stated otherwise,
		\item that all direct or indirect sources used are acknowledged as references and all extracts from work of others, either verbatim or in spirit, are stated as such,
		\item that neither the thesis itself nor parts of this thesis have been part of another examination procedure,
		\item that neither the thesis itself nor parts of this thesis have been published and
		\item that all copies of this thesis, either digital or printed, coincide.
	\end{enumerate}
	Therewith, this declaration of authorship is in accordance with the examination regulations from 29th July 2013 of the master's program \emph{Simulation Technology} of the University of Stuttgart.\\\\
\hline
\end{tabular}
\end{table}

\vspace{4cm}
\hrulefill\\
Name
\hspace{7cm}
Date, City, Signature
\end{appendices}
%----------------------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\addcontentsline{toc}{section}{References}
\bibliographystyle{abbrvnat}
\bibliography{../../literature/references.bib}

\end{document}
