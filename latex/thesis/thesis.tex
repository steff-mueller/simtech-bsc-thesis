\documentclass[twoside,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,bindingoffset=0.5cm,inner=2.5cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[a4paper,bindingoffset=1cm,inner=2cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry} %,showframe
\usepackage{helvet}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\sfdefault}
% \usepackage[german,ngerman]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amstext,amssymb,bm}
\usepackage{xcolor,color}
\usepackage{pifont}
\usepackage{array}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{pgf}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{theoremref}
%%% Title page
\usepackage{common/titlePageST}
%%% Appendix in TOC
\usepackage[toc,page]{appendix}
%%% Setstrech on title page
\usepackage{setspace}
%%% Tilde in URL in literature
\usepackage{url}
%%% multiple rows
\usepackage{multirow}
%%% fancy column and row seperators
\usepackage{hhline}
%%% custom items in enumerate and itemize
\usepackage{enumitem}

\clubpenalty=5000
\widowpenalty=5000

\setlength{\emergencystretch}{2cm}
%----------------------------------------------------------------------------------------
%	abbreviation includes
%----------------------------------------------------------------------------------------
\input{common/default_abbrev}
\input{common/specific_abbrev}
%%% Clever refing
\usepackage[capitalise,noabbrev]{cleveref}

%----------------------------------------------------------------------------------------
%	References with cleveref
%----------------------------------------------------------------------------------------
\crefformat{equation}{(#2#1#3)}

%----------------------------------------------------------------------------------------
%	Theorem environments
%----------------------------------------------------------------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

%----------------------------------------------------------------------------------------
%	fancyhdr
%----------------------------------------------------------------------------------------
\usepackage{fancyhdr}

\makeatletter
\newcommand{\theauthor}{Steffen Müller} %
\makeatother

\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}
\fancyfoot[OL,ER]{University of Stuttgart} % inner
\fancyfoot[OR,EL]{\thepage} % outer
\fancyhead[OL,ER]{\theauthor} % inner 
\fancyhead[OR,EL]{IANS -- Institute of Applied Analysis and Numerical Simulation} % outer
\fancyfoot[C]{}

% \headsep=4mm
% \footskip=4mm
\parindent=0mm
\parskip=6pt
% \renewcommand{\footskip}{3pt}

%----------------------------------------------------------------------------------------
%	Something
%----------------------------------------------------------------------------------------
\usepackage{textpos}
\setlength{\TPHorizModule}{1mm}%
\setlength{\TPVertModule}{1mm}%
% \headsep=5mm
% \footskip=5mm
\pagestyle{fancy}
\newcommand{\articleheading}[3]{
{\large #1}\\[3mm]
{\Large\bf #2}\\[3mm]
{\large #3}
}

%----------------------------------------------------------------------------------------
%	Start Document
%----------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
%----------------------------------------------------------------------------------------
%
% TITLE PAGE
%
%----------------------------------------------------------------------------------------
%------------------------------------------
\begin{titlePageST}
%------------------------------------------
\makeLogo%
{-10pt}{
\includegraphics[width=0.7\textwidth]{figures/logos/simtech.pdf}
}%
{0pt}{
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/logos/ians.pdf}
	\end{center}}%
{0pt}{
\begin{flushright}
	\vspace{-10pt}
	\includegraphics[width=0.9\textwidth]{figures/logos/unistuttgart_logo_englisch_cmyk.eps}
\end{flushright}}%
\vspace{35pt}%
%------------------------------------------
\makeHeader%
[Research Group: Numerical Mathematics] %
{Institute of Applied Analysis and Numerical Simulation} %
\vspace{50pt}%
%------------------------------------------
\makeTitle%
{Simulation Technology Degree Course} %
{Bachelor Thesis} %
\vspace{80pt}%
%------------------------------------------
\makeTitleThesis%
{Symplectic Neural Networks}
\vspace{90pt}%
%------------------------------------------
\begin{supervisorST}{3}%
\addSuper%
{First Reviewer}%
{Prof. Dr. B. Haasdonk}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\addSuper%
{Second Reviewer}%
{Prof. Dr. D. Pflüger}%
{Institute of Parallel and Distributed\\[-0.2cm]
Systems (Scientific Computing)}%
\addSuper%
{Advisor}%
{Patrick Buchfink, M.Sc.}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\end{supervisorST}%
\vspace{80pt}%
%------------------------------------------
\begin{authorST}{Submitted by}%
\addAuthorInfo{Author}{Steffen Müller}
\addAuthorInfo{Student ID}{3260643}
\addAuthorInfo{SimTech ID}{119} %
\addAuthorInfo{Submission Date}{...} %FILLIN
\end{authorST}%
%------------------------------------------
\end{titlePageST}
%----------------------------------------------------------------------------------------
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ABSTRACT
%
%----------------------------------------------------------------------------------------
\section*{Abstract}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ACKNOWLEDGEMENTS
%
%----------------------------------------------------------------------------------------
\section*{Acknowledgements}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% TOC
%
%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\newpage\thispagestyle{plain}\null
%----------------------------------------------------------------------------------------
%
% Begin with document content
%
%----------------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic} 
%----------------------------------------------------------------------------------------
%
% Introduction
%
%----------------------------------------------------------------------------------------
\section{Introduction}

\todo{Introduce Hamiltonain systems with general skew-symmetric $S$?}

\subsection{Outline}

\subsection{Notation}

Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be an arbitrary function and $x \in \mathbb{R}^n$
a vector. When we write $f(x)$ we mean the element-wise application of $f$ 
on the vector $x \in \mathbb{R}^n$, i.e. $f(x) = \lb
	f(x_1), ..., f(x_n)
\rb^T$.

The $d$-by-$d$ identity matrix is denoted by $I_d$. If the dimension $d$ can be inferred
from context we may just write $I$.

$\onevec{n}$ denotes the $n$-dimensional $1$-vector.

\todo{Introduce block matrix notation for general functions}

We denote the $k$-th partial derivative of a function $f: \mathbb{R}^n \to \mathbb{R},
x \mapsto f(x)$ with $\deldel{x_k} f(x)$.

If $f: \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$ is a multi-dimensional function

\todo{"partial derivative" of a vector, Jacobian matrices}

\todo{Gradient}

\todo{Describe $d$}

%----------------------------------------------------------------------------------------
%
% Content
%
%----------------------------------------------------------------------------------------
\newpage
\section{Problem setup}

\section{Architecture}

In this section we describe the architecture of our symplectic neural networks.
The linear layers, activation layers and gradient layers are recapitulated as 
proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
The convolution layers and normalized variants of the activation and gradient layers
are contributions of this work. 
\todo{Convolution gradient module, refer to similar reference}
We supplement proofs for symplecticity for all layer types. 
A similar architecture was initially proposed by \citeauthor{Deco1995} in \cite{Deco1995} 
\todo{Have a deeper look at \cite{Deco1995}}.

\todo{we use that the composition of symplectic maps is again symplectic}

\todo{Relation to Residual Networks / ResNet}

\begin{lemma}\thlabel{jacobi_symmetric}
	Let
	\begin{equation*}
		f_{up} \qpvec := \uppersympop{g}
	\end{equation*}
	and
	\begin{equation*}
		f_{low} \qpvec := \lowersympop{g}
	\end{equation*}
	where $g: \mathbb{R}^d \to \mathbb{R}^d,\; p \mapsto g(p)$ is a function in 
	$C^1(\mathbb{R}^d, \mathbb{R}^d)$. 

	$f_{up}$ and $f_{low}$ are symplectic if and only if the Jacobian matrix $\jac{g}{p}$
	is symmetric.
\end{lemma}
\begin{proof}
	We show the result for $f_{up}$ only. The proof is analogous for $f_{low}$.

	\begin{equation*}
		\deldel[f_{up}]{(q,p)} = \begin{pmatrix}
			I & \deldel[g]{p} \\
			0 & I
		\end{pmatrix}
	\end{equation*}
	It follows
	\begin{align*}
		\jacp{f_{up}}{(q,p)}^T J \jacp{f_{up}}{(q,p)} 
		&= \left(\deldel[f_{up}]{(q,p)}\right)^T \begin{pmatrix}
			0 & I \\
			-I & -\jac{g}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			I & 0 \\
			\jacp{g}{p}^T & Id
		\end{pmatrix} \begin{pmatrix}
			0 & I \\
			-I & -\jac{g}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			0 & I \\
			-I & \jacp{g}{p}^T-\jac{g}{p}
		\end{pmatrix}
	\end{align*}
	Thus $f_{up}$ is symplectic if and only if $\jacp{g}{p}^T-\jac{g}{p}=0$, 
	i.e. if and only if the Jacobian $\deldel[g]{p}$ is everywhere symmetric.
\end{proof}

\begin{corollary}\thlabel{matrix_symmetric}
	Let
	\begin{equation*}
		f_{up} \qpvec := \begin{pmatrix}
			I & S \\
			0 & I
		\end{pmatrix} \qpvec
	\end{equation*}
	and
	\begin{equation*}
		f_{low} \qpvec := \begin{pmatrix}
			I & 0 \\
			S & I
		\end{pmatrix} \qpvec
	\end{equation*}
	where $S \in \mathbb{R}^{dxd}$. 

	$f_{up}$ and $f_{low}$ are symplectic if and only if the matrix $S$
	is symmetric.
\end{corollary}

The next corollary is useful to construct symplectic maps.
\begin{corollary}\thlabel{gradient_corollary}
	Let $V: \mathbb{R}^d \to \mathbb{R}, \; p \mapsto V(p)$ be a function in 
	$C^2(\mathbb{R}^d, \mathbb{R})$. 
	
	Then
	\begin{equation*}
		f_{up} \qpvec := \uppersympop{\grad{V}}
	\end{equation*}
	and
	\begin{equation*}
		f_{low} \qpvec := \lowersympop{\grad{V}}
	\end{equation*}
	define a symplectic map. We call $V$ a potential.
\end{corollary}
\begin{proof}
	The Jacobian matrix of $\grad{V}$ corresponds to the Hessian matrix of $V$,
	i.e. $\jac{(\grad{V})}{p} = HD$. \todo{Notation for Hessian}
	The Hessian is symmetric, thus the result follows with \thref{jacobi_symmetric}.
\end{proof}

\todo{Relation to 'Generating functions' in symplectic literature?}

\subsection{Linear layers}

\begin{align*}
	\ell_{up}: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad \ell_{up} \qpvec &:= \begin{pmatrix}
		I & S \\
		0 & I
	\end{pmatrix} \qpvec + b, \\
	\ell_{low}: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad \ell_{low} \qpvec &:= \begin{pmatrix}
		I & 0 \\
		S & I
	\end{pmatrix} \qpvec + b
\end{align*}
where $S \in \mathbb{R}^{d \times d}$ symmetric and $b \in \mathbb{R}^{2d}$
are learnable parameters. In practice, we parametrize the symmetric matrix $S\in \mathbb{R}^{d \times d}$
with $S = A^T + A$ via another arbitrary matrix $A\in \mathbb{R}^{d \times d}$, as
most optimization methods used for learning neural networks are designed for
unconstrained optimization problems.

It follows directly from \thref{matrix_symmetric} that the linear layers are
symplectic. Alternatively, we can apply \thref{gradient_corollary} by
choosing the potential $V(p) := p^TAp$.

\begin{proof}
	Let
	\begin{align*}
		\phi_1: \mathbb{R}^d \to \mathbb{R}^{2d}, \quad &\phi_1(p) := \begin{pmatrix}
			p \\
			Ap
		\end{pmatrix} \\
		\phi_2: \mathbb{R}^{2d} \to \mathbb{R}, \quad &\phi_2(p, \hat{p}) := 
		p^T\hat{p}
	\end{align*}
	
	Then $V(p) = \phi_2(\phi_1(p))$ and the corresponding Jacobian matrices are
	\begin{align*}
		\jac{\phi_1}{p}(p) &= \begin{pmatrix}
			I \\
			A
		\end{pmatrix} \\
		\jac{\phi_2}{(p,\hat{p})}(p, \hat{p}) &= \begin{pmatrix}
			\hat{p}^T && p^T
		\end{pmatrix}
	\end{align*}
	Thus
	\begin{align*}
		\jac{V}{p}(p) &= 
		\jac{\phi_2}{(p,\hat{p})} (\phi_1(p))
		\jac{\phi_1}{p}(p) \\
		&= \begin{pmatrix}
			(Ap)^T && p^T
		\end{pmatrix}
		\begin{pmatrix}
			I \\
			A
		\end{pmatrix} \\
		&= (Ap)^T + p^TA = (Ap + A^Tp)^T \\
		&= ((A+A^T)p)^T
	\end{align*}
	Therefore $\grad{V}(p) = \lb \deldel{p}V(p) \rb^T = (A+A^T)p = Sp$ with
	$S := A+A^T$ symmetric. Symplecticity follows with \thref{gradient_corollary}.
\end{proof}

We may enhance expressivity of a linear layer by alternately composing multiple
$\ell_{up}$ and $\ell_{low}$:

\begin{align*}
	\mathcal{L}^{n}_{up} \qpvec &= \begin{pmatrix}
		I && 0 / S_n \\
		S_n / 0 && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && 0 \\
		S_2 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix} + b \\
	\mathcal{L}^{n}_{low} \qpvec &= \begin{pmatrix}
		I && S_n / 0 \\
		0 / S_n && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && 0 \\
		S_1 && I
	\end{pmatrix} + b
\end{align*}

$\mathcal{L}^{n}_{up}$ and $\mathcal{L}^{n}_{low}$ are again symplectic because they 
are a composition of the symplectic maps $\ell_{up}$ and $\ell_{low}$.

\citeauthor{jin2020unit} show in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map. In other words, 
the set of all possible $\mathcal{L}^{9}_{up}$ is equal to the set of all symplectic linear maps.
\todo{check how \cite{jin2020unit} appears in bibliography}

It does not make sense to put two upper linear layers or two lower linear layers after each other,
because this reduces to a single upper or lower linear layer:

\begin{proof}
	\begin{align*}
		\ell_{up,2}\lb\ell_{up,1} \qpvec\rb &=
		\begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix}
		\lb
		\begin{pmatrix}
			I && S_1 \\
			0 && I
		\end{pmatrix}
		\qpvec + b_1
		\rb + b_2 \\
		&= \begin{pmatrix}
			I && S_1 + S_2 \\
			0 && I
		\end{pmatrix} \qpvec
		+ \begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix} b_1
		+ b_2 \\
		&= \begin{pmatrix}
			I && S \\
			0 && I
		\end{pmatrix} \qpvec + b
	\end{align*}
	with $S := S_1 + S_2$ and $b := \begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix} b_1
	+ b_2$.
\end{proof}

\subsection{Convolution layers}

Convolution layers are a subset of linear layers. \todo{Thus symplectic}

\todo{Try motivating convolution layers by showing that the linear layers explode if $d$ becomes large.}

\todo{Bias important if non-zero Dirichlet boundaries, but only at borders!}

\todo{Padding modes, e.g. replicate for sine gordon}

\todo{Interesting: Finite difference basis only contains even-order derivatives, otherwise it
would not be symplectic (symmetric). What happens if Hamiltonian contains even-order derivative?
Symmetric Hessian does not explain it, because we only take gradient of Hamiltonian and not Hessian.
Relation to Störmer-Verlet.}

\subsection{Activation layers}

$V(p) = 1_n^T(\int \sigma)(p)$

\begin{equation*}
	\grad{V}(p) = \lb\jac{V}{p}(p)\rb^T = \lb1_n^Tdiag\lb\sigma(p)\rb\rb^T
	= diag(\activation(p)) 1_n
	= \activation(p)
\end{equation*}

\todo{Differentiate between $a \in \mathbb{R}$ and $a \in \mathbb{R}^d$?}

\subsection{Gradient layers}

\begin{equation*}
	\mathcal{G}_{up} \qpvec := \uppersympop{\hat{\activation}} := \begin{pmatrix}
		q + K^T diag(a) \activation(Kp + b) \\
		p
	\end{pmatrix}
\end{equation*}
\begin{equation*}
	\mathcal{G}_{low} \qpvec := \lowersympop{\hat{\activation}} := \begin{pmatrix}
		q \\
		K^T diag(a) \activation(Kq + b) + p
	\end{pmatrix}
\end{equation*}
where $K \in \mathbb{R}^{n \times d}$ and $a,b \in \mathbb{R}^n$
are learnable parameters.
$n \in \mathbb{N}$ denotes the width of the gradient layer and can be chosen freely.

\newtheorem*{glayer}{Lemma}
\begin{glayer}
	Given an activation function $\sigma \in C^1$ the gradient layers $\mathcal{G}_{\{up,low\}}$ 
	are symplectic.
\end{glayer}
\begin{proof}
	Let $\mathcal{F}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{F} = \activation$ and
	$V(p) := \onevec{n}^Tdiag(a)\mathcal{F}(Kp+b)$.

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{align*}
		\nabla V(p) &= \left(\jac{V}{p}(p)\right)^T \\
		&= \left(1_n^Tdiag(a)diag\left(\sigma(Kp+b)\right)K\right)^T \\
		&= K^Tdiag(a)diag\left(\sigma(Kp+b)\right)1_n \\
		&= K^Tdiag(a)\sigma(Kp+b)
	\end{align*}

	With \thref{gradient_corollary} follows that the Gradient layers $\mathcal{G}_{\{up,low\}}$
	are symplectic.
\end{proof}

\todo{For high dimensions $K$ and $K^T$ can be implemented as transposed convolution
and convolution operators.}

\subsection{Normalization}

\todo{Introduce batch normalization from literature.}

\todo{Normalized activation layers}

\todo{Normalized gradient layers}

\section{Relation to geometric integrators}

\section{Numerical experiments}

\subsection{Low-dimensional systems}

\subsubsection{Harmonic Oscillator}

\subsubsection{Simple Pendulum}

\subsection{High-dimensional systems}

\todo{Describe Hamiltonian PDEs. A PDE describes system locally, thus it makes sense to share parameters, for
example in the form of CNNs etc.}

Assumptions:
\begin{itemize}
	\item Original PDE has constant coefficients \todo{Or rather should not depend on $x$ itself?}
	\item Equidistant grid points
\end{itemize}

\todo{Explain why we need assumptions.}

\subsubsection{Linear wave equation in one dimension}

\todo{The choice of basis for the symmetric convolution kernels is critical (at least for fast convergence)!}

\subsubsection{Sine-Gordon}

\todo{Activation layer with sin works well if $a \in \mathbb{R}$, i.e. activation function is applied
isotropic.}

\todo{Maybe compare different optimizers (SGD, Adam, AdamW, amsgrad, ...)}

\subsubsection{Linear elastic beam in three dimensions}

\section{Implementation}

\todo{Describe use of PyTorch, TensorBoard, Docker, MLFlow, Unit Tests, ...}

%----------------------------------------------------------------------------------------
%
% Resume
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\section{R\'esum\'e}
\subsection{Summary and conclusion}

\subsection{Outlook}

%----------------------------------------------------------------------------------------
%
% Appendices
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\section{First Appendix Section}

\newpage~\newpage
\section{Declaration of authorship}

\vspace{3cm}

\begin{table}[h!]
\centering
\begin{tabular}{|p{13cm}|}
\hline\\
	I hereby certify
	\begin{enumerate}
		\item that this thesis has been composed by me and is based on my own work, unless stated otherwise,
		\item that all direct or indirect sources used are acknowledged as references and all extracts from work of others, either verbatim or in spirit, are stated as such,
		\item that neither the thesis itself nor parts of this thesis have been part of another examination procedure,
		\item that neither the thesis itself nor parts of this thesis have been published and
		\item that all copies of this thesis, either digital or printed, coincide.
	\end{enumerate}
	Therewith, this declaration of authorship is in accordance with the examination regulations from 29th July 2013 of the master's program \emph{Simulation Technology} of the University of Stuttgart.\\\\
\hline
\end{tabular}
\end{table}

\vspace{4cm}
\hrulefill\\
Name
\hspace{7cm}
Date, City, Signature
\end{appendices}
%----------------------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\addcontentsline{toc}{section}{References}
\bibliographystyle{abbrvnat}
\bibliography{../../literature/references.bib}

\end{document}
