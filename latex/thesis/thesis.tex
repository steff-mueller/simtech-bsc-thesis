\documentclass[twoside,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,bindingoffset=0.5cm,inner=2.5cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[a4paper,bindingoffset=1cm,inner=2cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry} %,showframe
\usepackage{helvet}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\sfdefault}
% \usepackage[german,ngerman]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amstext,amssymb,bm}
\usepackage{mathtools}
\usepackage{xcolor,color}
\usepackage{pifont}
\usepackage{array}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{pgf}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algpseudocode,algorithm}
%%% Title page
\usepackage{common/titlePageST}
%%% Appendix in TOC
\usepackage[toc,page]{appendix}
%%% Setstrech on title page
\usepackage{setspace}
%%% Tilde in URL in literature
\usepackage{url}
%%% multiple rows
\usepackage{multirow}
%%% fancy column and row seperators
\usepackage{hhline}
%%% custom items in enumerate and itemize
\usepackage{enumitem}
%%% custom format for algorithm comments
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em // #1}

\clubpenalty=5000
\widowpenalty=5000

\setlength{\emergencystretch}{2cm}
%----------------------------------------------------------------------------------------
%	abbreviation includes
%----------------------------------------------------------------------------------------
\input{common/default_abbrev}
\input{common/specific_abbrev}
%%% Clever refing
\usepackage[capitalise,noabbrev]{cleveref}

%----------------------------------------------------------------------------------------
%	References with cleveref
%----------------------------------------------------------------------------------------
\crefformat{equation}{(#2#1#3)}

%----------------------------------------------------------------------------------------
%	Theorem environments
%----------------------------------------------------------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

%----------------------------------------------------------------------------------------
%	fancyhdr
%----------------------------------------------------------------------------------------
\usepackage{fancyhdr}

\makeatletter
\newcommand{\theauthor}{Steffen Müller} %
\makeatother

\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}
\fancyfoot[OL,ER]{University of Stuttgart} % inner
\fancyfoot[OR,EL]{\thepage} % outer
\fancyhead[OL,ER]{\theauthor} % inner 
\fancyhead[OR,EL]{IANS -- Institute of Applied Analysis and Numerical Simulation} % outer
\fancyfoot[C]{}

% \headsep=4mm
% \footskip=4mm
\parindent=0mm
\parskip=6pt
% \renewcommand{\footskip}{3pt}

%----------------------------------------------------------------------------------------
%	Something
%----------------------------------------------------------------------------------------
\usepackage{textpos}
\setlength{\TPHorizModule}{1mm}%
\setlength{\TPVertModule}{1mm}%
% \headsep=5mm
% \footskip=5mm
\pagestyle{fancy}
\newcommand{\articleheading}[3]{
{\large #1}\\[3mm]
{\Large\bf #2}\\[3mm]
{\large #3}
}

%----------------------------------------------------------------------------------------
%	Start Document
%----------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
%----------------------------------------------------------------------------------------
%
% TITLE PAGE
%
%----------------------------------------------------------------------------------------
%------------------------------------------
\begin{titlePageST}
%------------------------------------------
\makeLogo%
{-10pt}{
\includegraphics[width=0.7\textwidth]{figures/logos/simtech.pdf}
}%
{0pt}{
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/logos/ians.pdf}
	\end{center}}%
{0pt}{
\begin{flushright}
	\vspace{-10pt}
	\includegraphics[width=0.9\textwidth]{figures/logos/unistuttgart_logo_englisch_cmyk.eps}
\end{flushright}}%
\vspace{35pt}%
%------------------------------------------
\makeHeader%
[Research Group: Numerical Mathematics] %
{Institute of Applied Analysis and Numerical Simulation} %
\vspace{50pt}%
%------------------------------------------
\makeTitle%
{Simulation Technology Degree Course} %
{Bachelor Thesis} %
\vspace{80pt}%
%------------------------------------------
\makeTitleThesis%
{Symplectic Neural Networks}
\vspace{90pt}%
%------------------------------------------
\begin{supervisorST}{3}%
\addSuper%
{First Reviewer}%
{Prof. Dr. B. Haasdonk}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\addSuper%
{Second Reviewer}%
{Prof. Dr. D. Pflüger}%
{Institute of Parallel and Distributed\\[-0.2cm]
Systems (Scientific Computing)}%
\addSuper%
{Advisor}%
{Patrick Buchfink, M.Sc.}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\end{supervisorST}%
\vspace{80pt}%
%------------------------------------------
\begin{authorST}{Submitted by}%
\addAuthorInfo{Author}{Steffen Müller}
\addAuthorInfo{Student ID}{3260643}
\addAuthorInfo{SimTech ID}{119} %
\addAuthorInfo{Submission Date}{...} %FILLIN
\end{authorST}%
%------------------------------------------
\end{titlePageST}
%----------------------------------------------------------------------------------------
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ABSTRACT
%
%----------------------------------------------------------------------------------------
\section*{Abstract}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ACKNOWLEDGEMENTS
%
%----------------------------------------------------------------------------------------
\section*{Acknowledgements}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% TOC
%
%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\newpage\thispagestyle{plain}\null
%----------------------------------------------------------------------------------------
%
% Begin with document content
%
%----------------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic} 
%----------------------------------------------------------------------------------------
%
% Introduction
%
%----------------------------------------------------------------------------------------
\section{Introduction}

\subsection{Outline}

\subsection{Notation}

Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be an arbitrary function and $x \in \mathbb{R}^n$
a vector. When we write $f(x)$ we mean the element-wise application of $f$ 
on the vector $x \in \mathbb{R}^n$, i.e. $f(x) = \lb
	f(x_1), ..., f(x_n)
\rb^T$.

The $d$-by-$d$ identity matrix is denoted by $I_d$. If the dimension $d$ can be inferred
from context we may just write $I$.

$\onevec{n}$ denotes the $n$-dimensional $1$-vector.

\todo{Introduce block matrix notation for general functions}

We denote the $k$-th partial derivative of a function $f: \mathbb{R}^n \to \mathbb{R},
x \mapsto f(x)$ with $\deldel{x_k} f(x)$.

If $f: \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$ is a multi-dimensional function

\todo{"partial derivative" of a vector, Jacobian matrices}

\todo{Gradient}

\todo{Describe $d$}

\begin{equation*}
	J := \begin{pmatrix}
		0 & I_d \\
		-I_d & 0
	\end{pmatrix}
\end{equation*}

%----------------------------------------------------------------------------------------
%
% Content
%
%----------------------------------------------------------------------------------------
\newpage
\section{Problem setup}

\begin{definition}
	A matrix $A \in \mathbb{R}^{2d \times 2d}$ is called symplectic if $A^TJA=J$.
\end{definition}

\begin{definition}
	A differentiable map $\phi : U \to \mathbb{R}^{2d}$ (where $U \subset \mathbb{R}^{2d}$ is an open set)
	is called symplectic if the Jacobian matrix $\jac{\phi}{x}$ is everywhere symplectic, i.e.
	\begin{equation*}
		\lb \jac{\phi}{x} \rb^T J \lb \jac{\phi}{x} \rb = J
	\end{equation*}
\end{definition}

\todo{Introduce Hamiltonain systems with general skew-symmetric $S$?}
A Hamiltonian ODE system can be written in canonical form as
\begin{align*}
	\dot{y} &= J \grad{H}(y) \\
	y(t_0) &= y_0
\end{align*}
where $y: \mathbb{R} \to \mathbb{R}^{2d},\, t \mapsto y(t) = \begin{pmatrix}
	q(t) \\
	p(t)
\end{pmatrix}$ and $y_0 = \begin{pmatrix}
	q_0 \\
	p_0
\end{pmatrix} \in \mathbb{R}^{2d}$ the initial value for $t_0 \in \mathbb{R}$. 
The map $H: \mathbb{R}^{2d} \to \mathbb{R}$ is called the Hamiltonian 
or the total energy. $q = q(t) \in \mathbb{R}^d$ are called generalized coordinates
and $p=p(t) \in \mathbb{R}^d$ are called conjugate momenta. 
The phase space $\mathbb{R}^{2d}$ has even dimension for Hamiltonian systems.

Note that the Hamiltonian $H$ is a first integral, i.e. the total energy is preserved, because
\begin{align*}
	\ddt H(y(t)) &= \lsb \grad{H}(y(t)) \rsb^T \dot{y}(t) = 
	\lsb \grad{H}(y(t)) \rsb^T J \grad{H}(y(t)) \\
	&= \begin{pmatrix}
		\grad[q]{H}(y(t)) & \grad[p]{H}(y(t))
	\end{pmatrix} \begin{pmatrix}
		\grad[p]{H}(y(t)) \\
		- \grad[q]{H}(y(t))
	\end{pmatrix} = 0
\end{align*}

Let $\phi_{t,H} : U \subset \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ be the flow for a Hamiltonian system 
with Hamiltonian $H$ for a fixed time step $t$, i.e.
\begin{equation*}
	\phi_{t,H}\begin{pmatrix}
		q_0 \\
		p_0
	\end{pmatrix}
	= \begin{pmatrix}
		q(t; q_0, p_0) \\
		p(t; q_0, p_0)
	\end{pmatrix}
\end{equation*}
where $q(t; q_0, p_0)$ and $p(t; q_0, p_0)$ denote the solution of the Hamiltonian system
at time $t$ for initial conditions $y_0 = \begin{pmatrix}
	q_0 \\
	p_0
\end{pmatrix}$. Poincaré has shown that the flow of a Hamiltonian system is symplectic.

\begin{theorem}(Poincaré 1899)
	Let $H: \mathbb{R}^{2d} \to \mathbb{R}$ be a twice continuously differentiable
	function on $U \subset \mathbb{R}^{2d}$. Then, for each fixed $t$, the flow
	$\phi_{t,H}$ is a symplectic map wherever it is defined.
\end{theorem}
\begin{proof}
	We refer to \citet[Theorem 2.4, p.~184]{hairer2006} 
	or to \citet[Theorem 1, p.~54]{leimkuhler_reich_2005}.
\end{proof}

Note that a Hamiltonian ODE system in general can be written as
\begin{align*}
	\dot{y} &= S \grad{H}(y) \\
	y(t_0) &= y_0
\end{align*}
with an arbitrary nondegenerate skew-symmetric matrix $S \in \mathbb{R}^{2d \times 2d}$.
However there always exists a transformation to express such a Hamiltonian ODE system in
canonical form (see \citet[Remark 3.8]{peng2016}). \todo{Maybe add more details?}

Our goal is to learn the flow map $\phi_{t,H} : U \to \mathbb{R}^{2d}$ with a neural network
for fixed $t$.
This leads to the idea that we embed symplecticity into the neural network itself structurally. 

\section{Architecture}

In this section we describe the architecture of our symplectic neural networks.
The linear layers, activation layers and gradient layers are recapitulated as 
proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
The convolution layers and normalized variants of the activation and gradient layers
are contributions of this work. 
\todo{Convolution gradient module, refer to similar reference}
We proof symplecticity for all layer types. 

We use that the composition of symplectic maps is again symplectic. So we impose that every
layer of the neural network must be symplectic, which leads to the overall neural network
to be symplectic.

All layers we introduce will have unit triangular structure in block matrix notation, 
i.e. can either be expressed in upper variant
\begin{equation*}
	L_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{L} \qpvec = \uppersympop{\hat{\activation}_L}
	= \begin{pmatrix}
		q + \hat{\activation}_L(p) \\
		p
	\end{pmatrix}
\end{equation*}
or lower variant
\begin{equation*}
	L_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{L} \qpvec = \lowersympop{\hat{\activation}_L}
	= \begin{pmatrix}
		q \\
		\hat{\activation}_L(q) + p
	\end{pmatrix}
\end{equation*}
where $\hat{\activation}_L : \mathbb{R}^d \to \mathbb{R}^d$ is a Layer-specific transformation.
In order that $L_{up}$ and $L_{low}$ are symplectic we have to demand additional requirements
to $\hat{\activation}_L$ as the following lemma and corollaries will show.

The structure was initially proposed by \citeauthor{Deco1995} in \cite{Deco1995} 
\todo{Have a deeper look at \cite{Deco1995}} for volume-conserving neural networks.

\todo{Relation to Residual Networks / ResNet}

\begin{lemma}\label{jacobi_symmetric}
	Let
	\begin{equation*}
		f_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{up} \qpvec := \uppersympop{g}
	\end{equation*}
	and
	\begin{equation*}
		f_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{low} \qpvec := \lowersympop{g}
	\end{equation*}
	where $g: \mathbb{R}^d \to \mathbb{R}^d,\; p \mapsto g(p)$ is a function in 
	$C^1(\mathbb{R}^d, \mathbb{R}^d)$. 

	$f_{up}$ and $f_{low}$ are symplectic if and only if the Jacobian matrix $\jac{g}{p}$
	is symmetric.
\end{lemma}
\begin{proof}
	We show the result for $f_{up}$ only. The proof is analogous for $f_{low}$.

	\begin{equation*}
		\deldel[f_{up}]{(q,p)} = \begin{pmatrix}
			I & \deldel[g]{p} \\
			0 & I
		\end{pmatrix}
	\end{equation*}
	It follows
	\begin{align*}
		\jacp{f_{up}}{(q,p)}^T J \jacp{f_{up}}{(q,p)} 
		&= \left(\deldel[f_{up}]{(q,p)}\right)^T \begin{pmatrix}
			0 & I \\
			-I & -\jac{g}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			I & 0 \\
			\jacp{g}{p}^T & Id
		\end{pmatrix} \begin{pmatrix}
			0 & I \\
			-I & -\jac{g}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			0 & I \\
			-I & \jacp{g}{p}^T-\jac{g}{p}
		\end{pmatrix}
	\end{align*}
	Thus $f_{up}$ is symplectic if and only if $\jacp{g}{p}^T-\jac{g}{p}=0$, 
	i.e. if and only if the Jacobian $\deldel[g]{p}$ is everywhere symmetric.
\end{proof}

\begin{corollary}\label{matrix_symmetric}
	Let
	\begin{equation*}
		f_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{up} \qpvec := \begin{pmatrix}
			I & S \\
			0 & I
		\end{pmatrix} \qpvec
	\end{equation*}
	and
	\begin{equation*}
		f_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{low} \qpvec := \begin{pmatrix}
			I & 0 \\
			S & I
		\end{pmatrix} \qpvec
	\end{equation*}
	where $S \in \mathbb{R}^{dxd}$. 

	$f_{up}$ and $f_{low}$ are symplectic if and only if the matrix $S$
	is symmetric.
\end{corollary}

The next corollary is useful to construct symplectic maps.
\begin{corollary}\label{gradient_corollary}
	Let $V: \mathbb{R}^d \to \mathbb{R}, \; p \mapsto V(p)$ be a function in 
	$\mathbb{C}^2$. 
	
	Then
	\begin{equation*}
		f_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{up} \qpvec := \uppersympop{\grad{V}}
	\end{equation*}
	and
	\begin{equation*}
		f_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{low} \qpvec := \lowersympop{\grad{V}}
	\end{equation*}
	define a symplectic map with $\grad{V} : \mathbb{R}^d \to \mathbb{R}^d,\, p \mapsto \grad{V}(p)$. 
	We call $V$ a potential.
\end{corollary}
\begin{proof}
	The Jacobian matrix of $\grad{V}$ corresponds to the Hessian matrix of $V$,
	i.e. $\jac{(\grad{V})}{p} = HD$. \todo{Notation for Hessian}
	The Hessian is symmetric, thus the result follows with \cref{jacobi_symmetric}.
\end{proof}

\todo{Relation to 'Generating functions' in symplectic literature?}

\subsection{Linear layers}

\begin{alignat*}{2}
	\ell_{up}&: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad \ell_{up} \qpvec &:= \begin{pmatrix}
		I & S \\
		0 & I
	\end{pmatrix} \qpvec + b, \\[7pt]
	\ell_{low}&: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad \ell_{low} \qpvec &:= \begin{pmatrix}
		I & 0 \\
		S & I
	\end{pmatrix} \qpvec + b
\end{alignat*}
where $S \in \mathbb{R}^{d \times d}$ symmetric and $b \in \mathbb{R}^{2d}$
are learnable parameters. In practice, we parametrize the symmetric matrix $S\in \mathbb{R}^{d \times d}$
with $S = A^T + A$ via another arbitrary matrix $A\in \mathbb{R}^{d \times d}$, as
most optimization methods used for learning neural networks are designed for
unconstrained optimization problems.

It follows directly from \cref{matrix_symmetric} that the linear layers are
symplectic. Alternatively, we can apply \cref{gradient_corollary} by
choosing the potential $V(p) := p^TAp$.

\begin{proof}
	Let
	\begin{align*}
		\phi_1: \mathbb{R}^d \to \mathbb{R}^{2d}, \quad &\phi_1(p) := \begin{pmatrix}
			p \\
			Ap
		\end{pmatrix} \\
		\phi_2: \mathbb{R}^{2d} \to \mathbb{R}, \quad &\phi_2(p, \hat{p}) := 
		p^T\hat{p}
	\end{align*}
	
	Then $V(p) = \phi_2(\phi_1(p))$ and the corresponding Jacobian matrices are
	\begin{align*}
		\jac{\phi_1}{p}(p) &= \begin{pmatrix}
			I \\
			A
		\end{pmatrix} \\
		\jac{\phi_2}{(p,\hat{p})}(p, \hat{p}) &= \begin{pmatrix}
			\hat{p}^T && p^T
		\end{pmatrix}
	\end{align*}
	Thus
	\begin{align*}
		\jac{V}{p}(p) &= 
		\jac{\phi_2}{(p,\hat{p})} (\phi_1(p))
		\jac{\phi_1}{p}(p) \\
		&= \begin{pmatrix}
			(Ap)^T && p^T
		\end{pmatrix}
		\begin{pmatrix}
			I \\
			A
		\end{pmatrix} \\
		&= (Ap)^T + p^TA = (Ap + A^Tp)^T \\
		&= ((A+A^T)p)^T
	\end{align*}
	Therefore $\grad{V}(p) = \lb \deldel{p}V(p) \rb^T = (A+A^T)p = Sp$ with
	$S := A+A^T$ symmetric. Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

We may enhance expressivity of a linear layer by alternately composing multiple
$\ell_{up}$ and $\ell_{low}$. We define 
$\mathcal{L}^{n}_{up},\, \mathcal{L}^{n}_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ with

\begin{align*}
	\mathcal{L}^{n}_{up} \qpvec &:= \begin{pmatrix}
		I && 0 / S_n \\
		S_n / 0 && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && 0 \\
		S_2 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix} + b \\
	\mathcal{L}^{n}_{low} \qpvec &:= \begin{pmatrix}
		I && S_n / 0 \\
		0 / S_n && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && 0 \\
		S_1 && I
	\end{pmatrix} + b
\end{align*}

$\mathcal{L}^{n}_{up}$ and $\mathcal{L}^{n}_{low}$ are again symplectic because they 
are a composition of the symplectic maps $\ell_{up}$ and $\ell_{low}$.

\citeauthor{jin2020unit} show in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map. In other words, 
the set of all possible $\mathcal{L}^{9}_{up}$ is equal to the set of all symplectic linear maps.
\todo{check how \cite{jin2020unit} appears in bibliography}

It does not make sense to put two upper linear layers or two lower linear layers after each other,
because this reduces to a single upper or lower linear layer:

\begin{proof}
	We show the statement for two upper linear layers.
	\begin{align*}
		\ell_{up,2}\lb\ell_{up,1} \qpvec\rb &=
		\begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix}
		\lb
		\begin{pmatrix}
			I && S_1 \\
			0 && I
		\end{pmatrix}
		\qpvec + b_1
		\rb + b_2 \\
		&= \begin{pmatrix}
			I && S_1 + S_2 \\
			0 && I
		\end{pmatrix} \qpvec
		+ \begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix} b_1
		+ b_2 \\
		&= \begin{pmatrix}
			I && S \\
			0 && I
		\end{pmatrix} \qpvec + b
	\end{align*}
	with $S := S_1 + S_2$ and $b := \begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix} b_1
	+ b_2$.
\end{proof}

\subsection{Convolution layers}

\todo{Motivate convolution layers}

Given a kernel $k \in \mathbb{R}^{s_k}$ with kernel size $s_k \in \mathbb{N}$
the $1$-dimensional convolution operator is defined as
\begin{equation*}
	conv1d[k]: \mathbb{R}^{L_{in}} \to \mathbb{R}^{L_{out}}, \quad
	\lb conv1d[k](x) \rb_i := \sum_{u=1}^{s_k} k_u x_{i+u-1} \quad
	(1 \leq i \leq L_{out})
\end{equation*}
where $L_{in}$ is the input dimension and $L_{out} = L_{in} - s_k + 1$.
\todo{Briefly mention more general versions with stride and dilation.}
The convolution operator reduces the input dimension $L_{in}$ (if $s_k > 1$).

\todo{Add illustration for convolution}

As we will justify later we only use kernels with odd kernel size $s_k = 2m+1$, $m \in \mathbb{N}$.
Therefore from now on we always implicitly assume a kernel with odd kernel size $s_k = 2m+1$.
\todo{in this section, later conv gradient module not?}
To simplify notation we introduce new indexing for kernels by counting from $-m$ to $m$,
i.e. $k = (k_{-m}, k_{-m+1}, \dots, k_{-1} k_0, k_1, \dots, k_{m})^T \in \mathbb{R}^{2m+1}$.
With new indexing the convolution operator becomes 
\begin{equation*}
	\lb conv1d[k](x) \rb_i = \sum_{u=-m}^{m} k_u x_{i+u+m} \quad
	(1 \leq i \leq L_{out})
\end{equation*}

We call a kernel $k \in \mathbb{R}^{2m+1}$ symmetric if $k_i = k_{-i}$ for all $i = 0, \dots, m$.

A padding operator increases the input dimension by padding
the input boundaries. Let us define $1$-dimensional constant padding and symmetric padding operators.
Let $s_p \in \mathbb{N}$ be the padding size and $L_{in}$ the input dimension.

Constant padding adds constant values at the left and right boundary.
\begin{align*}
	&constant\_pad[s_p,l,r] : \mathbb{R}^{L_{in}} \to \mathbb{R}^{L_{in}+2s_p}, \\[7pt]
	&(constant\_pad[s_p,l,r](x))_i :=
	\begin{cases}
		l_i & 1 \leq i \leq s_p \\
		x_{i-s_p} & s_p+1 \leq i \leq L_{in}+s_p \\
		r_{i-L_{in}+s_p} & L_{in}+s_p+1 \leq i \leq L_{in} + 2s_p
	\end{cases}
	\quad (1 \leq i \leq L_{in}+2s_p)
\end{align*}
where $l,r \in \mathbb{R}^{s_p}$ are the constant padding values for the left and right boundaries.

Symmetric padding mirrors the inner values at the left and right boundaries.
\begin{align*}
	&symmetric\_pad[s_p] : \mathbb{R}^{L_{in}} \to \mathbb{R}^{L_{in}+2s_p}, \\[7pt]
	&(symmetric\_pad[s_p](x))_i :=
	\begin{cases}
		x_{s_p-i+1} & 1 \leq i \leq s_p \\
		x_{i-s_p} & s_p+1 \leq i \leq L_{in}+s_p \\
		x_{2L_{in}+s_p+1-i} & L_{in}+s_p+1 \leq i \leq L_{in} + 2s_p
	\end{cases}
	\quad (1 \leq i \leq L_{in}+2s_p)
\end{align*}
For symmetric padding it must hold $L_{in} \geq s_p$.

\todo{Add illustrations for padding}

We define the symplectic $1$-dimensional convolution layers by
\begin{alignat*}{2}
	\mathcal{C}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad&\mathcal{C}_{up} \qpvec := \uppersympop{conv1d[k] \circ pad[s_p,\dots]} \\[7pt]
	\mathcal{C}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	&\mathcal{C}_{low} \qpvec := \lowersympop{conv1d[k] \circ pad[s_p,\dots]}
\end{alignat*}
where kernel $k \in \mathbb{R}^{2m+1}$ symmetric, $s_p = m$ and either
\begin{equation*}
	pad[s_p,\dots] = constant\_pad[s_p,l,r],\; l,r \in \mathbb{R}^{s_p}
\end{equation*}
or
\begin{equation*}
	pad[s_p,\dots] = symmetric\_pad[s_p]
\end{equation*}
For $pad[s_p,\dots] = symmetric\_pad[s_p]$ it must hold $d \geq m$.
The kernel $k \in \mathbb{R}^{2m+1}$ is learnable during training.
If using constant padding $l,r \in \mathbb{R}^{s_p}$ can be either fixed or learnable during training.

Indeed $conv1d[k] \circ pad[s_p,\dots]$ maps from $\mathbb{R}^d$ to $\mathbb{R}^d$ because
\begin{align*}
	L_{in,pad[s_p,\dots]} &= d \\
	L_{out,pad[s_p,\dots]} &= L_{in,pad[s_p,\dots]} + 2s_p = d+2s_p = d+2m \\
	L_{in,conv1d[k]} &= L_{out,pad[s_p,\dots]} = d+2m \\
	L_{out,conv1d[k]} &= L_{in,conv1d[k]} - s_k + 1 = (d+2m) - (2m+1) + 1 = d
\end{align*}

Furthermore the two conditions
\begin{align}
	L_{in,conv1d[k]} &= L_{out,pad[s_p,\dots]} = d+2s_p \label{conv_dim1} \\ 
	L_{out,conv1d[k]} &= L_{in,conv1d[k]} - s_k + 1 = d \label{conv_dim2}
\end{align}
indeed imply that the kernel size $s_k$ must be odd.
\begin{equation*}
	\text{\cref{conv_dim1} in \cref{conv_dim2}:} \quad d+2s_p - s_k + 1 = d \iff s_k = 2s_p+1
\end{equation*}

Let ${b_1, b_2, \dots b_m} \in \mathbb{R}^{2m+1}$ be a basis of the space 
$\{ k \in \mathbb{R}^{2m+1} : k \text{ is symmetric} \}$.
The symmetric kernel $k$ is then parametrized by the coefficients $\beta \in \mathbb{R}^m$ via
\begin{equation*}
	k = (b_1, b_2, \dots, b_m) \beta = \beta_1 b_1 + \beta_2 b_2 + \dots + \beta_m b_m
\end{equation*}

We can choose the canonical basis with basis vectors
\begin{align*}
	b_1^C = (0, \dots, 0, 0, 0, &1, 0, 0, 0, \dots, 0)^T \in \mathbb{R}^{2m+1}, \\
	b_2^C = (0, \dots, 0, 0, 1, &0, 1, 0, 0, \dots, 0)^T \in \mathbb{R}^{2m+1}, \\
	b_3^C = (0, \dots, 0, 1, 0, &0, 0, 1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1}, \\
	&\vdots \\
	b_m^C = (1, 0, \dots, &0, \dots, 0, 1)^T \in \mathbb{R}^{2m+1}
\end{align*}

Another possible basis choice inspired by finite differences is
\todo{Cite paper with similar idea?}
\begin{align*}
	b_1^{FD} = (0, \dots, 0,& 1,0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_2^{FD} = (0, \dots, 0, 1,-&2,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_3^{FD} = (0, 0, \dots, 0, 1,-4,& 6,-4,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1} \\
	&\vdots
\end{align*}
The entries for a basis vector $b_i^{FD} \in \mathbb{R}^{2m+1}$ $(1 \leq i \leq m)$ 
originate from Pascal's triangle.
\begin{equation*}
	\lb b_i^{FD} \rb_j = \begin{dcases}
		(-1)^j \binom{2(i-1)}{j+i-1} : \abs{j} < i \\
		0 : else
	\end{dcases}
	\quad (1 \leq i \leq m, -m \leq j \leq m )
\end{equation*}
where $j$ is indexed from $-m$ to $m$ in accordance to how we defined indexing for kernel entries.

\todo{Move this to Appendix?}
The symmetry of the kernel basis vectors $b_i^{FD}$ follows with the definition of the binomial coefficient.
\begin{align*}
	\binom{2(i-1)}{j+i-1} &= \frac{2(i-1)!}{(j+i-1)!(2(i-1)-(j+i-1))!} \\
	&= \frac{2(i-1)!}{(j+i-1)!(i-j-1)!}
\end{align*}
Thus we have
\begin{equation*}
	\binom{2(i-1)}{(-j)+i-1} = \binom{2(i-1)}{j+i-1}
\end{equation*}
and
\begin{align*}
	\lb b_i^{FD} \rb_{-j} &= \begin{dcases}
		(-1)^{-j} \binom{2(i-1)}{(-j)+i-1} : \abs{-j} < i \\
		0 : else
	\end{dcases} \\
	&= \begin{dcases}
		(-1)^{j} \binom{2(i-1)}{j+i-1} : \abs{j} < i \\
		0 : else
	\end{dcases} \\
	&= (b_i^{FD})_j
\end{align*}
for $i=1, \dots, m$ and $j= 0, \dots, m$. So $b_i^{FD}$ $(i=1, \dots, m)$ are a valid basis of
the space $\{ k \in \mathbb{R}^{2m+1} : k \text{ is symmetric} \}$.

\todo{Interesting: Finite difference basis only contains even-order derivatives, otherwise it
would not be symplectic (symmetric). What happens if Hamiltonian contains even-order derivative?
Symmetric Hessian does not explain it, because we only take gradient of Hamiltonian and not Hessian.
Relation to Störmer-Verlet.}

As it turns out in our numerical experiments
parametrization plays an important role how well a neural network learns.

\todo{Maybe move the proof of these theorems to Appendices to keep this cleaner?}

\begin{theorem}
	The $1$-dimensional convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with $pad[m,\dots] = constant\_pad[m,l,r]$ ($l,r \in \mathbb{R}^p$) are symplectic.
\end{theorem}
\begin{proof}
	We show that the Jacobian matrix 
	\begin{equation*}
		\jac{\lb conv1d \lsb k \rsb \circ constant\_pad \lsb m,l,r \rsb \rb}{p}
	\end{equation*}
	is symmetric according to \cref{jacobi_symmetric}.

	We have
	\begin{equation*}
		\lb \jac{conv1d \lsb k \rsb}{x} \rb_{ij} = \begin{cases}
			k_{j-i-m} & : \abs{j-i-m} \leq m \\
			0 & : else
		\end{cases}
		\quad (1 \leq i \leq d, 1 \leq j \leq d+2m)
	\end{equation*}
	and
	\begin{equation*}
		\lb \jac{constant\_pad \lsb m,l,r \rsb}{x} \rb_{ij} = \begin{cases}
			1 & : i-j=m \\
			0 & : else
		\end{cases}
		\quad (1 \leq i \leq d+2m, 1 \leq j \leq d)
	\end{equation*}
	Let $1 \leq i,j \leq d$. By the chain rule we have
	\begin{align*}
		\lb \jac{\lb conv1d \lsb k \rsb \circ constant\_pad \lsb m,l,r \rsb \rb}{p} \rb_{ij}
		&= \lb  
			\jac{conv1d \lsb k \rsb}{x}
			\jac{constant\_pad \lsb m,l,r \rsb}{x} 
		\rb_{ij} \\
		&= \sum_{u=1}^{d+2m} \lb \jac{conv1d \lsb k \rsb}{x} \rb_{iu}
		\lb \jac{constant\_pad \lsb m,l,r \rsb}{x} \rb_{uj} \\
		&= \lb \jac{conv1d \lsb k \rsb}{x} \rb_{i(j+m)}
		\quad (\text{bc. } u-j=m \iff u=j+m) \\
		&= \begin{cases}
			k_{(j+m)-i-m} & : \abs{(j+m)-i-m} \leq m \\
			0 & : else
		\end{cases} \\
		&= \begin{cases}
			k_{j-i} & : \abs{j-i} \leq m \\
			0 & : else
		\end{cases}
	\end{align*}
	Then 
	\begin{align*}
		\lb \jac{\lb conv1d \lsb k \rsb \circ constant\_pad \lsb m,l,r \rsb \rb}{p} \rb_{ji}
		&= \begin{cases}
			k_{i-j} & : \abs{i-j} \leq m \\
			0 & : else
		\end{cases} \\
		&= \begin{cases}
			k_{j-i} & : \abs{j-i} \leq m \\
			0 & : else
		\end{cases} \quad \text{($k$ symmetric)} \\
		&= \lb \jac{\lb conv1d \lsb k \rsb \circ constant\_pad \lsb m,l,r \rsb \rb}{p} \rb_{ij}
	\end{align*}
	We have shown that the Jacobian matrix is symmetric.
\end{proof}

\begin{theorem}
	The $1$-dimensional convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with $pad[m,\dots] = symmetric\_pad[m]$ are symplectic.
\end{theorem}
\begin{proof}
	Let $1 \leq i \leq d$.
	Then
	\begin{equation*}
		\lb conv1d \lsb k \rsb \circ symmetric\_pad \lsb m \rsb(p) \rb_i
		= \sum_{u=-m}^m k_u \lb symmetric\_pad \lsb m \rsb(p) \rb_{i+u+m} = (*)
	\end{equation*}
	With the variable transform $v=v(u):=i+u+m \iff u=v-i-m$ we have
	\begin{equation*}
		(*) = \sum_{v=i}^{i+2m} k_{v-i-m} \lb symmetric\_pad \lsb m \rsb(p) \rb_{v}
	\end{equation*}
	Let $I := \{ v \in \mathbb{N} : i \leq v \leq i+2m \}$ and
	\begin{align*}
		I_1 &:= I \cap \{ v \in \mathbb{N} : 1 \leq v \leq m \} \\
		I_2 &:= I \cap \{ v \in \mathbb{N} : m+1 \leq v \leq d+m \} \\
		I_3 &:= I \cap \{ v \in \mathbb{N} : d+m+1 \leq v \leq d+2m \}
	\end{align*}
	Then
	\begin{align*}
		(*) &= \sum_{v_1 \in I_1} k_{v_1-i-m} p_{m+1-v_1} +
		\sum_{v_2 \in I_2} k_{v_2-i-m} p_{v_2-m} +
		\sum_{v_3 \in I_3} k_{v_3-i-m} p_{2d+m+1-v_3}
	\end{align*}
	We apply a variable transform on each sum.
	\begin{align*}
		w_1 = w_1(v_1) := m-v_1+1 &\implies v_1=m+1-w_1 \\
		&\implies \widetilde{I_1} := w_1(I_1) \\
		& \phantomrel{\implies \widetilde{I_1} :}
		= \{ w \in \mathbb{N} : 1-m-i \leq w \leq 1+m-i \} \\
		& \phantomrel{\implies \widetilde{I_1} :=}
		\cap \{ w \in \mathbb{N} : 1 \leq w \leq m \}  \\
		& \phantomrel{\implies \widetilde{I_1} :}
		= \{ w \in \mathbb{N} : 1 \leq w \leq 1+m-i \} \\[20pt]
		%
		w_2 = w_2(v_2) := v_2-m &\implies v_2=w+m \\
		&\implies \widetilde{I_2} := w_2(I_2) \\
		&\phantomrel{\implies \widetilde{I_2} :}
		= \{ w \in \mathbb{N} : i-m \leq w \leq i+m \} \\
		& \phantomrel{\implies \widetilde{I_2} :=}
		\cap \{ w \in \mathbb{N} : 1 \leq w \leq d \}  \\
		&\phantomrel{\implies \widetilde{I_2} :}
		= \{ w \in \mathbb{N} : i-m \leq w \leq i+m \} \\[20pt]
		%
		w_3 = w_3(v_3) := 2d+m+1-v_3 &\implies v_3=2d+m+1-w \\
		&\implies \widetilde{I_3} := w_3(I_3) \\
		& \phantomrel{\implies \widetilde{I_3} :}
		= \{ w \in \mathbb{N} : 2d+1-m-i \leq w \leq 2d+m+1-i \} \\
		& \phantomrel{\implies \widetilde{I_3} :=}
		\cap \{ w \in \mathbb{N} : d+1-m \leq w \leq d \} \\
		& \phantomrel{\implies \widetilde{I_3} :}
		= \{ w \in \mathbb{N} : 2d+1-m-i \leq w \leq d \}
	\end{align*}
	Then
	\begin{align*}
		(*) = \sum_{w_1 \in \widetilde{I_1}} k_{1-i-w_1} p_{w_1} +
		\sum_{w_2 \in \widetilde{I_2}} k_{w_2-i} p_{w_2} +
		\sum_{w_3 \in \widetilde{I_3}} k_{2d+1-i-w_3} p_{w_3}
	\end{align*}
	Let $ 1 \leq j \leq d$. Then for the Jacobian holds
	\begin{align*}
		\hspace{6em}&\hspace{-6em}
		\lb conv1d \lsb k \rsb \circ symmetric\_pad \lsb m \rsb(p) \rb_{ij}
		= \deldel{p_j} \lb conv1d \lsb k \rsb \circ symmetric\_pad \lsb m \rsb(p) \rb_i \\
		&= \underbrace{\lb \begin{cases}
			k_{1-i-j} & : j \in \widetilde{I_1} \\
			0 & : else
		\end{cases} \rb}_{:= K_1(i,j)} + \underbrace{\lb \begin{cases}
			k_{j-i} & : j \in \widetilde{I_2} \\
			0 & : else
		\end{cases} \rb}_{:= K_2(i,j)} + \underbrace{\lb \begin{cases}
			k_{2d+1-i-j} & : j \in \widetilde{I_3} \\
			0 & : else
		\end{cases} \rb}_{:= K_3(i,j)} \\
		&= K_1(i,j) + K_2(i,j) + K_3(i,j)
	\end{align*}
	We have
	\begin{align*}
		 K_1(i,j) &= \begin{cases}
			k_{1-i-j} & : j \in \widetilde{I_1} \\
			0 & : else
		\end{cases} = \begin{cases}
			k_{1-i-j} & : 1 \leq j \leq 1+m-i \\
			0 & : else
		\end{cases} \\[7pt]
		&= \begin{cases}
			k_{1-i-j} & : j \leq 1+m-i \\
			0 & : else
		\end{cases} = \begin{cases}
			k_{1-i-j} & : j+i \leq 1+m \\
			0 & : else
		\end{cases} \\[25pt]
		%
		K_2(i,j) &= \begin{cases}
			k_{j-i} & : j \in \widetilde{I_2} \\
			0 & : else
		\end{cases} = \begin{cases}
			k_{j-i} & : i-m \leq j \leq i+m \\
			0 & : else
		\end{cases} = \begin{cases}
			k_{j-i} & : \abs{j-i} \leq m \\
			0 & : else
		\end{cases} \\[25pt]
		%
		K_3(i,j) &= \begin{cases}
			k_{2d+1-i-j} & : j \in \widetilde{I_3} \\
			0 & : else
		\end{cases} = \begin{cases}
			k_{2d+1-i-j} & : 2d+1-m-i \leq j \leq d \\
			0 & : else
		\end{cases} \\[7pt]
		&= \begin{cases}
			k_{2d+1-i-j} & : 2d+1-m-i \leq j \\
			0 & : else
		\end{cases}
		= \begin{cases}
			k_{2d+1-i-j} & : 2d+1-m \leq j+i \\
			0 & : else
		\end{cases}
	\end{align*}
	We show that $K_l$, $l=1,2,3$ are symmetric, i.e. $K_l(j,i) = K_l(i,j)$ for
	$l=1,2,3$. This implies that the Jacobian is symmetric.
	\begin{equation*}
		K_1(j,i) = \begin{cases}
			k_{1-j-i} & : i+j \leq 1+m \\
			0 & : else
		\end{cases} = \begin{cases}
			k_{1-i-j} & : j+i \leq 1+m \\
			0 & : else
		\end{cases} = K_1(i,j) \\[20pt]
	\end{equation*}
	\begin{equation*}
		K_2(j,i) = \begin{cases}
			k_{i-j} & : \abs{i-j} \leq m \\
			0 & : else
		\end{cases} \stackrel{k \text{ symmetric}}{=} \begin{cases}
			k_{j-i} & : \abs{j-i} \leq m \\
			0 & : else
		\end{cases} = K_2(i,j) \\[20pt]
	\end{equation*}
	\begin{equation*}
		K_3(j,i) = \begin{cases}
			k_{2d+1-j-i} & : 2d+1-m \leq i+j \\
			0 & : else
		\end{cases} = \begin{cases}
			k_{2d+1-i-j} & : 2d+1-m \leq j+i \\
			0 & : else
		\end{cases} = K_3(i,j)
	\end{equation*}
	Thus symplecticity follows with \cref{jacobi_symmetric}.
\end{proof}

\subsection{Activation layers}

\begin{alignat*}{2}
	\mathcal{N}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{N}_{up} \qpvec &:= \uppersympop{\hat{\activation}_a}
	:= \begin{pmatrix}
		q + diag(a)\activation(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{N}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{N}_{low} \qpvec &:= \lowersympop{\hat{\activation}_a}
	:= \begin{pmatrix}
		q \\
		diag(a)\activation(q) + p
	\end{pmatrix}
\end{alignat*}
where $a \in \mathbb{R}^d$ is a learnable parameter and 
$\activation : \mathbb{R} \to \mathbb{R}$ an activation function,
which is applied element-wise.

\begin{corollary}
	Given an activation function $\activation \in \mathbb{C}^1$
	the activation layers $\mathcal{N}_{up}$ and $\mathcal{N}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$ and $V(p) := a^T\mathcal{A}(p)$.

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{equation*}
		\grad{V}(p) = \lb\jac{V}{p}(p)\rb^T = \lb a^Tdiag\lb\sigma(p)\rb\rb^T
		= diag(\activation(p)) a
		= diag(a) \activation(p)
	\end{equation*}
	Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

\subsection{Gradient layers}

\begin{alignat*}{2}
	\mathcal{G}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}_{up} \qpvec &:= \uppersympop{\hat{\activation}_g} := \begin{pmatrix}
		q + K^T diag(a) \activation(Kp + b) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{G}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}_{low} \qpvec &:= \lowersympop{\hat{\activation}_g} := \begin{pmatrix}
		q \\
		K^T diag(a) \activation(Kq + b) + p
	\end{pmatrix}
\end{alignat*}
where $K \in \mathbb{R}^{n \times d}$ and $a,b \in \mathbb{R}^n$
are learnable parameters. $\activation : \mathbb{R} \to \mathbb{R}$ 
is an activation function, which is applied element-wise.
$n \in \mathbb{N}$ denotes the width of the gradient layer and can be chosen freely.

\begin{corollary}
	Given an activation function $\sigma \in C^1$ the gradient layers $\mathcal{G}_{up}$
	and $\mathcal{G}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$ and
	$V(p) := \onevec{n}^Tdiag(a)\mathcal{A}(Kp+b)$.

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{align*}
		\grad{V(p)} &= \left(\jac{V}{p}(p)\right)^T \\
		&= \left(1_n^Tdiag(a)diag\left(\sigma(Kp+b)\right)K\right)^T \\
		&= K^Tdiag\left(\sigma(Kp+b)\right)diag(a)1_n \\
		&= K^Tdiag(a)diag\left(\sigma(Kp+b)\right)1_n \\
		&= K^Tdiag(a)\sigma(Kp+b)
	\end{align*}

	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}_{up}$ and $\mathcal{G}_{low}$ are symplectic.
\end{proof}

Note that activation layers are a subset of gradient layers. We can see this by choosing
$n=d$, $K=I_d$ and $b=0$.

\todo{For high dimensions $K$ and $K^T$ can be implemented as transposed convolution
and convolution operators.}

\subsection{Normalization}

\todo{Introduce batch normalization from literature.}

Batch normalization is a well-known method to accelerate training initially proposed by
\citeauthor{batchnorm-ioffe15} in \cite{batchnorm-ioffe15}. 
We introduce a possibility to incorporate batch normalization
into activation and gradient layers while maintaining symplecticity.
\todo{Cite similar idea}

Sigmoid activation function saturates for large values. Vanishing gradient.
Therefore normalize input before applying activation function.

Let us define the batch normalization transformation.
\begin{equation*}
	BN_{\gamma, \beta} : \mathbb{R}^d \to \mathbb{R}^d,\quad
	BN_{\gamma, \beta}(p) 
	:= diag(\gamma)diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb (p-\mu_\mathcal{B}) + \beta
\end{equation*}
where $\gamma, \beta \in \mathbb{R}^{d}$ are learnable parameters.
$\epsilon \in \mathbb{R}$ is a small positive value to avoid division by zero.
$\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ refers to the mini-batch variance and
$\mu_\mathcal{B} \in \mathbb{R}^{d}$ refers to the mini-batch mean.

\todo{Move this to Notation}
The fraction has to be interpreted in broadcasting sense, i.e.
$\frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \in \mathbb{R}^d$ is a vector with entries
\begin{equation*}
	\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb_i
	:= \frac{1}{ \sqrt{ \lb \sigma^2_\mathcal{B} \rb_i + \epsilon}}
	\quad (1 \leq i \leq d)
\end{equation*}

Given a mini-batch $\mathcal{B}$ with input training data $p_1, p_2, \dots, p_n \in \mathbb{R}^{d}$ the
mean and variance are estimated by
\begin{align*}
	\mu_\mathcal{B} &= \frac{1}{n} \sum_{i=1}^{n} p_i \\
	\sigma^2_\mathcal{B} &= \frac{1}{n} \sum_{i=1}^n (p_i - \mu_\mathcal{B})^2
\end{align*}

During training the mini-batch variance $\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ and
the mini-batch $\mu_\mathcal{B} \in \mathbb{R}^{d}$ are continously updated.
When training has finished we switch to evaluation mode. In evaluation mode we keep
$\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ and $\mu_\mathcal{B} \in \mathbb{R}^{d}$ constant
and remember the last value from training.

The learnable parameters $\gamma, \beta \in \mathbb{R}^{d}$ allow the neural network to modify
the normalization during training if necessary. The batch normalization transform is able to 
represent the identity transform by setting appropriate $\gamma, \beta$.

\begin{algorithm}
	\caption{Batch normalization transform}
	\textbf{Input:} $p_j$ and mini batch $\mathcal{B} = \{p_1, p_2, \dots, p_n\}$  \\
	\textbf{Output:} $BN_{\gamma, \beta}(p_j)$
	\setstretch{1.5}
	\begin{algorithmic}
		\If{training\_mode} \Comment{Update mean and variane if training, otherwise
		keep previous values}
			\State $\mu_\mathcal{B} \gets \frac{1}{n} \sum_{i=1}^{n} p_i$
			\State $\sigma^2_\mathcal{B} \gets \frac{1}{n} \sum_{i=1}^n (p_i - \mu_\mathcal{B})^2$
		\EndIf
		\State \Return 
		$diag(\gamma)diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb 
		(p-\mu_\mathcal{B}) + \beta$
	\end{algorithmic}
\end{algorithm}

The batch normalization transform can be implemented in a straightforward way with modern
deep learning frameworks, as for backpropagation during training 
$\jac{BN_{\gamma, \beta}}{(\gamma, \beta)}$ is obtained via automatic differentiation.

If the training data is small enough so that splitting the data into multiple mini batches
is not necessary, the mean and variance are calculated for the whole training data. This is the
case for our numerical experiments, as we work with very small training data sets.
If the training data set is split into mini batches there exist also alternative methods to estimate
the mean and variance for the whole training data set, for example using moving averages.

\subsubsection{Normalized gradient layers}

\begin{alignat*}{2}
	\mathcal{G}^{BN}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}^{BN}_{up} \qpvec &:= \uppersympop{\hat{\activation}^{BN}_g} := \begin{pmatrix}
		q + \hat{\activation}^{BN}_g(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{G}^{BN}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}^{BN}_{low} \qpvec &:= \lowersympop{\hat{\activation}^{BN}_g} := \begin{pmatrix}
		q \\
		\hat{\activation}^{BN}_g(q) + p
	\end{pmatrix}
\end{alignat*}
with $\hat{\activation}^{BN}_g : \mathbb{R}^d \to \mathbb{R}^d$ defined as
\begin{equation*}
	\hat{\activation}^{BN}_g(p) := 
	K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
	diag(a) \activation(BN_{\gamma, \beta}(Kp+b))
\end{equation*}
where $K \in \mathbb{R}^{n \times d}$ and $a,b, \gamma, \beta \in \mathbb{R}^n$
are learnable parameters. $\activation : \mathbb{R} \to \mathbb{R}$ 
is an activation function, which is applied element-wise.
$n \in \mathbb{N}$ denotes the width of the gradient layer.
$\sigma^2_\mathcal{B}$ refers to the mini-batch variance of $Kp+b$ for $\mathcal{G}^{BN}_{up}$
or $Kq+b$ for $\mathcal{G}^{BN}_{low}$.

\todo{$diag(\gamma)$ and $\beta$ redundant, but numerical experiments better result
with $diag(\gamma)$ and ease of implementation. Double-check?
Having $diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb$ greatly improves
learning.}

\todo{Maybe add illustration for normalized gradient layer}

\begin{corollary}
	Given an activation function $\sigma \in C^1$
	the normalized gradient layers $\mathcal{G}^{BN}_{up}$ and $\mathcal{G}^{BN}_{down}$
	are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$ and
	\begin{equation*}
		V(p) := \onevec{n}^Tdiag(a)\mathcal{A}(BN_{\gamma, \beta}(Kp+b))
	\end{equation*}

	We have $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{equation*}
		\jac{BN_{\gamma, \beta}(p)}{p} = 
		diag(\gamma)diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb
	\end{equation*}

	Then
	\begin{align*}
		\grad{V(p)} &= \left(\jac{V}{p}(p)\right)^T \\
		&= \left( 1_n^T diag(a) diag \lb \activation(BN_{\gamma, \beta}(Kp+b))\rb
		\jac{BN_{\gamma, \beta}(p)}{p} K \right)^T \\
		&= \left( 1_n^T diag(a) diag \lb \activation(BN_{\gamma, \beta}(Kp+b))\rb
		diag(\gamma) diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb K \right)^T \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag\left(\activation(BN_{\gamma, \beta}(Kp+b))\right) diag(a) 1_n \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag(a) diag\left(\activation(BN_{\gamma, \beta}(Kp+b))\right) 1_n \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag(a) \activation(BN_{\gamma, \beta}(Kp+b)) = \hat{\activation}^{BN}_g(p)
	\end{align*}

	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}^{BN}_{up}$ and $\mathcal{G}^{BN}_{low}$ are symplectic.
\end{proof}

\subsubsection{Normalized activation layers}

\begin{alignat*}{2}
	\mathcal{N}^{BN}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{N}^{BN}_{up} \qpvec &:= \uppersympop{\hat{\activation}^{BN}_a} := \begin{pmatrix}
		q + \hat{\activation}^{BN}_a(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{N}^{BN}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{N}^{BN}_{low} \qpvec &:= \lowersympop{\hat{\activation}^{BN}_a} := \begin{pmatrix}
		q \\
		\hat{\activation}^{BN}_a(q) + p
	\end{pmatrix}
\end{alignat*}
with $\hat{\activation}^{BN}_a : \mathbb{R}^d \to \mathbb{R}^d$ defined as
\begin{equation*}
	\hat{\activation}^{BN}_a(p) := 
	diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
	diag(a) \activation(BN_{\gamma, \beta}(p))
\end{equation*}
where $a, \gamma, \beta \in \mathbb{R}^d$ are learnable parameters and 
$\activation : \mathbb{R} \to \mathbb{R}$ an activation function,
which is applied element-wise. $\sigma^2_\mathcal{B}$ refers to the mini-batch variance of $p$
for $\mathcal{N}^{BN}_{up}$ or $q$ for $\mathcal{N}^{BN}_{low}$.

\begin{corollary}
	Given an activation function $\sigma \in C^1$
	the normalized activation layers $\mathcal{N}^{BN}_{up}$ and $\mathcal{N}^{BN}_{low}$
	are symplectic.
\end{corollary}
\begin{proof}
	Symplecticity follows because an activation layer is a gradient layer
	(choose $n=d$, $K=I_d$ and $b=0$). We have already shown that a gradient layer is symplectic.
\end{proof}

\section{Relation to geometric integrators}

The symplectic Euler schemes are given by
\begin{equation*}
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H} (p_{n+1}, q_n) \\
			q_{n+1} &= q_n + h \grad[p]{H}(p_{n+1}, q_n)	
	\end{split}
	\quad\quad \text{or} \quad\quad
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H}(p_n, q_{n+1}) \\
			q_{n+1} &= q_n + h \grad[p]{H}{p}(p_n, q_{n+1})	
	\end{split}
\end{equation*}

If the Hamiltonian $H$ is separable, i.e. $H(q,p) = U(q) + T(p)$, both variants
become explicit. Then the left variant can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix} \begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix} 
\end{equation*}
and the right variant as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
where $\grad[p]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $p \mapsto \grad[p]{H}(p)$ 
and $\grad[q]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $q \mapsto \grad[q]{H}(q)$.

Similarly given a separable Hamiltonian $H(q,p) = U(q) + T(p)$ the $p$-staggered Störmer-Verlet scheme
\begin{align*}
	p_{n+1/2} &= p_n - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_n) \\[7pt]
	q_{n+1} &= q_n + \frac{h}{2} \lb \grad[p]{H}(p_{n+1/2}, q_n) + \grad[p]{H}(p_{n+1/2}, q_{n+1}) \rb \\[7pt]
	p_{n+1} &= p_{n+1/2} - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_{n+1})
\end{align*}
becomes explicit and can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & 0 \\
		-\frac{h}{2} \grad[q]{H}
	\end{bmatrix}
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-\frac{h}{2} \grad[q]{H}
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
and the $q$-staggered Störmer-Verlet scheme
\begin{align*}
	q_{n+1/2} &= q_n + \frac{h}{2} \grad[p]{H}(p_n, q_{n+1/2}) \\[7pt]
	p_{n+1} &= q_n - \frac{h}{2} \lb \grad[q]{H}(p_n, q_{n+1/2}) + \grad[q]{H}(p_{n+1}, q_{n+1/2}) \rb \\[7pt]
	q_{n+1} &= q_{n+1/2} + \frac{h}{2} \grad[p]{H}(p_{n+1}, q_{n+1/2})
\end{align*}
becomes explicit can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & \frac{h}{2} \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{bmatrix}
		I & \frac{h}{2} \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

Symplecticity for the explicit Euler and Störmer-Verlet schemes follows directly 
from \cref{gradient_corollary} and the fact that the composition of symplectic maps is again symplectic.

\todo{Add specific example like linear wave equation or Sine-Gordon and relate to neural network layers.}

\section{Numerical experiments}

\subsection{Low-dimensional systems}

\subsubsection{Harmonic Oscillator}

\subsubsection{Simple Pendulum}

\subsection{High-dimensional systems}

\todo{Describe Hamiltonian PDEs. A PDE describes system locally, thus it makes sense to share parameters, for
example in the form of CNNs etc.}

Assumptions:
\begin{itemize}
	\item Original PDE has constant coefficients \todo{Or rather should not depend on $x$ itself?}
	\item Equidistant grid points
\end{itemize}

\todo{Explain why we need assumptions.}

\subsubsection{Linear wave equation}

\todo{The choice of basis for the symmetric convolution kernels is critical (at least for fast convergence)!}

\subsubsection{Sine-Gordon}

\todo{Bias important if non-zero Dirichlet boundaries, but only at borders! // Padding modes}

\todo{Activation layer with sin works well if $a \in \mathbb{R}$, i.e. activation function is applied
isotropic.}

\todo{Maybe compare different optimizers (SGD, Adam, AdamW, amsgrad, ...)}

\todo{Activation layers: Differentiate between $a \in \mathbb{R}$ and $a \in \mathbb{R}^d$}

%----------------------------------------------------------------------------------------
%
% Resume
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\section{R\'esum\'e}
\subsection{Summary and conclusion}

\subsection{Outlook}

%----------------------------------------------------------------------------------------
%
% Appendices
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\section{First Appendix Section}

\newpage~\newpage
\section{Declaration of authorship}

\vspace{3cm}

\begin{table}[h!]
\centering
\begin{tabular}{|p{13cm}|}
\hline\\
	\todo{Change to bachelor thesis}
	I hereby certify
	\begin{enumerate}
		\item that this thesis has been composed by me and is based on my own work, unless stated otherwise,
		\item that all direct or indirect sources used are acknowledged as references and all extracts from work of others, either verbatim or in spirit, are stated as such,
		\item that neither the thesis itself nor parts of this thesis have been part of another examination procedure,
		\item that neither the thesis itself nor parts of this thesis have been published and
		\item that all copies of this thesis, either digital or printed, coincide.
	\end{enumerate}
	Therewith, this declaration of authorship is in accordance with the examination regulations from 29th July 2013 of the master's program \emph{Simulation Technology} of the University of Stuttgart.\\\\
\hline
\end{tabular}
\end{table}

\vspace{4cm}
\hrulefill\\
Name
\hspace{7cm}
Date, City, Signature
\end{appendices}
%----------------------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\addcontentsline{toc}{section}{References}
\bibliographystyle{abbrvnat}
\bibliography{../../literature/references.bib}

\end{document}
