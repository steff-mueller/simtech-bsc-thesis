\documentclass[twoside,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,bindingoffset=0.5cm,inner=2.5cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[a4paper,bindingoffset=1cm,inner=2cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry} %,showframe
\usepackage{helvet}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\sfdefault}
% \usepackage[german,ngerman]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amstext,amssymb,bm}
\usepackage{mathtools}
\usepackage{xcolor,color}
\usepackage{pifont}
\usepackage{array}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algpseudocode,algorithm}
%%% pgfplots for plotting
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
%%% Title page
\usepackage{common/titlePageST}
%%% Appendix in TOC
\usepackage[toc,page]{appendix}
%%% Setstrech on title page
\usepackage{setspace}
%%% Tilde in URL in literature
\usepackage{url}
%%% multiple rows
\usepackage{multirow}
%%% fancy column and row seperators
\usepackage{hhline}
%%% custom items in enumerate and itemize
\usepackage{enumitem}
%%% custom format for algorithm comments
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em // #1}

\clubpenalty=5000
\widowpenalty=5000

\setlength{\emergencystretch}{2cm}
%----------------------------------------------------------------------------------------
%	abbreviation includes
%----------------------------------------------------------------------------------------
\input{common/default_abbrev}
\input{common/specific_abbrev}
%%% Clever refing
\usepackage[capitalise,noabbrev]{cleveref}

%----------------------------------------------------------------------------------------
%	References with cleveref
%----------------------------------------------------------------------------------------
\crefformat{equation}{(#2#1#3)}

%----------------------------------------------------------------------------------------
%	Theorem environments
%----------------------------------------------------------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

%----------------------------------------------------------------------------------------
%	fancyhdr
%----------------------------------------------------------------------------------------
\usepackage{fancyhdr}

\makeatletter
\newcommand{\theauthor}{Steffen Müller} %
\makeatother

\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}
\fancyfoot[OL,ER]{University of Stuttgart} % inner
\fancyfoot[OR,EL]{\thepage} % outer
\fancyhead[OL,ER]{\theauthor} % inner 
\fancyhead[OR,EL]{IANS -- Institute of Applied Analysis and Numerical Simulation} % outer
\fancyfoot[C]{}

% \headsep=4mm
% \footskip=4mm
\parindent=0mm
\parskip=6pt
% \renewcommand{\footskip}{3pt}

%----------------------------------------------------------------------------------------
%	Something
%----------------------------------------------------------------------------------------
\usepackage{textpos}
\setlength{\TPHorizModule}{1mm}%
\setlength{\TPVertModule}{1mm}%
% \headsep=5mm
% \footskip=5mm
\pagestyle{fancy}
\newcommand{\articleheading}[3]{
{\large #1}\\[3mm]
{\Large\bf #2}\\[3mm]
{\large #3}
}

%----------------------------------------------------------------------------------------
%	Start Document
%----------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
%----------------------------------------------------------------------------------------
%
% TITLE PAGE
%
%----------------------------------------------------------------------------------------
%------------------------------------------
\begin{titlePageST}
%------------------------------------------
\makeLogo%
{-10pt}{
\includegraphics[width=0.7\textwidth]{figures/logos/simtech.pdf}
}%
{0pt}{
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/logos/ians.pdf}
	\end{center}}%
{0pt}{
\begin{flushright}
	\vspace{-10pt}
	\includegraphics[width=0.9\textwidth]{figures/logos/unistuttgart_logo_englisch_cmyk.eps}
\end{flushright}}%
\vspace{35pt}%
%------------------------------------------
\makeHeader%
[Research Group: Numerical Mathematics] %
{Institute of Applied Analysis and Numerical Simulation} %
\vspace{50pt}%
%------------------------------------------
\makeTitle%
{Simulation Technology Degree Course} %
{Bachelor Thesis} %
\vspace{80pt}%
%------------------------------------------
\makeTitleThesis%
{Symplectic Neural Networks}
\vspace{90pt}%
%------------------------------------------
\begin{supervisorST}{3}%
\addSuper%
{First Reviewer}%
{Prof. Dr. B. Haasdonk}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\addSuper%
{Second Reviewer}%
{Prof. Dr. D. Pflüger}%
{Institute of Parallel and Distributed\\[-0.2cm]
Systems (Scientific Computing)}%
\addSuper%
{Advisor}%
{Patrick Buchfink, M.Sc.}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\end{supervisorST}%
\vspace{80pt}%
%------------------------------------------
\begin{authorST}{Submitted by}%
\addAuthorInfo{Author}{Steffen Müller}
\addAuthorInfo{Student ID}{3260643}
\addAuthorInfo{SimTech ID}{119} %
\addAuthorInfo{Submission Date}{...} %FILLIN
\end{authorST}%
%------------------------------------------
\end{titlePageST}
%----------------------------------------------------------------------------------------
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ABSTRACT
%
%----------------------------------------------------------------------------------------
\section*{Abstract}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ACKNOWLEDGEMENTS
%
%----------------------------------------------------------------------------------------
\section*{Acknowledgements}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% TOC
%
%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\newpage\thispagestyle{plain}\null
%----------------------------------------------------------------------------------------
%
% Begin with document content
%
%----------------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic} 
%----------------------------------------------------------------------------------------
%
% Introduction
%
%----------------------------------------------------------------------------------------
\section{Introduction}

\subsection{Outline}

\subsection{Notation}

Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be an arbitrary function and $x \in \mathbb{R}^n$
a vector. When we write $f(x)$ we mean the element-wise application of $f$ 
on the vector $x \in \mathbb{R}^n$, i.e. $f(x) = \lb
	f(x_1), ..., f(x_n)
\rb^T$.

The $d$-by-$d$ identity matrix is denoted by $I_d$. If the dimension $d$ can be inferred
from context we may just write $I$.

$\onevec{n}$ denotes the $n$-dimensional $1$-vector.

\todo{Introduce block matrix notation for general functions}

We denote the $k$-th partial derivative of a function $f: \mathbb{R}^n \to \mathbb{R},
x \mapsto f(x)$ with $\deldel{x_k} f(x)$.

If $f: \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$ is a multi-dimensional function

\todo{"partial derivative" of a vector, Jacobian matrices}

\todo{Gradient}

\todo{Describe $d$}

\begin{equation*}
	J := \begin{pmatrix}
		0 & I_d \\
		-I_d & 0
	\end{pmatrix}
\end{equation*}

%----------------------------------------------------------------------------------------
%
% Content
%
%----------------------------------------------------------------------------------------
\newpage
\section{Problem setup}

\begin{definition}
	A matrix $A \in \mathbb{R}^{2d \times 2d}$ is called symplectic if $A^TJA=J$.
\end{definition}

\begin{definition}
	A differentiable map $\phi : U \to \mathbb{R}^{2d}$ (where $U \subset \mathbb{R}^{2d}$ is an open set)
	is called symplectic if the Jacobian matrix $\jac{\phi}{x}$ is everywhere symplectic, i.e.
	\begin{equation*}
		\lb \jac{\phi}{x} \rb^T J \lb \jac{\phi}{x} \rb = J
	\end{equation*}
\end{definition}

\begin{definition}
	A Hamiltonian (ODE) system can be written in canonical form as
	\begin{align*}
		\dot{y}(t) &= J \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
		y(t_0) &= y_0
	\end{align*}
	where $y: I \subset \mathbb{R} \to \mathbb{R}^{2d},\, t \mapsto y(t) = (q(t),p(t))$ and 
	$y_0 = (q_0, p_0) \in \mathbb{R}^{2d}$ the initial value for $t_0 \in \mathbb{R}$. 
	The function $H: \mathbb{R}^{2d} \to \mathbb{R}$ is called the Hamiltonian 
	or the total energy. $q = q(t) \in \mathbb{R}^d$ are called generalized coordinates
	and $p=p(t) \in \mathbb{R}^d$ are called conjugate momenta. 
	The phase space $\mathbb{R}^{2d}$ has even dimension for Hamiltonian systems.
\end{definition}

Note that the Hamiltonian $H$ is a first integral, i.e. the total energy is preserved, because
\begin{equation*}
	\ddt H(y(t)) = \lsb \grad{H}(y(t)) \rsb^T \dot{y}(t) = 
	\lsb \grad{H}(y(t)) \rsb^T J \grad{H}(y(t)) = 0
\end{equation*}
since $J$ skew-symmetric.

Let $\phi_{t,H} : U \subset \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ be the flow for a 
canonical Hamiltonian system with Hamiltonian $H$ for a fixed time step $t$, i.e.
\begin{equation*}
	\phi_{t,H}\begin{pmatrix}
		q_0 \\
		p_0
	\end{pmatrix}
	= \begin{pmatrix}
		q(t; q_0, p_0) \\
		p(t; q_0, p_0)
	\end{pmatrix}
\end{equation*}
where $q(t; q_0, p_0)$ and $p(t; q_0, p_0)$ denote the solution of the Hamiltonian system
at time $t$ for initial conditions $y_0 = (q_0,p_0) \in \mathbb{R}^{2d}$. 
Poincaré has shown that the flow of a Hamiltonian system is symplectic.

\begin{theorem}(Poincaré 1899)
	Let $H: \mathbb{R}^{2d} \to \mathbb{R}$ be a twice continuously differentiable
	function on $U \subset \mathbb{R}^{2d}$. Then, for each fixed $t$, the flow
	$\phi_{t,H}$ is a symplectic map wherever it is defined.
\end{theorem}
\begin{proof}
	We refer to \citet[Theorem 2.4, p.~184]{hairer2006} 
	or to \citet[Theorem 1, p.~54]{leimkuhler_reich_2005}.
\end{proof}

Note that a Hamiltonian (ODE) system in general can be written as
\begin{align*}
	\dot{y(t)} &= S \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
	y(t_0) &= y_0
\end{align*}
with an arbitrary nondegenerate skew-symmetric matrix $S \in \mathbb{R}^{2d \times 2d}$.
However there always exists a transformation to express such a Hamiltonian ODE system in
canonical form (see \citet[Remark 3.8]{peng2016}).

Our goal is to learn the flow map $\phi_{t,H} : U \to \mathbb{R}^{2d}$ for fixed $t$ with a neural network.
To be specific, for fixed $t$, we supply the neural network with training pairs $(y_i, \phi_{t,H}(y_i))$
($y_i \in \mathbb{R}^{2d}$ and $i=1, \dots, n_{\text{train}}$) and want the neural network
to be able to predict $\phi_{t,H}(y)$ for arbitrary $y \in \mathbb{R}^{2d}$.
This leads to the idea that we embed symplecticity into the neural network itself structurally.

Symplecticity has already been successfully embedded into numerical integrators, which
led to the development of geometric integrators.

\section{Architecture of SympNets}

In this section we recapitulate the architecture of SympNets as
proposed by \citeauthor{Jin2020} in \cite{Jin2020}. We proof
symplecticity for all layer types.

\subsection{General architecture}

We use that the composition of symplectic maps is again symplectic. So we impose that every
layer of the neural network must be symplectic, which leads to the overall neural network
to be symplectic.

All layers we introduce will have unit triangular structure in block matrix notation, 
i.e. can either be expressed in upper variant
\begin{equation*}
	L_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{L} \qpvec = \uppersympop{\hat{\activation}_L}
	= \begin{pmatrix}
		q + \hat{\activation}_L(p) \\
		p
	\end{pmatrix}
\end{equation*}
or lower variant
\begin{equation*}
	L_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{L} \qpvec = \lowersympop{\hat{\activation}_L}
	= \begin{pmatrix}
		q \\
		\hat{\activation}_L(q) + p
	\end{pmatrix}
\end{equation*}
where $\hat{\activation}_L : \mathbb{R}^d \to \mathbb{R}^d$ is a Layer-specific transformation.
In order that $L_{up}$ and $L_{low}$ are symplectic we have to demand additional requirements
to $\hat{\activation}_L$ as the following lemma and corollaries will show.

The structure was initially proposed by \citeauthor{Deco1995} in \cite{Deco1995} 
\todo{Have a deeper look at \cite{Deco1995}} for volume-conserving neural networks.

\todo{Relation to Residual Networks / ResNet}

\begin{lemma}\label{jacobi_symmetric}
	Let
	\begin{equation*}
		f_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{up} \qpvec := \uppersympop{g}
	\end{equation*}
	and
	\begin{equation*}
		f_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{low} \qpvec := \lowersympop{g}
	\end{equation*}
	where $g: \mathbb{R}^d \to \mathbb{R}^d,\; p \mapsto g(p)$ is a function in 
	$C^1(\mathbb{R}^d, \mathbb{R}^d)$. 

	$f_{up}$ and $f_{low}$ are symplectic if and only if the Jacobian matrix $\jac{g}{p}$
	is symmetric.
\end{lemma}
\begin{proof}
	We show the result for $f_{up}$ only. The proof is analogous for $f_{low}$.

	\begin{equation*}
		\deldel[f_{up}]{(q,p)} = \begin{pmatrix}
			I & \deldel[g]{p} \\
			0 & I
		\end{pmatrix}
	\end{equation*}
	It follows
	\begin{align*}
		\jacp{f_{up}}{(q,p)}^T J \jacp{f_{up}}{(q,p)} 
		&= \left(\deldel[f_{up}]{(q,p)}\right)^T \begin{pmatrix}
			0 & I \\
			-I & -\jac{g}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			I & 0 \\
			\jacp{g}{p}^T & Id
		\end{pmatrix} \begin{pmatrix}
			0 & I \\
			-I & -\jac{g}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			0 & I \\
			-I & \jacp{g}{p}^T-\jac{g}{p}
		\end{pmatrix}
	\end{align*}
	Thus $f_{up}$ is symplectic if and only if $\jacp{g}{p}^T-\jac{g}{p}=0$, 
	i.e. if and only if the Jacobian $\deldel[g]{p}$ is everywhere symmetric.
\end{proof}

\begin{corollary}\label{matrix_symmetric}
	Let
	\begin{equation*}
		f_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{up} \qpvec := \begin{pmatrix}
			I & S \\
			0 & I
		\end{pmatrix} \qpvec
	\end{equation*}
	and
	\begin{equation*}
		f_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{low} \qpvec := \begin{pmatrix}
			I & 0 \\
			S & I
		\end{pmatrix} \qpvec
	\end{equation*}
	where $S \in \mathbb{R}^{dxd}$. 

	$f_{up}$ and $f_{low}$ are symplectic if and only if the matrix $S$
	is symmetric.
\end{corollary}

The next corollary is useful to construct symplectic maps.
\begin{corollary}\label{gradient_corollary}
	Let $V: \mathbb{R}^d \to \mathbb{R}, \; p \mapsto V(p)$ be a function in 
	$\mathbb{C}^2$. 
	
	Then
	\begin{equation*}
		f_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{up} \qpvec := \uppersympop{\grad{V}}
	\end{equation*}
	and
	\begin{equation*}
		f_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d},\quad
		f_{low} \qpvec := \lowersympop{\grad{V}}
	\end{equation*}
	define a symplectic map with $\grad{V} : \mathbb{R}^d \to \mathbb{R}^d,\, p \mapsto \grad{V}(p)$. 
	We call $V$ a potential.
\end{corollary}
\begin{proof}
	The Jacobian matrix of $\grad{V}$ corresponds to the Hessian matrix of $V$,
	i.e. $\jac{(\grad{V})}{p} = HD$. \todo{Notation for Hessian}
	The Hessian is symmetric, thus the result follows with \cref{jacobi_symmetric}.
\end{proof}

\todo{Relation to 'Generating functions' in symplectic literature?}

\subsection{Linear layers}

\begin{alignat*}{2}
	\ell_{up}&: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad \ell_{up} \qpvec &:= \begin{pmatrix}
		I & S \\
		0 & I
	\end{pmatrix} \qpvec + b, \\[7pt]
	\ell_{low}&: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad \ell_{low} \qpvec &:= \begin{pmatrix}
		I & 0 \\
		S & I
	\end{pmatrix} \qpvec + b
\end{alignat*}
where $S \in \mathbb{R}^{d \times d}$ symmetric and $b \in \mathbb{R}^{2d}$
are learnable parameters. In practice, we parametrize the symmetric matrix $S\in \mathbb{R}^{d \times d}$
with $S = A^T + A$ via another arbitrary matrix $A\in \mathbb{R}^{d \times d}$, as
most optimization methods used for learning neural networks are designed for
unconstrained optimization problems.

It follows directly from \cref{matrix_symmetric} that the linear layers are
symplectic. Alternatively, we can apply \cref{gradient_corollary} by
choosing the potential $V(p) := p^TAp$.

\begin{proof}
	Let
	\begin{align*}
		\phi_1: \mathbb{R}^d \to \mathbb{R}^{2d}, \quad &\phi_1(p) := \begin{pmatrix}
			p \\
			Ap
		\end{pmatrix} \\
		\phi_2: \mathbb{R}^{2d} \to \mathbb{R}, \quad &\phi_2(p, \hat{p}) := 
		p^T\hat{p}
	\end{align*}
	
	Then $V(p) = \phi_2(\phi_1(p))$ and the corresponding Jacobian matrices are
	\begin{align*}
		\jac{\phi_1}{p}(p) &= \begin{pmatrix}
			I \\
			A
		\end{pmatrix} \\
		\jac{\phi_2}{(p,\hat{p})}(p, \hat{p}) &= \begin{pmatrix}
			\hat{p}^T && p^T
		\end{pmatrix}
	\end{align*}
	Thus
	\begin{align*}
		\jac{V}{p}(p) &= 
		\jac{\phi_2}{(p,\hat{p})} (\phi_1(p))
		\jac{\phi_1}{p}(p) \\
		&= \begin{pmatrix}
			(Ap)^T && p^T
		\end{pmatrix}
		\begin{pmatrix}
			I \\
			A
		\end{pmatrix} \\
		&= (Ap)^T + p^TA = (Ap + A^Tp)^T \\
		&= ((A+A^T)p)^T
	\end{align*}
	Therefore $\grad{V}(p) = \lb \deldel{p}V(p) \rb^T = (A+A^T)p = Sp$ with
	$S := A+A^T$ symmetric. Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

We may enhance expressivity of a linear layer by alternately composing multiple
$\ell_{up}$ and $\ell_{low}$. We define 
$\mathcal{L}^{n}_{up},\, \mathcal{L}^{n}_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ with

\begin{align*}
	\mathcal{L}^{n}_{up} \qpvec &:= \begin{pmatrix}
		I && 0 / S_n \\
		S_n / 0 && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && 0 \\
		S_2 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix} + b \\
	\mathcal{L}^{n}_{low} \qpvec &:= \begin{pmatrix}
		I && S_n / 0 \\
		0 / S_n && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && 0 \\
		S_1 && I
	\end{pmatrix} + b
\end{align*}

$\mathcal{L}^{n}_{up}$ and $\mathcal{L}^{n}_{low}$ are again symplectic because they 
are a composition of the symplectic maps $\ell_{up}$ and $\ell_{low}$.

\citeauthor{jin2020unit} show in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map. In other words, 
the set of all possible $\mathcal{L}^{9}_{up}$ is equal to the set of all symplectic linear maps.
\todo{check how \cite{jin2020unit} appears in bibliography}

It does not make sense to put two upper linear layers or two lower linear layers after each other,
because this reduces to a single upper or lower linear layer:

\begin{proof}
	We show the statement for two upper linear layers.
	\begin{align*}
		\ell_{up,2}\lb\ell_{up,1} \qpvec\rb &=
		\begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix}
		\lb
		\begin{pmatrix}
			I && S_1 \\
			0 && I
		\end{pmatrix}
		\qpvec + b_1
		\rb + b_2 \\
		&= \begin{pmatrix}
			I && S_1 + S_2 \\
			0 && I
		\end{pmatrix} \qpvec
		+ \begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix} b_1
		+ b_2 \\
		&= \begin{pmatrix}
			I && S \\
			0 && I
		\end{pmatrix} \qpvec + b
	\end{align*}
	with $S := S_1 + S_2$ and $b := \begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix} b_1
	+ b_2$.
\end{proof}

\subsection{Activation layers}

\begin{alignat*}{2}
	\mathcal{N}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{N}_{up} \qpvec &:= \uppersympop{\hat{\activation}_a}
	:= \begin{pmatrix}
		q + diag(a)\activation(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{N}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d},
	\quad \mathcal{N}_{low} \qpvec &:= \lowersympop{\hat{\activation}_a}
	:= \begin{pmatrix}
		q \\
		diag(a)\activation(q) + p
	\end{pmatrix}
\end{alignat*}
where $a \in \mathbb{R}^d$ is a learnable parameter and 
$\activation : \mathbb{R} \to \mathbb{R}$ an activation function,
which is applied element-wise.

\begin{corollary}
	Given an activation function $\activation \in \mathbb{C}^1$
	the activation layers $\mathcal{N}_{up}$ and $\mathcal{N}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$ and $V(p) := a^T\mathcal{A}(p)$.

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{equation*}
		\grad{V}(p) = \lb\jac{V}{p}(p)\rb^T = \lb a^Tdiag\lb\sigma(p)\rb\rb^T
		= diag(\activation(p)) a
		= diag(a) \activation(p)
	\end{equation*}
	Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

\subsection{Gradient layers}

\begin{alignat*}{2}
	\mathcal{G}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}_{up} \qpvec &:= \uppersympop{\hat{\activation}_g} := \begin{pmatrix}
		q + K^T diag(a) \activation(Kp + b) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{G}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}_{low} \qpvec &:= \lowersympop{\hat{\activation}_g} := \begin{pmatrix}
		q \\
		K^T diag(a) \activation(Kq + b) + p
	\end{pmatrix}
\end{alignat*}
where $K \in \mathbb{R}^{n \times d}$ and $a,b \in \mathbb{R}^n$
are learnable parameters. $\activation : \mathbb{R} \to \mathbb{R}$ 
is an activation function, which is applied element-wise.
$n \in \mathbb{N}$ denotes the width of the gradient layer and can be chosen freely.

\begin{corollary}
	Given an activation function $\sigma \in C^1$ the gradient layers $\mathcal{G}_{up}$
	and $\mathcal{G}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$ and
	$V(p) := \onevec{n}^Tdiag(a)\mathcal{A}(Kp+b)$.

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{align*}
		\grad{V(p)} &= \left(\jac{V}{p}(p)\right)^T \\
		&= \left(1_n^Tdiag(a)diag\left(\sigma(Kp+b)\right)K\right)^T \\
		&= K^Tdiag\left(\sigma(Kp+b)\right)diag(a)1_n \\
		&= K^Tdiag(a)diag\left(\sigma(Kp+b)\right)1_n \\
		&= K^Tdiag(a)\sigma(Kp+b)
	\end{align*}

	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}_{up}$ and $\mathcal{G}_{low}$ are symplectic.
\end{proof}

Note that activation layers are a subset of gradient layers. We can see this by choosing
$n=d$, $K=I_d$ and $b=0$.

\section{Convolution and Normalization for SympNets}

In this section we introduce extensions to SympNets. In particular, we bring the concept
of convolution to SympNets and we propose a possibility how to embed normalization while
maintaining symplecticity.

\subsection{Introduction to Convolution}

\todo{Motivate convolution layers}

We introduce convolution in a rather abstract way in order to simplify the following proofs.
General convolution is defined on infinite-dimensional function spaces.
Neural networks implement a finite-dimensional version, as the operation has to be
computable.

Given two discrete functions $f,g \in \mathbb{R}^\mathbb{Z}$ convolution is defined as 
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau - a)
\end{equation*}
However, must popular neural network libraries implement the so called cross-correlation.
Cross-correlation is a flipped convolution
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau + a)
\end{equation*}
For this reason, if we write $*$, we mean cross-correlation from now on. However, all following statements 
could also be proofed with convolution.
\todo{Write this in a better way... and cite Goodfellow?}

We work with finite-dimensional vectors in neural networks. Therefore, we define a bijective mapping between
finite-dimensionial vectors and $\mathbb{R}^{\mathbb{Z}}$ via zero continuation on $\mathbb{Z}$:
\begin{align*}
	\mathcal{I}_k &: \mathbb{R}^k \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb\mathcal{I}_k(x)\rb(\tau) := \sum_{a=1}^{k} x_a \delta_{\tau a} \\
	%
	\mathcal{I}_k^{-1} &: \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^k,
	\quad \mathcal{I}_k^{-1}(f) := \lb f(\tau) \rb_{\tau=1}^k
\end{align*}

Additionially, we define a shift operator $\mathcal{S}_v$, 
which shifts a discrete function $f \in \mathbb{R}^\mathbb{Z}$ by $v \in \mathbb{Z}$ positions
in the right direction
\begin{equation*}
	\mathcal{S}_v : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb \mathcal{S}_v(f) \rb (\tau) := f(\tau - v)
\end{equation*}

\begin{definition}
	(Valid cross-correlation)
	We define the valid cross-correlation of a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$ and 
	a finite-dimensional input $x \in \mathbb{R}^{n_x}$ with $n_x \geq n_k$ as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_x-n_k+1},
		\quad *_{\text{v}}(k,x) := \mathcal{I}_{n_x-n_k+1}^{-1} (
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}
In other words, the valid cross-correlation $*_{\text{v}}$ evaluates the cross-correlation 
only at positions where the finite-dimensional kernel $k$ and the 
finite-dimensional input $x$ fully overlap and puts the result into a finite-dimensional vector.

\todo{Add illustration}

We define a stride operator $\mathcal{D}_v$ for a stride number $v \in \mathbb{N}$ with
\begin{equation*}
	\mathcal{D}_v : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (\mathcal{D}_v)(\tau) := f(v \tau)
\end{equation*}
The stride operators skips $v$ entries of $f \in \mathbb{R}^\mathbb{Z}$.

\begin{definition}
	(Valid cross-correlation with stride)
	Given a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$, a finite-dimensional
	input $x \in \mathbb{R}^{n_x}$, stride number $v \in \mathbb{N}$ and output dimension
	\begin{equation*}
		n_{\text{out}} = \left\lfloor \frac{n_x - n_k}{v} \right\rfloor + 1
	\end{equation*}
	the valid-cross correlation with stride is defined as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_{\text{out}}},
		\quad *_{\text{v}}(k,x) := 
		(\mathcal{I}_{n_{\text{out}}}^{-1} \circ D_v)
		(
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}

\todo{Add illustration}

\subsection{Convolution layers}

The valid cross correlation (without stride) has the output dimension $n_x - n_k + 1$.
Thus, the valid cross-correlation $*_{\text{v}}$ decreases the dimension of the input $x$.
However, we want to incorporate the valid cross-correlation as the layer transform of a
unit triangular layer. The layer transform must keep the dimension.
Therefore we apply a padding operation on $x$ before passing it to the valid
cross-correlation. The padding operation increases the dimension of $x$ in order
that the composition of padding and cross-correlation keeps the overall dimension.
More specifically, let $d \in \mathbb{N}$ denote the input dimension of the layer transform.
Let $c \in \mathbb{N}$ denote the number by which the padding operator increases 
the input dimension. In order that the composition of padding and cross-correlation
keeps the dimension, it must hold
\begin{equation*}
	n_x - n_k + 1 = \underbrace{(d+c)}_{\substack{
		\text{increased dim.} \\
		\text{by } c \text{ via padding}
	}} - \, n_k + 1 = d \iff c = n_k-1
\end{equation*}
For symmetry reasons, we pad the same number of values at the beginning and 
the end of the input vector. This means that the number of padding values $c$ is even. 
Consequently, the equation above implies that the kernel size $n_k$ is odd, i.e. we can write
$n_k = 2m+1$ for a $m \in \mathbb{N}_0$, and $c=2m$. We define two different padding operators,
which both increase the input vector dimension by $2m$.

\begin{definition}
	Given the padding values $l,r \in \mathbb{R}^m$, we define constant padding 
	$c_{\text{pad},m}$ as
	\begin{equation*}
		c_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad c_{\text{pad},m}(p) = c_{\text{pad},m}(p;l,r) := \lb l^T, p^T, r^T \rb^T
	\end{equation*}
\end{definition}
The constant padding $c_{\text{pad},m}$ is an affine linear map, because we can write
\begin{equation}\label{cpad_affine}
	c_{\text{pad},m}(p;l,r) = c_{\text{pad}}(p;0_m,0_m)
	+ c_{\text{pad},m}(0_d;l,r)
\end{equation}
and $c_{\text{pad},m}(p;0_m,0_m)$ is linear regarding $p$.

\begin{definition}
	We define symmetric padding as
	\begin{equation*}
		s_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad s_{\text{pad},m}(p) := (
			\underbrace{p_m, p_{m-1} \dots, p_1}_{m \text{ values}}, \,
			\underbrace{p_1, p_2 \dots, p_d}_{d \text{ values}}, \,
			\underbrace{p_d, p_{d-1} \dots, p_{d-m+1}}_{m \text{ values}}
		)^T
	\end{equation*}
\end{definition}
The symmetric padding $s_{\text{pad},m}$ is a linear map.

Given an odd kernel $k \in \mathbb{R}^{2m+1}$ we set
\begin{equation*}
	\hat{k} = \hat{k}(k) := \mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}
Then $\hat{k}(-m) = k_1, \, \dots,\, \hat{k}(m) = k_{2m+1}$ and
$\hat{k}(\tau)=0$ for $\abs{\tau} > m$. With $\hat{k}$, we can express the valid cross-correlation as
\begin{equation*}
	*_{\text{v}}(k,x) = \mathcal{I}_{n_x-2m}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \mathcal{I}_{n_x}(x)
	)
\end{equation*}
because
\begin{equation*}
	\mathcal{S}_{m}(\hat{k}) 
	= \mathcal{S}_{m}(\mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k)))
	= \mathcal{S}_{m+(-m-1)}(\mathcal{I}_{2m+1}(k))
	= \mathcal{S}_{-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}

\begin{definition}
	We call an odd kernel $k \in \mathbb{R}^{2m+1}$ symmetric if
	\begin{equation*}
		\hat{k}(\tau) = \hat{k}(-\tau)
	\end{equation*}
	for all $\tau \in \mathbb{Z}$.
\end{definition}

Let us now define symplectic convolution layers by combining padding and valid 
cross-convolution.
\begin{definition}
	Given a symmetric kernel $k \in \mathbb{R}^{2m+1}$ and padding
	$(\bullet)_{\text{pad},m} = c_{\text{pad},m}$ or $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$,
	we call the upper and lower unit triangular layers with layer transform
	\begin{equation*}
		\activation_{\mathcal{C}}(p) := *_{\text{v}}(k,((\bullet)_{\text{pad}})(p))
	\end{equation*}
	the convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$.
\end{definition}

With $\hat{(\bullet)}_{\text{pad},m}(p) := \mathcal{I}_{d+2m}((\bullet)_{\text{pad},m}(p))$,
we can write the layer transform as
\begin{equation}\label{eq_conv_layer_transform}
	\activation_{\mathcal{C}}(p) =
	\mathcal{I}_{d}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \hat{(\bullet)}_{\text{pad}}(p)
	)
\end{equation}

\begin{lemma}\label{jac_linear_map}
	Let $f: \mathbb{R}^d \to \mathbb{R}^d$ be a linear map. Then the Jacobian matrix
	$\jac{f}{x}$ is constant, i.e.
	\begin{equation*}
		\jac{f}{x} \bigg|_{x = v} = \jac{f}{x} \bigg|_{x = w} \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
	and for the Jacobian-vector product holds
	\begin{equation*}
		\lb \jac{f}{x} \bigg|_{x = v} \rb w = f(w) \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
\end{lemma}
\begin{proof}
	$f$ is linear $\implies$ There exists a matrix representation $f(x) = Ax$ with
	$A \in \mathbb{R}^{d \times d}$ \\
	$\implies \jac{f}{x} \big|_{x=v} = A = \text{const.} \quad \forall v \in \mathbb{R}^d$
	$\implies \lb \jac{f}{x} \big|_{x=v} \rb w = Aw = f(w) \quad \forall v,w \in \mathbb{R}^d$
\end{proof}
As the Jacobian matrix for a linear map is constant, we omit the evaluation point, i.e.
$\jac{f}{x} \big|_{x=v} = \jac{f}{x}$.

\begin{theorem}\label{thm_conv_const_pad_symplectic}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $(\bullet)_{\text{pad},m} = c_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	We have to show that the Jacobian of the layer transform $\activation_{\mathcal{C}}(p)$
	is symmetric (\cref{jacobi_symmetric}).
	Because of \cref{cpad_affine}, it suffices to show the case $l,r=0$ (the Jacobian
	of a constant has only zero-valued entries). For $l,r=0$, the layer transform 
	$\activation_{\mathcal{C}}(p)$ 
	is a linear map, because it is a composition of linear maps only.

	Define the bilinear form
	\begin{equation}\label{eq_bilinear_proof_conv}
		b : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R},
		\quad b(v,w) := \ip{v}{\lb \jac{\activation_{\mathcal{C}}}{p} \rb w}
	\end{equation}
	To show symmetry of the Jacobian, we show that 
	$b(e_i, e_j) = b(e_j, e_i)$ for all $i,j=1,\dots,d$.
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{\lb \jac{\activation_{\mathcal{C}}}{p} \rb e_j}
		= \ip{e_i}{\activation_{\mathcal{C}}(e_j)} \quad \text{(\cref{jac_linear_map})} \\
		&\stackrel{\cref{eq_conv_layer_transform}}{=} \ip{e_i}{\mathcal{I}_{d}^{-1} (
			\mathcal{S}_{m}(\hat{k}) * \hat{c}_{\text{pad}}(e_j)
		)} \\
		&= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				\underbrace{(\hat{c}_{\text{pad}}(e_j))(\tau+a)}_{
					= \delta_{(\tau+a) (j+m)}
				}
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((j-i+m)-m) = \hat{k}(j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\begin{theorem}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	With $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$, the layer transform 
	$\activation_{\mathcal{C}}$ is linear, because it is a composition of linear maps. Thus,
	we proof the statement the same way as \cref{thm_conv_const_pad_symplectic} above.
	For $\tau,a \in \mathbb{Z}$ we have
	\begin{equation*}
		\hat{s}_{\text{pad},m}(e_j)(\tau + a) = 
		\delta_{(\tau + a) (m-j+1)} + \delta_{(\tau + a) (j+m)} + \delta_{(\tau + a) (2d+m-j+1)}
	\end{equation*}
	Consequently, for $(\bullet)_{\text{pad},m} = s_{\text{pad},m}$ and $i,j=1, \dots, d$, 
	the bilinear form \cref{eq_bilinear_proof_conv} becomes
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				(\hat{s}_{\text{pad}}(e_j))(\tau+a)
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((m-j+1-i)-m) + \hat{k}((j+m-i)-m) + \hat{k}((2d+m-j+1-i)-m) \\
		&= \hat{k}(1-j-i) + \hat{k}(j-i) + \hat{k}(1+2d-j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\subsubsection*{Parametrization of a symmetric kernel}

Let ${b_1, b_2, \dots b_m} \in \mathbb{R}^{2m+1}$ be a basis of the space 
$\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.
The symmetric kernel $k$ is then parametrized by the coefficients $\beta \in \mathbb{R}^m$ via
\begin{equation*}
	k = (b_1, b_2, \dots, b_m) \beta = \beta_1 b_1 + \beta_2 b_2 + \dots + \beta_m b_m
\end{equation*}

The basis vectors $b_1, \dots, b_m \in \mathbb{R}^{2m+1}$ for the canonical basis are given by
\begin{equation*}
	(b_i)_{j+m+1} = \hat{k}(b_i)(j) = \begin{cases}
		1 &: \abs{j} = i-1 \\
		0 &:else
	\end{cases} 
	\quad (j=-m, \dots, m)
\end{equation*}

Another possible basis choice inspired by finite differences is
\todo{Cite paper with similar idea?}
\begin{align*}
	b_1^{FD} = (0, \dots, 0,& 1,0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_2^{FD} = (0, \dots, 0, 1,-&2,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_3^{FD} = (0, 0, \dots, 0, 1,-4,& 6,-4,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1} \\
	&\vdots
\end{align*}
The entries for a basis vector $b_i^{FD} \in \mathbb{R}^{2m+1}$ $(1 \leq i \leq m)$ 
originate from Pascal's triangle.
\begin{equation*}
	\lb b_i^{FD} \rb_{j+m+1} = \hat{k}(b_i^{FD})(j) = \begin{dcases}
		(-1)^j \binom{2(i-1)}{j+i-1} &: \abs{j} < i \\
		0 &: else
	\end{dcases}
	\quad (j=-m, \dots, m)
\end{equation*}

\todo{Move this to Appendix?}
The symmetry of the kernel basis vectors $b_i^{FD}$ follows with the definition of the binomial coefficient.
\begin{align*}
	\binom{2(i-1)}{j+i-1} &= \frac{2(i-1)!}{(j+i-1)!(2(i-1)-(j+i-1))!} \\
	&= \frac{2(i-1)!}{(j+i-1)!(i-j-1)!}
\end{align*}
Thus we have
\begin{equation*}
	\binom{2(i-1)}{(-j)+i-1} = \binom{2(i-1)}{j+i-1}
\end{equation*}
and for $i=1, \dots, m$ and $j= 0, \dots, m$
\begin{align*}
	\hat{k}(b_i^{FD})(-j) &= \begin{dcases}
		(-1)^{-j} \binom{2(i-1)}{(-j)+i-1} : \abs{-j} < i \\
		0 : else
	\end{dcases} \\
	&= \begin{dcases}
		(-1)^{j} \binom{2(i-1)}{j+i-1} : \abs{j} < i \\
		0 : else
	\end{dcases} \\
	&= \hat{k}(b_i^{FD})(j) 
\end{align*}
So $b_i^{FD}$ $(i=1, \dots, m)$ form a valid basis of
the space $\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.

It turns out in our numerical experiments that
parametrization plays an important role how well a neural network learns.

\subsection{Convolution Gradient Layers}

As we have seen valid cross-correlation is a linear operator. Therefore
there exists a representative matrix $A_{*_{\text{v}}}$ with $*_{\text{v}}(p) = A_{*_{\text{v}}}p$.
Transposed valid cross-correlation 
is defined as the linear map associated with the transpose $A_{*_{\text{v}}}^T$ of $A_{*_{\text{v}}}$.

Again, most popular neural network libraries say convolution, but actually implement
valid cross-correlation. In this case transposed convolution actually refers to
transposed valid cross-correlation.

For large $d \in \mathbb{N}$, it can make sense to use convolution inside Gradient layers 
instead of a full matrix $W$.

\begin{equation*}
	\mathcal{G}_{up} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}_{up} \qpvec := \uppersympop{\hat{\activation}_g} := \begin{pmatrix}
		q + K^T diag(a) \activation(Kp + b) \\
		p
	\end{pmatrix}
\end{equation*}

In particular, a Gradient layer with width $n \in \mathbb{N}$ can be implemented by 
using valid cross-correlation (with stride) for $K^T \in \mathbb{R}^{d \times n}$ and the
corresponding transposed valid cross-correlation (with stride) for $K \in \mathbb{R}^{n \times d}$.
The choice is intentional, because we want to upscale the input $p \in \mathbb{R}^d$ ($n >> d$). 
Cross-correlation without padding decreases the dimension. Therefore we first apply 
transposed cross-correlation on $p$, because the transpose increases the dimension.
A large kernel size $s_k$ and a large stride value $v$ allow for significant upscaling.
We emphasize that the kernel $k$ has not to be symmetric in this setting.
\todo{Cite similar idea}

\subsection{Normalization}

\todo{Introduce batch normalization from literature.}

Batch normalization is a well-known method to accelerate training initially proposed by
\citeauthor{batchnorm-ioffe15} in \cite{batchnorm-ioffe15}. 
We introduce a possibility to incorporate batch normalization
into activation and gradient layers while maintaining symplecticity.
\todo{Cite similar idea}

Sigmoid activation function saturates for large values. Vanishing gradient.
Therefore normalize input before applying activation function.

Let us define the batch normalization transformation.
\begin{equation*}
	BN_{\gamma, \beta} : \mathbb{R}^d \to \mathbb{R}^d,\quad
	BN_{\gamma, \beta}(p) 
	:= diag(\gamma)diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb (p-\mu_\mathcal{B}) + \beta
\end{equation*}
where $\gamma, \beta \in \mathbb{R}^{d}$ are learnable parameters.
$\epsilon \in \mathbb{R}$ is a small positive value to avoid division by zero.
$\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ refers to the mini-batch variance and
$\mu_\mathcal{B} \in \mathbb{R}^{d}$ refers to the mini-batch mean.

\todo{Move this to Notation}
The fraction has to be interpreted in broadcasting sense, i.e.
$\frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \in \mathbb{R}^d$ is a vector with entries
\begin{equation*}
	\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb_i
	:= \frac{1}{ \sqrt{ \lb \sigma^2_\mathcal{B} \rb_i + \epsilon}}
	\quad (1 \leq i \leq d)
\end{equation*}

Given a mini-batch $\mathcal{B}$ with input training data $p_1, p_2, \dots, p_n \in \mathbb{R}^{d}$ the
mean and variance are estimated by
\begin{align*}
	\mu_\mathcal{B} &= \frac{1}{n} \sum_{i=1}^{n} p_i \\
	\sigma^2_\mathcal{B} &= \frac{1}{n} \sum_{i=1}^n (p_i - \mu_\mathcal{B})^2
\end{align*}

During training the mini-batch variance $\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ and
the mini-batch $\mu_\mathcal{B} \in \mathbb{R}^{d}$ are continously updated.
When training has finished we switch to evaluation mode. In evaluation mode we keep
$\sigma^2_\mathcal{B} \in \mathbb{R}^{d}$ and $\mu_\mathcal{B} \in \mathbb{R}^{d}$ constant
and remember the last value from training.

The learnable parameters $\gamma, \beta \in \mathbb{R}^{d}$ allow the neural network to modify
the normalization during training if necessary. The batch normalization transform is able to 
represent the identity transform by setting appropriate $\gamma, \beta$.

\begin{algorithm}
	\caption{Batch normalization transform}
	\textbf{Input:} $p_j$ and mini batch $\mathcal{B} = \{p_1, p_2, \dots, p_n\}$  \\
	\textbf{Output:} $BN_{\gamma, \beta}(p_j)$
	\setstretch{1.5}
	\begin{algorithmic}
		\If{training\_mode} \Comment{Update mean and variane if training, otherwise
		keep previous values}
			\State $\mu_\mathcal{B} \gets \frac{1}{n} \sum_{i=1}^{n} p_i$
			\State $\sigma^2_\mathcal{B} \gets \frac{1}{n} \sum_{i=1}^n (p_i - \mu_\mathcal{B})^2$
		\EndIf
		\State \Return 
		$diag(\gamma)diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb 
		(p-\mu_\mathcal{B}) + \beta$
	\end{algorithmic}
\end{algorithm}

The batch normalization transform can be implemented in a straightforward way with modern
deep learning frameworks, as for backpropagation during training 
$\jac{BN_{\gamma, \beta}}{(\gamma, \beta)}$ is obtained via automatic differentiation.

If the training data is small enough so that splitting the data into multiple mini batches
is not necessary, the mean and variance are calculated for the whole training data set. This is the
case for our numerical experiments, as we work with very small training data sets.
If the training data set is split into mini batches, there exist also alternative methods to estimate
the mean and variance for the whole training data set, for example using moving averages.

\todo{Add special note for convolution, respect convolution when normalizing.}

\subsubsection{Normalized gradient layers}

\begin{alignat*}{2}
	\mathcal{G}^{BN}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}^{BN}_{up} \qpvec &:= \uppersympop{\hat{\activation}^{BN}_g} := \begin{pmatrix}
		q + \hat{\activation}^{BN}_g(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{G}^{BN}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{G}^{BN}_{low} \qpvec &:= \lowersympop{\hat{\activation}^{BN}_g} := \begin{pmatrix}
		q \\
		\hat{\activation}^{BN}_g(q) + p
	\end{pmatrix}
\end{alignat*}
with $\hat{\activation}^{BN}_g : \mathbb{R}^d \to \mathbb{R}^d$ defined as
\begin{equation*}
	\hat{\activation}^{BN}_g(p) := 
	K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
	diag(a) \activation(BN_{\gamma, \beta}(Kp+b))
\end{equation*}
where $K \in \mathbb{R}^{n \times d}$ and $a,b, \gamma, \beta \in \mathbb{R}^n$
are learnable parameters. $\activation : \mathbb{R} \to \mathbb{R}$ 
is an activation function, which is applied element-wise.
$n \in \mathbb{N}$ denotes the width of the gradient layer.
$\sigma^2_\mathcal{B}$ refers to the mini-batch variance of $Kp+b$ for $\mathcal{G}^{BN}_{up}$
or $Kq+b$ for $\mathcal{G}^{BN}_{low}$.

\todo{$diag(\gamma)$ and $\beta$ redundant, but numerical experiments better result
with $diag(\gamma)$ and ease of implementation. Double-check?
Having $diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb$ greatly improves
learning.}

\todo{Maybe add illustration for normalized gradient layer}

\begin{corollary}
	Given an activation function $\sigma \in C^1$
	the normalized gradient layers $\mathcal{G}^{BN}_{up}$ and $\mathcal{G}^{BN}_{down}$
	are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$ and
	\begin{equation*}
		V(p) := \onevec{n}^Tdiag(a)\mathcal{A}(BN_{\gamma, \beta}(Kp+b))
	\end{equation*}

	We have $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and
	\begin{equation*}
		\jac{BN_{\gamma, \beta}(p)}{p} = 
		diag(\gamma)diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb
	\end{equation*}

	Then
	\begin{align*}
		\grad{V(p)} &= \left(\jac{V}{p}(p)\right)^T \\
		&= \left( 1_n^T diag(a) diag \lb \activation(BN_{\gamma, \beta}(Kp+b))\rb
		\jac{BN_{\gamma, \beta}(p)}{p} K \right)^T \\
		&= \left( 1_n^T diag(a) diag \lb \activation(BN_{\gamma, \beta}(Kp+b))\rb
		diag(\gamma) diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb K \right)^T \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag\left(\activation(BN_{\gamma, \beta}(Kp+b))\right) diag(a) 1_n \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag(a) diag\left(\activation(BN_{\gamma, \beta}(Kp+b))\right) 1_n \\
		&= K^T diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
		diag(a) \activation(BN_{\gamma, \beta}(Kp+b)) = \hat{\activation}^{BN}_g(p)
	\end{align*}

	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}^{BN}_{up}$ and $\mathcal{G}^{BN}_{low}$ are symplectic.
\end{proof}

\subsubsection{Normalized activation layers}

\begin{alignat*}{2}
	\mathcal{N}^{BN}_{up} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{N}^{BN}_{up} \qpvec &:= \uppersympop{\hat{\activation}^{BN}_a} := \begin{pmatrix}
		q + \hat{\activation}^{BN}_a(p) \\
		p
	\end{pmatrix} \\[7pt]
	%
	\mathcal{N}^{BN}_{low} &: \mathbb{R}^{2d} \to \mathbb{R}^{2d}, \quad
	\mathcal{N}^{BN}_{low} \qpvec &:= \lowersympop{\hat{\activation}^{BN}_a} := \begin{pmatrix}
		q \\
		\hat{\activation}^{BN}_a(q) + p
	\end{pmatrix}
\end{alignat*}
with $\hat{\activation}^{BN}_a : \mathbb{R}^d \to \mathbb{R}^d$ defined as
\begin{equation*}
	\hat{\activation}^{BN}_a(p) := 
	diag\lb \frac{1}{\sqrt{\sigma^2_\mathcal{B} + \epsilon}} \rb diag(\gamma)
	diag(a) \activation(BN_{\gamma, \beta}(p))
\end{equation*}
where $a, \gamma, \beta \in \mathbb{R}^d$ are learnable parameters and 
$\activation : \mathbb{R} \to \mathbb{R}$ an activation function,
which is applied element-wise. $\sigma^2_\mathcal{B}$ refers to the mini-batch variance of $p$
for $\mathcal{N}^{BN}_{up}$ or $q$ for $\mathcal{N}^{BN}_{low}$.

\begin{corollary}
	Given an activation function $\sigma \in C^1$
	the normalized activation layers $\mathcal{N}^{BN}_{up}$ and $\mathcal{N}^{BN}_{low}$
	are symplectic.
\end{corollary}
\begin{proof}
	Symplecticity follows because an activation layer is a gradient layer
	(choose $n=d$, $K=I_d$ and $b=0$). We have already shown that a gradient layer is symplectic.
\end{proof}

\section{Relation to geometric integrators}

The symplectic Euler schemes are given by
\begin{equation*}
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H} (p_{n+1}, q_n) \\
			q_{n+1} &= q_n + h \grad[p]{H}(p_{n+1}, q_n)	
	\end{split}
	\quad\quad \text{or} \quad\quad
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H}(p_n, q_{n+1}) \\
			q_{n+1} &= q_n + h \grad[p]{H}{p}(p_n, q_{n+1})	
	\end{split}
\end{equation*}

If the Hamiltonian $H$ is separable, i.e. $H(q,p) = U(q) + T(p)$, both variants
become explicit. Then the left variant can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix} \begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix} 
\end{equation*}
and the right variant as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
where $\grad[p]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $p \mapsto \grad[p]{H}(p)$ 
and $\grad[q]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $q \mapsto \grad[q]{H}(q)$.

Similarly given a separable Hamiltonian $H(q,p) = U(q) + T(p)$ the $p$-staggered Störmer-Verlet scheme
\begin{align*}
	p_{n+1/2} &= p_n - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_n) \\[7pt]
	q_{n+1} &= q_n + \frac{h}{2} \lb \grad[p]{H}(p_{n+1/2}, q_n) + \grad[p]{H}(p_{n+1/2}, q_{n+1}) \rb \\[7pt]
	p_{n+1} &= p_{n+1/2} - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_{n+1})
\end{align*}
becomes explicit and can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & 0 \\
		-\frac{h}{2} \grad[q]{H}
	\end{bmatrix}
	\begin{bmatrix}
		I & h \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-\frac{h}{2} \grad[q]{H}
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
and the $q$-staggered Störmer-Verlet scheme
\begin{align*}
	q_{n+1/2} &= q_n + \frac{h}{2} \grad[p]{H}(p_n, q_{n+1/2}) \\[7pt]
	p_{n+1} &= q_n - \frac{h}{2} \lb \grad[q]{H}(p_n, q_{n+1/2}) + \grad[q]{H}(p_{n+1}, q_{n+1/2}) \rb \\[7pt]
	q_{n+1} &= q_{n+1/2} + \frac{h}{2} \grad[p]{H}(p_{n+1}, q_{n+1/2})
\end{align*}
becomes explicit can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		I & \frac{h}{2} \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{bmatrix}
		I & 0 \\
		-h \grad[q]{H} & I
	\end{bmatrix}
	\begin{bmatrix}
		I & \frac{h}{2} \grad[p]{H} \\
		0 & I
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

Symplecticity for the explicit Euler and Störmer-Verlet schemes follows directly 
from \cref{gradient_corollary} and the fact that the composition of symplectic maps is again symplectic.

\todo{Add specific example like linear wave equation or Sine-Gordon and relate to neural network layers.}

\section{Numerical experiments}

\subsection{Low-dimensional systems}

\subsubsection{Harmonic Oscillator}

\subsubsection{Simple Pendulum}

\begin{tikzpicture}
	\begin{axis}[
		title={Phase plot},
		xlabel={q},
		ylabel={p},
	]

	\addplot[
		color=red,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact_phase_plot.csv};

	\addplot[
		color=blue,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/la-sympnet_phase_plot.csv};

	\addplot[
		color=green,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet_phase_plot.csv};

	\addplot[
		color=blue,
		dashed,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/normalized-la-sympnet_phase_plot.csv};

	\addplot[
		color=green,
		dashed,
		no markers
	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/normalized-g-sympnet_phase_plot.csv};
		
	\end{axis}
\end{tikzpicture}

\subsection{High-dimensional systems}

\todo{Describe Hamiltonian PDEs. A PDE describes system locally, thus it makes sense to share parameters, for
example in the form of CNNs etc.}

Assumptions:
\begin{itemize}
	\item Original PDE has constant coefficients \todo{Or rather should not depend on $x$ itself?}
	\item Equidistant grid points
\end{itemize}

\todo{Explain why we need assumptions.}

\subsubsection{Linear wave equation}

\todo{The choice of basis for the symmetric convolution kernels is critical (at least for fast convergence)!}

\subsubsection{Sine-Gordon}

\todo{Bias important if non-zero Dirichlet boundaries, but only at borders! // Padding modes}

\todo{Activation layer with sin works well if $a \in \mathbb{R}$, i.e. activation function is applied
isotropic.}

\todo{Maybe compare different optimizers (SGD, Adam, AdamW, amsgrad, ...)}

\todo{Activation layers: Differentiate between $a \in \mathbb{R}$ and $a \in \mathbb{R}^d$}

%----------------------------------------------------------------------------------------
%
% Resume
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\section{R\'esum\'e}
\subsection{Summary and conclusion}

\subsection{Outlook}

%----------------------------------------------------------------------------------------
%
% Appendices
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\section{First Appendix Section}

\newpage~\newpage
\section{Declaration of authorship}

\vspace{3cm}

\begin{table}[h!]
\centering
\begin{tabular}{|p{13cm}|}
\hline\\
	\todo{Change to bachelor thesis}
	I hereby certify
	\begin{enumerate}
		\item that this thesis has been composed by me and is based on my own work, unless stated otherwise,
		\item that all direct or indirect sources used are acknowledged as references and all extracts from work of others, either verbatim or in spirit, are stated as such,
		\item that neither the thesis itself nor parts of this thesis have been part of another examination procedure,
		\item that neither the thesis itself nor parts of this thesis have been published and
		\item that all copies of this thesis, either digital or printed, coincide.
	\end{enumerate}
	Therewith, this declaration of authorship is in accordance with the examination regulations from 29th July 2013 of the master's program \emph{Simulation Technology} of the University of Stuttgart.\\\\
\hline
\end{tabular}
\end{table}

\vspace{4cm}
\hrulefill\\
Name
\hspace{7cm}
Date, City, Signature
\end{appendices}
%----------------------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\addcontentsline{toc}{section}{References}
\bibliographystyle{abbrvnat}
\bibliography{../../literature/references.bib}

\end{document}
