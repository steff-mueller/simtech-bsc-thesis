\documentclass[twoside,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,bindingoffset=0.5cm,inner=2.5cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[a4paper,bindingoffset=1cm,inner=2cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry} %,showframe
\usepackage{helvet}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\sfdefault}
% \usepackage[german,ngerman]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amstext,amssymb,bm}
\usepackage{mathtools}
\usepackage{xcolor,color}
\usepackage{pifont}
\usepackage{array}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algpseudocode,algorithm}
%%% Title page
\usepackage{common/titlePageST}
%%% Appendix in TOC
\usepackage[toc,page]{appendix}
%%% Setstrech on title page
\usepackage{setspace}
%%% Tilde in URL in literature
\usepackage{url}
%%% multiple rows
\usepackage{multirow}
%%% fancy column and row seperators
\usepackage{hhline}
%%% custom items in enumerate and itemize
\usepackage{enumitem}
%%% custom format for algorithm comments
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em // #1}

\clubpenalty=5000
\widowpenalty=5000

\setlength{\emergencystretch}{2cm}
%----------------------------------------------------------------------------------------
%	abbreviation includes
%----------------------------------------------------------------------------------------
\input{common/default_abbrev}
\input{common/specific_abbrev}
%%% Clever refing
\usepackage[capitalise,noabbrev]{cleveref}

%----------------------------------------------------------------------------------------
%	References with cleveref
%----------------------------------------------------------------------------------------
\crefformat{equation}{(#2#1#3)}

%----------------------------------------------------------------------------------------
%	Theorem environments
%----------------------------------------------------------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

%----------------------------------------------------------------------------------------
%	fancyhdr
%----------------------------------------------------------------------------------------
\usepackage{fancyhdr}

%----------------------------------------------------------------------------------------
%	pgfplots
%----------------------------------------------------------------------------------------
\usepackage{pgfplots,pgfplotstable}
\pgfplotsset{compat=1.16}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{external}
\tikzexternalize
\tikzsetexternalprefix{figures_cache/}

\pgfplotsset{every axis/.append style={
    ymajorgrids,
    grid style={dashed,lightgray,semithick},
   }
}

% Style to select only points from #1 to #2 (inclusive)
\pgfplotsset{select coords between index/.style 2 args={
    x filter/.code={
        \ifnum\coordindex<#1\def\pgfmathresult{}\fi
        \ifnum\coordindex>#2\def\pgfmathresult{}\fi
    }
}}
%----------------------------------------------------------------------------------------

\makeatletter
\newcommand{\theauthor}{Steffen Müller} %
\makeatother

\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}
\fancyfoot[OL,ER]{University of Stuttgart} % inner
\fancyfoot[OR,EL]{\thepage} % outer
\fancyhead[OL,ER]{\theauthor} % inner 
\fancyhead[OR,EL]{IANS -- Institute of Applied Analysis and Numerical Simulation} % outer
\fancyfoot[C]{}

% \headsep=4mm
% \footskip=4mm
\parindent=0mm
\parskip=6pt
% \renewcommand{\footskip}{3pt}

%----------------------------------------------------------------------------------------
%	Something
%----------------------------------------------------------------------------------------
\usepackage{textpos}
\setlength{\TPHorizModule}{1mm}%
\setlength{\TPVertModule}{1mm}%
% \headsep=5mm
% \footskip=5mm
\pagestyle{fancy}
\newcommand{\articleheading}[3]{
{\large #1}\\[3mm]
{\Large\bf #2}\\[3mm]
{\large #3}
}

%----------------------------------------------------------------------------------------
%	Start Document
%----------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
%----------------------------------------------------------------------------------------
%
% TITLE PAGE
%
%----------------------------------------------------------------------------------------
%------------------------------------------
\begin{titlePageST}
%------------------------------------------
\makeLogo%
{-10pt}{
\includegraphics[width=0.7\textwidth]{figures/logos/simtech.pdf}
}%
{0pt}{
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/logos/ians.pdf}
	\end{center}}%
{0pt}{
\begin{flushright}
	\vspace{-10pt}
	\includegraphics[width=0.9\textwidth]{figures/logos/unistuttgart_logo_englisch_cmyk.eps}
\end{flushright}}%
\vspace{35pt}%
%------------------------------------------
\makeHeader%
[Research Group: Numerical Mathematics] %
{Institute of Applied Analysis and Numerical Simulation} %
\vspace{50pt}%
%------------------------------------------
\makeTitle%
{Simulation Technology Degree Course} %
{Bachelor Thesis} %
\vspace{80pt}%
%------------------------------------------
\makeTitleThesis%
{Symplectic Neural Networks}
\vspace{90pt}%
%------------------------------------------
\begin{supervisorST}{3}%
\addSuper%
{First Reviewer}%
{Prof. Dr. B. Haasdonk}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\addSuper%
{Second Reviewer}%
{Prof. Dr. D. Pflüger}%
{Institute of Parallel and Distributed\\[-0.2cm]
Systems (Scientific Computing)}%
\addSuper%
{Advisor}%
{Patrick Buchfink, M.Sc.}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\end{supervisorST}%
\vspace{80pt}%
%------------------------------------------
\begin{authorST}{Submitted by}%
\addAuthorInfo{Author}{Steffen Müller}
\addAuthorInfo{Student ID}{3260643}
\addAuthorInfo{SimTech ID}{119} %
\addAuthorInfo{Submission Date}{...} %FILLIN
\end{authorST}%
%------------------------------------------
\end{titlePageST}
%----------------------------------------------------------------------------------------
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ABSTRACT
%
%----------------------------------------------------------------------------------------
\section*{Abstract}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ACKNOWLEDGEMENTS
%
%----------------------------------------------------------------------------------------
\section*{Acknowledgements}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% TOC
%
%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\newpage\thispagestyle{plain}\null
%----------------------------------------------------------------------------------------
%
% Begin with document content
%
%----------------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic} 
%----------------------------------------------------------------------------------------
%
% Introduction
%
%----------------------------------------------------------------------------------------
\section{Introduction}

\subsection{Outline}

\subsection{Notation}

The $d$-by-$d$ identity matrix is denoted by $I_d$. If the dimension $d$ can be inferred
from context we may just write $I$.

$\onevec{n} \in \R^n$ denotes the $n$-dimensional $1$-vector.

We denote the $k$-th partial derivative of a function $f: \mathbb{R}^n \to \mathbb{R},
x \mapsto f(x)$ with $\deldel{x_k} f(x)$.

If $f: \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$ is a multi-dimensional function

\todo{"partial derivative" of a vector, Jacobian matrices}

\todo{Gradient}

\todo{Describe special meaning of dimension $d$}

\begin{equation*}
	J := \begin{pmatrix}
		0 & I_d \\
		-I_d & 0
	\end{pmatrix}
\end{equation*}

vector $v \in \R^n$
\begin{equation*}
	v = \begin{pmatrix}
		v_i
	\end{pmatrix}_{i=1}^n
\end{equation*}

We denote the $i$-th entry of a vector $v \in \R^n$ with $v_i$ or $(v)_i$.

For $q,p \in \R^{d}$ and maps $f_{11}, f_{12}, f_{21}, f_{22} : \R^d \to \R^d$
we define the block operator as
\begin{equation*}
	\begin{bmatrix}
		f_{11} & f_{12} \\
		f_{21} & f_{22}
	\end{bmatrix}
	\qpvec
	:= \begin{pmatrix}
		f_{11}(q) + f_{12}(p) \\
		f_{21}(q) + f_{22}(p)
	\end{pmatrix} \in \R^{2d}
\end{equation*}

identity map $id$

$\R^{\mathbb{Z}} := \{ f : \mathbb{Z} \to \mathbb{R} \}$

$\N$ without zero, $\N_0$ with zero.

Kronecker delta
\begin{equation*}
	\delta_{ij} := \begin{cases}
		1 &: i = j \\
		0 &: \text{else}
	\end{cases}
\end{equation*}


%----------------------------------------------------------------------------------------
%
% Content
%
%----------------------------------------------------------------------------------------
\newpage
\section{Problem setup}

\begin{definition}
	A matrix $A \in \mathbb{R}^{2d \times 2d}$ is called symplectic if $A^TJA=J$.
\end{definition}

\begin{definition}
	A differentiable map $\phi : U \to \mathbb{R}^{2d}$ (where $U \subset \mathbb{R}^{2d}$ is an open set)
	is called symplectic if the Jacobian matrix $\jac{\phi}{x}$ is everywhere symplectic, i.e.
	\begin{equation*}
		\lb \jac{\phi}{x} \rb^T J \lb \jac{\phi}{x} \rb = J
	\end{equation*}
\end{definition}

\begin{definition}
	A Hamiltonian (ODE) system can be written in canonical form as
	\begin{align*}
		\dot{y}(t) &= J \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
		y(t_0) &= y_0
	\end{align*}
	where $y: I \subset \mathbb{R} \to \mathbb{R}^{2d},\, t \mapsto y(t) = (q(t),p(t))$ and 
	$y_0 = (q_0, p_0) \in \mathbb{R}^{2d}$ the initial value for $t_0 \in \mathbb{R}$. 
	The function $H: \mathbb{R}^{2d} \to \mathbb{R}$ is called the Hamiltonian 
	or the total energy. $q = q(t) \in \mathbb{R}^d$ are called generalized coordinates
	and $p=p(t) \in \mathbb{R}^d$ are called conjugate momenta. 
	The phase space $\mathbb{R}^{2d}$ has even dimension for Hamiltonian systems.
\end{definition}

Note that the Hamiltonian $H$ is a first integral, i.e. the total energy is preserved, because
\begin{equation*}
	\ddt H(y(t)) = \lsb \grad{H}(y(t)) \rsb^T \dot{y}(t) = 
	\lsb \grad{H}(y(t)) \rsb^T J \grad{H}(y(t)) = 0
\end{equation*}
since $J$ skew-symmetric.

Let $\phi_{t,H} : U \subset \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ be the flow for a 
canonical Hamiltonian system with Hamiltonian $H$ for a fixed time step $t$, i.e.
\begin{equation*}
	\phi_{t,H}\begin{pmatrix}
		q_0 \\
		p_0
	\end{pmatrix}
	= \begin{pmatrix}
		q(t; q_0, p_0) \\
		p(t; q_0, p_0)
	\end{pmatrix}
\end{equation*}
where $q(t; q_0, p_0)$ and $p(t; q_0, p_0)$ denote the solution of the Hamiltonian system
at time $t$ for initial conditions $y_0 = (q_0,p_0) \in \mathbb{R}^{2d}$. 
Poincaré has shown that the flow of a Hamiltonian system is symplectic.

\begin{theorem}(Poincaré 1899)
	Let $H: \mathbb{R}^{2d} \to \mathbb{R}$ be a twice continuously differentiable
	function on $U \subset \mathbb{R}^{2d}$. Then, for each fixed $t$, the flow
	$\phi_{t,H}$ is a symplectic map wherever it is defined.
\end{theorem}
\begin{proof}
	We refer to \citet[Theorem 2.4, p.~184]{hairer2006} 
	or to \citet[Theorem 1, p.~54]{leimkuhler_reich_2005}.
\end{proof}

Note that a Hamiltonian (ODE) system in general can be written as
\begin{align*}
	\dot{y}(t) &= S \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
	y(t_0) &= y_0
\end{align*}
with an arbitrary nondegenerate skew-symmetric matrix $S \in \mathbb{R}^{2d \times 2d}$.
However there always exists a transformation to express such a Hamiltonian ODE system in
canonical form (see \citet[Remark 3.8]{peng2016}). Thus, we restrict w.l.o.g.
to canonical systems.

Our goal is to learn the flow map $\phi_{t,H} : U \to \mathbb{R}^{2d}$ for fixed $t$ with a neural network.
To be specific, for fixed $t$, we supply the neural network with training pairs $(y_i, \phi_{t,H}(y_i))$
($y_i \in \mathbb{R}^{2d}$ and $i=1, \dots, n_{\text{train}}$) and want the neural network
to be able to predict $\phi_{t,H}(y)$ for arbitrary $y \in \mathbb{R}^{2d}$.
This leads to the idea that we embed symplecticity into the neural network itself structurally.

Symplecticity has already been successfully embedded into numerical integrators for ODEs, which
led to the development of geometric integrators, see for example \citeauthor{hairer2006}
(\cite{hairer2006}).

\section{Architecture of SympNets}

In this section we recapitulate the architecture of SympNets (Symplectic Networks) as
proposed by \citeauthor{Jin2020} in \cite{Jin2020}. We proof
symplecticity for all layer types.

Let us briefly introduce neural network terminology we will use.

\todo{Finish this introduction of neural networks terminology}

\begin{definition}
	(Layer)
	A neural network layer with input dimension $n_1$ and output dimension $n_2$
	and parameters $\theta \in \R^{n_\theta}$ is a map 
	$\phi : \R^{n_1} \to \R^{n_2},\, x \mapsto \mathcal{L}(x;\theta) = \mathcal{L}(x)$.
\end{definition}

A common layer is the so called fully-connected layer
$\phi(x) = Wx +b$ with parameters $W \in \R^{n_2 \times n_1},\, b \in \R^{n_2}$,
i.e. $\theta = (vec(W)^T, b^T)^T \in \R^{n_2 n_1 + n_2}$.

In our special case we will always have $n_1 = n_2 = 2d$.

\begin{definition}
	(Neural Network)
	A neural network $\Phi$ is a composition of one or multiple layers with
	compatible input and ouput dimensions. 
	\begin{equation*}
		\Phi(x;\Theta) = \phi_{n_L} \lb \phi_{n_L-1} \lb \cdots
		\lb \phi_2(
		\phi_1(x;\theta_1); \theta_2 ) \cdots \rb ; \theta_{n_L-1} \rb ; \theta_{n_L} \rb
	\end{equation*}
	We denote the vector of all
	layer parameters with $\Theta = (\theta_1^T, \dots, \theta_{n_L}^T)^T$.
\end{definition}

For a given loss function $L$, the neural network tries to minimize the loss 
function $L$ by learning locally optimal parameters $\Theta$.

\todo{Training data}
$\mathcal{T} = \{(x_1, y_1) \dots, (x_{n_{\text{train}}},y_{n_{\text{train}}}) \}
\subset \R^{n_x} \times \R^{n_y}$

\todo{$x_i$ refers to input, $y_i$ refers to the expected output (supervised learning)}

\todo{Mini batch}
$\mathcal{B} = \{(x_{i_1}, y_{i_1}), \dots, (x_{i_2},y_{i_2}) \} \subset \mathcal{T}$

\begin{equation*}
	L(\mathcal{B}; \Theta) = \sum_{(x_i,y_i) \in \mathcal{B}} l(\Phi(x_i; \Theta), y_i)
\end{equation*}

\todo{$l$ can be some norm}
\begin{equation*}
	l(x,y) = \norm{x-y}
\end{equation*}

\todo{Gradient descent}
\begin{equation*}
	\grad[\Theta]{L(\mathcal{B};\Theta)}
\end{equation*}

\todo{Forward pass, backward pass, automatic differentiation. 
Apply chain rule on computational graph to compute gradient.}

\todo{Add illustration}

\todo{Training}

\subsection{General architecture}

We use that the composition of symplectic maps is again symplectic. So we impose that every
layer of the neural network must be symplectic, which leads to the overall neural network
to be symplectic.

\begin{definition}
	(Unit Triangular Layer) A layer $\phi : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ 
	is called a unit triangular layer with layer transform 
	$\layertf : \mathbb{R}^d \to \mathbb{R}^d,\, p \mapsto \layertf(p)$
	and bias parameter $b \in \R^{2d}$, if $\phi$ can be expressed as
	\begin{equation*}
		\phi_\up \qpvec = \uppersympop{\layertf} + b
		= \begin{pmatrix}
			q + \layertf(p) \\
			p
		\end{pmatrix} + b \quad \text{(upper unit triangular layer)}
	\end{equation*}
	or
	\begin{equation*}
		\phi_\low \qpvec = \lowersympop{\layertf} + b
		= \begin{pmatrix}
			q \\
			\layertf(q) + p
		\end{pmatrix} + b \quad \text{(lower unit triangular layer)}
	\end{equation*}
	If we do not explicitly specify the bias parameter $b$, we assume
	that $b$ is a non-learnable constant with value $b=0$.
\end{definition}

This structure was initially proposed by \citeauthor{Deco1995} in \cite{Deco1995} 
\todo{Have a deeper look at \cite{Deco1995}} for volume-conserving neural networks.
\todo{Relation to Residual Networks / ResNet}

A SympNet only consists of unit triangular layers. In order that the unit triangular layers 
$\mathcal{L}_{\text{up}}$ or $\mathcal{L}_{\text{low}}$ are symplectic,
we have to put additional requirements on the layer transform $\layertf$.

\begin{lemma}\label{jacobi_symmetric}
	An upper or lower unit triangular layer $\phi_{\text{up}}$ or $\phi_{\text{low}}$
	is symplectic if and only if the Jacobian of the layer transform
	$\layertf \in C(\mathbb{R}^d, \R^d)$ 
	is everywhere symmetric.
\end{lemma}
\begin{proof}
	We show the result for $\phi_{\text{up}}$ only. 
	The proof is analogous for $\phi_{\text{low}}$.

	\begin{equation*}
		\jac{\phi_{\text{up}}}{(q,p)} = \begin{pmatrix}
			I & \jac{\layertf}{p} \\
			0 & I
		\end{pmatrix}
	\end{equation*}
	It follows
	\begin{align*}
		\lb \jac{\phi_{\text{up}}}{(q,p)} \rb^T J \lb \jac{\phi_{\text{up}}}{(q,p)} \rb
		&= \lb \jac{\phi_{\text{up}}}{(q,p)} \rb^T \begin{pmatrix}
			0 & I \\
			-I & -\jac{\layertf}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			I & 0 \\
			\lb \jac{\layertf}{p} \rb^T & Id
		\end{pmatrix} \begin{pmatrix}
			0 & I \\
			-I & -\jac{\layertf}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			0 & I \\
			-I & \lb \jac{\layertf}{p} \rb ^T-\jac{\layertf}{p}
		\end{pmatrix} \overset{!}{=} J
	\end{align*}
	Thus, $\phi_{\text{up}}$ is symplectic if and only if
	$\lb \jac{\layertf}{p} \rb ^T-\jac{\layertf}{p}=0$, 
	i.e. if and only if the Jacobian $\jac{\layertf}{p}$ is everywhere symmetric.
\end{proof}

The next corollary is useful to construct symplectic unit triangular layers.
\begin{corollary}\label{gradient_corollary}
	Let $V: \mathbb{R}^d \to \mathbb{R}, \; p \mapsto V(p)$ be a function in 
	$C^2(\R^d)$. Then the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = \grad{V}(p)
	\end{equation*}
	are symplectic. We call $V$ a potential.
\end{corollary}
\begin{proof}
	The Jacobian matrix of $\grad{V}$ corresponds to the Hessian matrix of $V$,
	i.e. $\jac{(\grad{V})}{p} = HV$. \todo{Notation for Hessian}
	The Hessian is everywhere symmetric due to $V \in C^2(\R^d)$ (Lemma of Schwarz),
	thus the result follows with \cref{jacobi_symmetric}.
\end{proof}

\subsection{Linear layers}

\begin{definition}
	(Linear layers)
	Given a symmetric matrix $S \in \R^{d \times d}$ and bias $b \in \R^{2d}$,
	we call the upper and lower unit triangular layers
	with layer transform $\layertf(p) = Sp$ the (symplectic) linear layers $\ell_\up$ and $\ell_\low$.
\end{definition}

The linear layers can be expressed with matrix-vector multiplication, for example
\begin{equation*}
	\ell_\up \qpvec = \begin{pmatrix}
		I & S \\
		0 & I
	\end{pmatrix} \qpvec + b
\end{equation*}

The matrix $S$ and the bias $b$ are learnable parameters.
In practice, we parametrize the symmetric matrix $S\in \mathbb{R}^{d \times d}$
with $S = A^T + A$ via another arbitrary matrix $A\in \mathbb{R}^{d \times d}$, as
most optimization methods used for learning neural networks are designed for
unconstrained optimization problems.

The linear layers are symplectic, because the Jacobian of the layer transform $\jac{\layertf}{p} = S$
is a symmetric matrix by defintion. Alternatively, we can apply \cref{gradient_corollary} by
choosing the potential $V(p) := p^TAp$.

\begin{proof}
	For $k=1, \dots, d$ we have
	\begin{align*}
		\lb \grad{V}(p) \rb_k &= \deldel{p_k} \lb \sum_{i,j=1}^d A_{ij} p_i p_j \rb
		= \sum_{i,j=1}^d A_{ij} \deldel{p_k}(p_i) p_j + \sum_{i,j=1}^d A_{ij} p_i \deldel{p_k}(p_j) \\
		&= \sum_{j=1}^d A_{kj} p_j + \sum_{i=1}^d A_{ik} p_i = ((A+A^T)p)_k \\
	\end{align*}
	Choose $A=\frac{1}{2}S \implies A+A^T=S$.
	Thus, symplecticity follows with \cref{gradient_corollary}.
\end{proof}

We may enhance expressivity of a linear layer by alternately composing multiple
$\ell_{up}$ and $\ell_{low}$. We define 
$\mathcal{L}^{n}_{up},\, \mathcal{L}^{n}_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ with

\begin{align*}
	\mathcal{L}^{n}_{up} \qpvec &:= \begin{pmatrix}
		I && 0 / S_n \\
		S_n / 0 && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && 0 \\
		S_2 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix} + b \\
	\mathcal{L}^{n}_{low} \qpvec &:= \begin{pmatrix}
		I && S_n / 0 \\
		0 / S_n && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && 0 \\
		S_1 && I
	\end{pmatrix} + b
\end{align*}

$\mathcal{L}^{n}_{up}$ and $\mathcal{L}^{n}_{low}$ are again symplectic because they 
are a composition of the symplectic maps $\ell_{up}$ and $\ell_{low}$.

\citeauthor{jin2020unit} show in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map. In other words, 
the set of all possible $\mathcal{L}^{9}_{up}$ is equal to the set of all symplectic linear maps.

It does not make sense to put two upper linear layers or two lower linear layers after each other,
because this reduces to a single upper or lower linear layer:

\begin{proof}
	We show the statement for two upper linear layers $\ell_{up,2}$ and $\ell_{up,1}$.
	\begin{align*}
		\lb \ell_{up,2} \circ \ell_{up,1} \rb \qpvec &=
		\begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix}
		\lb
		\begin{pmatrix}
			I && S_1 \\
			0 && I
		\end{pmatrix}
		\qpvec + b_1
		\rb + b_2 \\
		&= \begin{pmatrix}
			I && S_1 + S_2 \\
			0 && I
		\end{pmatrix} \qpvec
		+ \begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix} b_1
		+ b_2 \\
		&= \begin{pmatrix}
			I && S \\
			0 && I
		\end{pmatrix} \qpvec + b
	\end{align*}
	with $S := S_1 + S_2$ and $b := \begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix} b_1
	+ b_2$.
\end{proof}

\subsection{Activation layers}

\begin{definition}
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \activation(p_i) \rb_{i=1}^d
	\end{equation*}
	the upper and lower activation layers $\mathcal{N}_\up$ and $\mathcal{N}_\low$.
\end{definition}
The coefficients $a \in \mathbb{R}^d$ are learnable parameters.

\begin{corollary}
	Given an activation function $\activation \in \mathbb{C}^1(\R)$
	the activation layers $\mathcal{N}_{up}$ and $\mathcal{N}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be an antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{k=1}^d a_k \mathcal{A}(p_k)
	\end{equation*}

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $k=1, \dots d$
	\begin{equation*}
		\lb \grad{V}(p) \rb_i = \deldel{p_i} \lb \sum_{k=1}^d a_k \mathcal{A}(p_k) \rb
		= a_i \activation(p_i) = \lb \layertf(p) \rb_i
	\end{equation*}
	Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

\todo{Define LA-SympNet}

\subsection{Gradient layers}

\begin{definition}
	(Gradient layers)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \activation \lb (Kp)_j + c_j \rb \bigg)_{j=1}^n
	\end{equation*}
	the upper and lower gradient layers $\mathcal{G}_\up$ and $\mathcal{G}_\low$.
\end{definition}
$K \in \mathbb{R}^{n \times d}$ and $a,c \in \mathbb{R}^n$
are learnable parameters. 
$n \in \mathbb{N}$ denotes the width of the gradient layer and can be chosen freely.
In practice, we choose $n >> d$ for large expressivity.

\begin{corollary}
	Given an activation function $\sigma \in C^1$ the gradient layers $\mathcal{G}_{up}$
	and $\mathcal{G}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + c_j)
	\end{equation*}
	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $i=1, \dots, d$
	\begin{align*}
		(\grad{V(p)})_i &= \deldel{p_i} \lb \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + c_j) \rb
		= \sum_{j=1}^n a_j \activation((Kp)_j + c_j) 
		\underbrace{\deldel{p_i} \lb (Kp)_j \rb}_{=K_{ji} = K^T_{ij}}
		= (\layertf(p))_i
	\end{align*}
	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}_{up}$ and $\mathcal{G}_{low}$ are symplectic.
\end{proof}

Note that activation layers are a subset of gradient layers. We can see this by choosing
$n=d$, $K=I_d$ and $c=0$.

\todo{Define G-SympNet}

\subsection{Approximation theorems}

\todo{Briefly mention theoretical results from \citeauthor{Jin2020} (r-finite etc.)}

\citeauthor{Jin2020} have shown \citeauthor{Jin2020} approximation theorems.

\begin{definition}
	Let $r \in \N_0$ be given. $\sigma$ is r-finite if $\sigma \in C^r(\R)$
	and $0 < \int \abs{\sigma} d\lambda < \infty$. $\lambda$ is the Lebesgue measure on $\R$.
\end{definition}

\begin{definition}
	Let $m,n \in \N, \, r \in \N_0$ be given, $U \subset \R^m$ is an open set.
\end{definition}

\section{Convolution and Normalization for SympNets}

In this section we introduce new extensions to SympNets. In particular, we bring the concept
of convolution to SympNets and we propose a possibility how to embed normalization into SympNets while
maintaining symplecticity.

\subsection{Introduction to Convolution}

\todo{Motivate convolution layers (weight sharing, sparse-connectivity)}

We introduce convolution in a rather abstract way in order to simplify the following proofs.
General convolution is defined on infinite-dimensional function spaces.
Neural networks implement a finite-dimensional version, as the operation has to be
computable.

Given two discrete functions $f,g \in \mathbb{R}^\mathbb{Z}$ convolution is defined as 
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau - a)
\end{equation*}
However, most popular neural network libraries actually do not implement real convolution
in convolution layers. Instead, they implement the so called cross-correlation, but call it convolution. 
Cross-correlation is a flipped convolution and is defined as
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau + a)
\end{equation*}
Because most popular neural network libraries implement cross correlation and call it convolution,
we stick to the same convention and mean cross-correlation if we write $*$.
Still, the choice is arbitrary, as the following statements could be constructed analogously with convolution.
\todo{cite Goodfellow}

We work with finite-dimensional vectors in neural networks. Therefore, we define a bijective mapping between
vectors in $\mathbb{R}^n$ and functions in $\mathbb{R}^{\mathbb{Z}}$ 
via zero-continuation on $\mathbb{Z}$:
\begin{align*}
	\mathcal{I}_n &: \mathbb{R}^n \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb\mathcal{I}_n(x)\rb(\tau) := \sum_{a=1}^{n} x_a \delta_{\tau a} \\
	%
	\mathcal{I}_n^{-1} &: \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^n,
	\quad \mathcal{I}_n^{-1}(f) := \lb f(\tau) \rb_{\tau=1}^n
\end{align*}

Additionially, we define a shift operator $\mathcal{S}_\zeta$, 
which shifts a discrete function $f \in \mathbb{R}^\mathbb{Z}$ by $\zeta \in \mathbb{Z}$ positions
in the right direction
\begin{equation*}
	\mathcal{S}_\zeta : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb \mathcal{S}_\zeta(f) \rb (\tau) := f(\tau - \zeta)
\end{equation*}

\begin{definition}\label{def_cross_corr}
	(Valid cross-correlation)
	We define the valid cross-correlation of a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$ and 
	a finite-dimensional input $x \in \mathbb{R}^{n_x}$ with $n_x \geq n_k$ as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_x-n_k+1},
		\quad *_{\text{v}}(k,x) := \mathcal{I}_{n_x-n_k+1}^{-1} (
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}
In other words, the valid cross-correlation $*_{\text{v}}$ evaluates the cross-correlation 
only at positions where the finite-dimensional kernel $k$ and the 
finite-dimensional input $x$ fully overlap and puts the result into a finite-dimensional vector.

\todo{Add illustration}

The kernel $k$ will be a parameter which the neural network should learn.
Note that in this context it does not make a difference if we use convolution or cross-correlation, because
the neural network would just learn a flipped version of the kernel $k$ in the opposite case.

We define a stride operator $\mathcal{D}_s$ for a stride number $s \in \mathbb{N}$ with
\begin{equation*}
	\mathcal{D}_s : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (\mathcal{D}_s)(\tau) := f(s \tau)
\end{equation*}
The stride operators skips $s$ entries of $f \in \mathbb{R}^\mathbb{Z}$.

\begin{definition}\label{def_cross_corr_stride}
	(Valid cross-correlation with stride)
	Given a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$, a finite-dimensional
	input $x \in \mathbb{R}^{n_x}$, stride number $s \in \mathbb{N}$ and output dimension
	\begin{equation*}
		n_{\text{out}} = \left\lfloor \frac{n_x - n_k}{s} \right\rfloor + 1
	\end{equation*}
	the valid-cross correlation with stride is defined as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_{\text{out}}},
		\quad *_{\text{v}}(k,x) := 
		(\mathcal{I}_{n_{\text{out}}}^{-1} \circ D_s)
		(
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}

\todo{Add illustration}

Note that \cref{def_cross_corr_stride} equals \cref{def_cross_corr} for stride number $s=1$.

\subsection{Convolution layers}

The valid cross correlation (without stride) has the output dimension $n_x - n_k + 1$.
Thus, the valid cross-correlation $*_{\text{v}}$ decreases the dimension of the input $x$.
However, we want to incorporate the valid cross-correlation as the layer transform $\layertf$ of an
unit triangular layer. The layer transform $\layertf$ must keep the dimension $d$.
Therefore we apply a so-called padding operation on the layer transform input $p \in \R^d$ 
before passing it to the valid
cross-correlation. The padding operation increases the dimension of $p$ in order
that the composition of padding and cross-correlation keeps the overall dimension.
Let $c \in \mathbb{N}$ denote the number by which the padding operator increases 
the input dimension, such that dimension is $n_x = d+c$. 
In order that the composition of padding and cross-correlation keeps the dimension, it must hold
\begin{equation*}
	n_x - n_k + 1 = \underbrace{(d+c)}_{\substack{
		\text{increased dim.} \\
		\text{by } c \text{ via padding}
	}} - \, n_k + 1 = d \iff c = n_k-1
\end{equation*}
For symmetry reasons, we pad the same number of values at the beginning and 
the end of the input vector $p$. This means that the number of padded values $c$ has to be even,
i.e. we can write $c=2m$ for a $m \in \mathbb{N}_0$.
Consequently, the equation above implies that the kernel size $n_k$ has to be odd, i.e. 
$n_k = 2m+1$. We define two different padding operators,
which both increase the input vector dimension by $2m$.

\begin{definition}
	(Constant padding)
	Given the padding values $l,r \in \mathbb{R}^m$, we define constant padding 
	$c_{\text{pad},m}$ as
	\begin{equation*}
		c_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad c_{\text{pad},m}(p) = c_{\text{pad},m}(p;l,r) := \lb l^T, p^T, r^T \rb^T
	\end{equation*}
\end{definition}
The constant padding $c_{\text{pad},m}$ is an affine linear map in $p$, because we can write
\begin{equation}\label{cpad_affine}
	c_{\text{pad},m}(p;l,r) = c_{\text{pad}}(p;0_m,0_m)
	+ c_{\text{pad},m}(0_d;l,r)
\end{equation}
and $c_{\text{pad},m}(p;0_m,0_m)$ is linear regarding $p$.

\begin{definition}
	(Symmetric padding)
	We define symmetric padding as
	\begin{equation*}
		s_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad s_{\text{pad},m}(p) := (
			\underbrace{p_m, p_{m-1} \dots, p_1}_{m \text{ values}}, \,
			\underbrace{p_1, p_2 \dots, p_d}_{d \text{ values}}, \,
			\underbrace{p_d, p_{d-1} \dots, p_{d-m+1}}_{m \text{ values}}
		)^T
	\end{equation*}
\end{definition}
The symmetric padding $s_{\text{pad},m}$ is a linear map. We call the operation symmetric padding,
because the outermost left and right inner vector entries are mirrored at the beginning and the end
of the vector.

Given an odd kernel $k \in \mathbb{R}^{2m+1}$, we set
\begin{equation*}
	\hat{k} = \hat{k}(k) := \mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}
Then $\hat{k}(-m) = k_1, \, \dots,\, \hat{k}(m) = k_{2m+1}$ and
$\hat{k}(\tau)=0$ for $\abs{\tau} > m$. With $\hat{k}$, we can express the valid cross-correlation as
\begin{equation*}
	*_{\text{v}}(k,x) = \mathcal{I}_{n_x-2m}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \mathcal{I}_{n_x}(x)
	)
\end{equation*}
because
\begin{equation*}
	\mathcal{S}_{m}(\hat{k}) 
	= \mathcal{S}_{m}(\mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k)))
	= \mathcal{S}_{m+(-m-1)}(\mathcal{I}_{2m+1}(k))
	= \mathcal{S}_{-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}

\begin{definition}
	(Symmetric kernel)
	We call an odd kernel $k \in \mathbb{R}^{2m+1}$ symmetric if
	\begin{equation*}
		\hat{k}(\tau) = \hat{k}(-\tau)
	\end{equation*}
	for all $\tau \in \mathbb{Z}$.
\end{definition}

Let us now define symplectic convolution layers by combining padding and valid 
cross-convolution.
\begin{definition}
	(Convolution layers)
	Given a symmetric kernel $k \in \mathbb{R}^{2m+1}$, padding
	$\chi_{\text{pad},m} = c_{\text{pad},m}$ or $\chi_{\text{pad},m} = s_{\text{pad},m}$,
	we call the upper and lower unit triangular layers with bias $b \in \R^{2d}$ and layer transform
	\begin{equation*}
		\layertf(p) := *_{\text{v}}(k,\chi_{\text{pad},m}(p))
	\end{equation*}
	the convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$.
\end{definition}
The symmetric kernel $k \in \mathbb{R}^{2m+1}$ is a learnable parameter.
If $\chi_{\text{pad},m} = c_{\text{pad},m}$, the padding values $l,r \in \R^m$ can either
be learnable parameters or constant. The convolution layers can be used without any bias ($b=0$), a
constant non-learnable bias ($b \neq 0 =$ const), or with a learnable bias parameter 
($b$ is a learnable parameter). We stick to $b=0$ in our experiments.

With $\hat{\chi}_{\text{pad},m}(p) := \mathcal{I}_{d+2m}(\chi_{\text{pad},m}(p))$,
we can write the layer transform as
\begin{equation}\label{eq_conv_layer_transform}
	\layertf(p) =
	\mathcal{I}_{d}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \hat{\chi}_{\text{pad},m}(p)
	)
\end{equation}

\begin{lemma}\label{jac_linear_map}
	Let $f: \mathbb{R}^d \to \mathbb{R}^d$ be a linear map. Then the Jacobian matrix
	$\jac{f}{x}$ is constant, i.e.
	\begin{equation*}
		\jac{f}{x} \bigg|_{x = v} = \jac{f}{x} \bigg|_{x = w} \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
	and for the Jacobian-vector product holds
	\begin{equation*}
		\lb \jac{f}{x} \bigg|_{x = v} \rb w = f(w) \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
\end{lemma}
\begin{proof}
	$f$ is linear $\implies$ There exists a matrix representation $f(x) = Ax$ with
	$A \in \mathbb{R}^{d \times d}$ \\
	$\implies \jac{f}{x} \big|_{x=v} = A = \text{const.} \quad \forall v \in \mathbb{R}^d$
	$\implies \lb \jac{f}{x} \big|_{x=v} \rb w = Aw = f(w) \quad \forall v,w \in \mathbb{R}^d$
\end{proof}
As the Jacobian matrix for a linear map is constant, we omit the evaluation point, i.e.
$\jac{f}{x} \big|_{x=v} = \jac{f}{x}$.

\begin{theorem}\label{thm_conv_const_pad_symplectic}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $\chi_{\text{pad},m} = c_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	We have to show that the Jacobian of the layer transform $\layertf(p)$
	is symmetric (\cref{jacobi_symmetric}).
	Because of \cref{cpad_affine}, it suffices to show the case $l,r=0$ (the Jacobian
	of a constant has only zero-valued entries). For $l,r=0$, the layer transform 
	$\layertf(p)$ 
	is a linear map, because it is a composition of linear maps only.

	Define the bilinear form
	\begin{equation}\label{eq_bilinear_proof_conv}
		b : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R},
		\quad b(v,w) := \ip{v}{\lb \jac{\layertf}{p} \rb w}
	\end{equation}
	To show symmetry of the Jacobian, we show that 
	$b(e_i, e_j) = b(e_j, e_i)$ for all $i,j=1,\dots,d$.
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{\lb \jac{\layertf}{p} \rb e_j}
		= \ip{e_i}{\activation_{\mathcal{C}}(e_j)} \quad \text{(\cref{jac_linear_map})} \\
		&\stackrel{\cref{eq_conv_layer_transform}}{=} \ip{e_i}{\mathcal{I}_{d}^{-1} (
			\mathcal{S}_{m}(\hat{k}) * \hat{c}_{\text{pad},m}(e_j)
		)} \\
		&= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				\underbrace{(\hat{c}_{\text{pad},m}(e_j))(\tau+a)}_{
					= \delta_{(\tau+a) (j+m)}
				}
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((j-i+m)-m) = \hat{k}(j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\begin{theorem}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $\chi_{\text{pad},m} = s_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	With $\chi_{\text{pad},m} = s_{\text{pad},m}$, the layer transform 
	$\layertf$ is linear, because it is a composition of linear maps. Thus,
	we proof the statement the same way as \cref{thm_conv_const_pad_symplectic} above.
	For $\tau,a \in \mathbb{Z}$ we have
	\begin{equation*}
		\hat{s}_{\text{pad},m}(e_j)(\tau + a) = 
		\delta_{(\tau + a) (m-j+1)} + \delta_{(\tau + a) (j+m)} + \delta_{(\tau + a) (2d+m-j+1)}
	\end{equation*}
	Consequently, for $\chi_{\text{pad},m} = s_{\text{pad},m}$ and $i,j=1, \dots, d$, 
	the bilinear form \cref{eq_bilinear_proof_conv} becomes
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				(\hat{s}_{\text{pad},m}(e_j))(\tau+a)
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((m-j+1-i)-m) + \hat{k}((j+m-i)-m) + \hat{k}((2d+m-j+1-i)-m) \\
		&= \hat{k}(1-j-i) + \hat{k}(j-i) + \hat{k}(1+2d-j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\subsubsection*{Parametrization of a symmetric kernel}

Let $\{b_1, b_2, \dots b_m \} \subset \mathbb{R}^{2m+1}$ be a basis of the space 
$\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.
The symmetric kernel $k$ is then parametrized by the coefficients $\beta \in \mathbb{R}^m$ via
\begin{equation*}
	k = (b_1, b_2, \dots, b_m) \beta = \beta_1 b_1 + \beta_2 b_2 + \dots + \beta_m b_m
\end{equation*}

One choice is the canonical basis with basis vectors $b_1, \dots, b_m \in \mathbb{R}^{2m+1}$ given by
\begin{equation*}
	(b_i)_{j+m+1} = \hat{k}(b_i)(j) = \begin{cases}
		1 &: \abs{j} = i-1 \\
		0 &:else
	\end{cases} 
	\quad (j=-m, \dots, m)
\end{equation*}

Another possible basis choice inspired by finite differences is
\todo{Cite paper with similar idea}
\begin{align*}
	b_1^{FD} = (0, \dots, 0,& 1,0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_2^{FD} = (0, \dots, 0, 1,-&2,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_3^{FD} = (0, 0, \dots, 0, 1,-4,& 6,-4,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1} \\
	&\vdots
\end{align*}
In general, the entries for a basis vector $b_i^{FD} \in \mathbb{R}^{2m+1}$ $(1 \leq i \leq m)$ 
originate from Pascal's triangle.
\begin{equation*}
	\lb b_i^{FD} \rb_{j+m+1} = \hat{k}(b_i^{FD})(j) = \begin{dcases}
		(-1)^j \binom{2(i-1)}{j+i-1} &: \abs{j} < i \\
		0 &: \text{else}
	\end{dcases}
	\quad (j=-m, \dots, m)
\end{equation*}

\todo{Move this to Appendix?}
The symmetry of the kernel basis vectors $b_i^{FD}$ follows with the definition of the binomial coefficient.
\begin{align*}
	\binom{2(i-1)}{j+i-1} &= \frac{2(i-1)!}{(j+i-1)!(2(i-1)-(j+i-1))!} \\
	&= \frac{2(i-1)!}{(j+i-1)!(i-j-1)!}
\end{align*}
Thus we have
\begin{equation*}
	\binom{2(i-1)}{(-j)+i-1} = \binom{2(i-1)}{j+i-1}
\end{equation*}
and for $i=1, \dots, m$ and $j= 0, \dots, m$
\begin{align*}
	\hat{k}(b_i^{FD})(-j) &= \begin{dcases}
		(-1)^{-j} \binom{2(i-1)}{(-j)+i-1} : \abs{-j} < i \\
		0 : else
	\end{dcases} \\
	&= \begin{dcases}
		(-1)^{j} \binom{2(i-1)}{j+i-1} : \abs{j} < i \\
		0 : else
	\end{dcases} \\
	&= \hat{k}(b_i^{FD})(j) 
\end{align*}
So $\{ b_i^{FD} \}_{i=1}^m$ is a valid basis of
the space $\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.

In our numerical experiments, it turns out that
parametrization plays an important role how well a neural network learns.

\subsection{Convolution Gradient Layers}

For a fixed kernel $k$, valid cross-correlation is a linear map regarding the second argument. 
Therefore there exists a representation matrix $A_{k}$ with 
$*_{\text{v}}(k,p) = A_{k}p$. Transposed valid cross-correlation 
is defined as the linear map associated with the transpose $A_{k}^T$ of $A_{k}$.

Again, most popular neural network libraries say convolution, but actually implement
valid cross-correlation. In this case, transposed convolution actually refers to
transposed valid cross-correlation.

For large $d \in \mathbb{N}$, it can make sense to use convolution inside Gradient layers 
instead of a full matrix $K$. Let us repeat the layer transfrom $\layertf$ for Gradient layers.

\begin{equation*}
	\layertf(p) = K^T \bigg( a_j \activation \lb (Kp)_j + c_j \rb \bigg)_{j=1}^n
\end{equation*}

A Gradient layer with width $n \in \mathbb{N}$ can be implemented by 
using valid cross-correlation (with stride) for $K^T \in \mathbb{R}^{d \times n}$ and the
corresponding transposed valid cross-correlation (with stride) for $K \in \mathbb{R}^{n \times d}$.
The choice of transpose is intentional, because we want to upscale the input $p \in \mathbb{R}^d$ ($n >> d$). 
Cross-correlation without padding decreases the dimension. Therefore we first apply 
transposed cross-correlation on $p$, because the transpose increases the dimension.
A large kernel size $s_k$ and a large stride value $k$ allow for significant upscaling.
We emphasize that the kernel $k$ has not to be symmetric in this setting.
\todo{Cite similar idea}

\subsection{Normalization}

Batch normalization is a standard method to accelerate training initially proposed by
\citeauthor{batchnorm-ioffe15} in \cite{batchnorm-ioffe15}. 
We introduce a possibility to incorporate batch normalization
into activation and gradient layers while maintaining symplecticity.
\todo{Cite similar idea}

\todo{Sigmoid activation function saturates for large values. Vanishing gradient.
Therefore normalize input before applying activation function.}

Given a mini-batch input $\mathcal{B} = \{  x_1, x_2, \dots, x_{n_{\mathcal{B}}} \} \subset \mathbb{R}^{n}$ 
with size $n_{\mathcal{B}}$, we define the batch normalization transformation as
\begin{equation*}
	\eta_{\gamma, \beta} : \mathbb{R}^n \to \mathbb{R}^n,\quad
	\eta_{\gamma, \beta}(x) 
	:= \lb \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} 
	\lb x_j - \lb \mu_\mathcal{B} \rb_j \rb + \beta_j \rb_{j=1}^n
\end{equation*}
where $n$ is the input dimension of the input $x \in \mathbb{R}^n$ and $\gamma, \beta \in \mathbb{R}^{n}$ 
are learnable parameters.
The scalar $\epsilon \in \mathbb{R}$ is a small positive value to avoid division by zero.
$\sigma^2_\mathcal{B} \in \mathbb{R}^{n}$ refers to the mini-batch variance and
$\mu_\mathcal{B} \in \mathbb{R}^{n}$ refers to the mini-batch mean. The batch normalization results 
in zero mean and unit variance of the whole mini-batch input.

The learnable parameters $\gamma, \beta \in \mathbb{R}^{n}$ allow the neural network to modify
the normalization during training if necessary. The batch normalization transform is able to 
represent the identity transform by setting appropriate $\gamma, \beta$.

The mean $\mu_\mathcal{B} \in \R^n$ and variance $\sigma^2_\mathcal{B} \in \R^n$ are estimated by
\begin{align*}
	\mu_\mathcal{B} &= \frac{1}{n_{\mathcal{B}}} \sum_{i=1}^{n_{\mathcal{B}}} x_i \\
	\sigma^2_\mathcal{B} &= \frac{1}{n_{\mathcal{B}}} 
	\sum_{i=1}^{n_{\mathcal{B}}} (x_i - \mu_\mathcal{B})^2
\end{align*}

During training the mini-batch variance $\sigma^2_\mathcal{B} \in \mathbb{R}^{n}$ and
the mini-batch $\mu_\mathcal{B} \in \mathbb{R}^{n}$ are continously updated in the forward pass.
To be precise, for a new mini-batch input
$\mathcal{B} = \{ x_1, x_2, \dots, x_{n_{\mathcal{B}}} \} \subset \R^n$, 
the mean $\mu_\mathcal{B}$ and variance $\sigma^2_\mathcal{B}$
are updated based on $\mathcal{B}$, before $\eta_{\gamma, \beta}(x_k)$ for
$x_k \in \mathcal{B}$ is evaluated (see Algorithm 1).
When training has finished the neural network does not update $\sigma_\mathcal{B}^2$ 
and $\mu_\mathcal{B}$ anymore. Instead, the neural network remembers the last value from training.

Note that the batch normalization transformation may be incorporated into a layer
deep inside a neural network. If this is the case, the mini-batch input $\mathcal{B}$ 
refers to the collective output of the previous layer.

\begin{algorithm}\label{algo_batch_norm}
	\caption{Batch normalization transform}
	\textbf{Input:} mini batch $\mathcal{B} = \{x_1, x_2, \dots, x_{n_\mathcal{B}}\} \subset \R^n$ 
	and $x_k \in \mathcal{B}$ \\
	\textbf{Output:} $\eta_{\gamma, \beta}(x_k) \in \R^n$
	\setstretch{1.5}
	\begin{algorithmic}
		\If{training\_mode} \Comment{Update mean and variane if training, otherwise
		keep previous values}
			\State $\mu_\mathcal{B} \gets \frac{1}{n_\mathcal{B}} \sum_{i=1}^{n_\mathcal{B}} x_i$
			\State $\sigma^2_\mathcal{B} \gets \frac{1}{n_\mathcal{B}} \sum_{i=1}^{n_\mathcal{B}} 
			(x_i - \mu_\mathcal{B})^2$
		\EndIf
		\State \Return 
		$\lb \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} 
		\lb (x_k)_j - \lb \mu_\mathcal{B} \rb_j \rb + \beta_j \rb_{j=1}^n$
	\end{algorithmic}
\end{algorithm}

The batch normalization transform can be implemented in a straightforward way with modern
neural network libraries, as during training the Jacobian $\jac{\eta_{\gamma, \beta}}{(\gamma, \beta)}$ 
is obtained via automatic differentiation.

If the training data is small enough, so that splitting the data into multiple mini batches
is not necessary, the mean and variance are calculated for the whole training data set. This is the
case for our numerical experiments, as we work with very small training data sets.

\subsubsection{Normalized gradient layers}

We add normalization to gradient layers.

\begin{definition}
	(Normalized gradient layers - Variant 1)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \activation
		\lb \overline{p}_j \rb \bigg)_{j=1}^n
		\quad \text{with } \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	the upper and lower normalized gradient layers 
	$\mathcal{G}_\up^{\eta}$ and $\mathcal{G}_\low^{\eta}$ (variant 1).
\end{definition}

\begin{definition}
	(Normalized gradient layers - Variant 2)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon}
		\activation \lb \overline{p}_j \rb \bigg)_{j=1}^n
		\quad \text{with } \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	the upper and lower normalized gradient layers 
	$\mathcal{G}_\up^{\eta}$ and $\mathcal{G}_\low^{\eta}$ (variant 2).
	The variance $\sigma^2_\mathcal{B} \in \R^n$ refers to the mini-batch variance of $Kp+c$.
\end{definition}

$K \in \mathbb{R}^{n \times d}$ and $a,c, \gamma, \beta \in \mathbb{R}^n$
are learnable parameters.

\todo{Add special note for convolution gradient layers, respect convolution when normalizing.}

\todo{Discuss variant 1 vs. variant 2 (numerical experiments)}

\todo{Maybe add illustration for normalized gradient layer}

\begin{corollary}\label{cor_norm_gradient_layers_symp}
	Given an activation function $\sigma \in C^1(\R)$,
	the normalized gradient layers $\mathcal{G}^{\eta}_{up}$ and $\mathcal{G}^{\eta}_{down}$
	(variant 1 and variant 2) are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. For arbitrary $\alpha \in \R^n$, we define the potential
	\begin{equation*}
		V(p) := \sum_{j=1}^n \alpha_j a_j \mathcal{A}(\overline{p}_j)
		\quad \text{with } \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $i=1, \dots, d$
	\begin{align*}
		(\grad{V(p)})_i &= \deldel{p_i} \lb \sum_{j=1}^n \alpha_j a_j \mathcal{A}(\hat{p}_j) \rb
		= \sum_{j=1}^n \alpha_j a_j \activation(\overline{p}_j) \deldel{p_i} \overline{p}_j
		= \sum_{j=1}^n \alpha_j a_j \activation(\overline{p}_j)
		\frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} \underbrace{K_{ji}}_{=K^T_{ij}} \\
		%
		\implies
		\grad{V(p)}
		&= K^T \bigg( \alpha_j a_j \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon}
		\activation \lb \overline{p}_j \rb \bigg)_{j=1}^n
	\end{align*}
	If we choose $\alpha_j = 1$, follows $\grad{V(p)} = \layertf(p)$ for variant 2.
	If we choose $\alpha_j = \frac{(\sigma^2_\mathcal{B})_j + \epsilon}{\gamma_j}$,
	follows $\grad{V(p)} = \layertf(p)$ for variant 1.
	With \cref{gradient_corollary} follows that both normalized Gradient layer variants
	$\mathcal{G}_{up}^\eta$ and $\mathcal{G}_{low}^\eta$ are symplectic.
\end{proof}

\todo{Introduce N1-G-SympNet and N2-G-SympNet}

\subsubsection{Normalized activation layers}

\begin{definition}
	(Normalized activation layers - Variant 1)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \activation(\hat{p}_i) \rb_{i=1}^d
		\quad \text{with } \hat{p} = \eta_{\gamma, \beta} \lb p \rb \in \R^d
	\end{equation*}
	the normalized upper and lower activation layers $\mathcal{N}_\up^\eta$ and $\mathcal{N}_\low^\eta$
	(variant 1).
\end{definition}

\begin{definition}
	(Normalized activation layers - Variant 2)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \frac{\gamma_i}{(\sigma^2_\mathcal{B})_i + \epsilon} 
		\activation(\hat{p}_i) \rb_{i=1}^d
		\quad \text{with } \hat{p} = \eta_{\gamma, \beta} \lb p \rb \in \R^d
	\end{equation*}
	the normalized upper and lower activation layers $\mathcal{N}_\up^\eta$ and $\mathcal{N}_\low^\eta$
	(variant 2).
	The variance $\sigma^2_\mathcal{B} \in \R^d$ refers to the mini-batch variance of $p$.
\end{definition}

The coefficients $a \in \mathbb{R}^d$ are learnable parameters.

\begin{corollary}
	Given an activation function $\sigma \in C^1(\R)$,
	the normalized activation layers $\mathcal{N}^{\eta}_{up}$ and $\mathcal{N}^{\eta}_{low}$
	(variant 1 and variant 2) are symplectic.
\end{corollary}
\begin{proof}
	Symplecticity follows because the normalized activation layers are a special case
	of normalized gradient layers (choose $n=d$, $K=I_d$ and $c=0$ for normalized gradient layers). 
	We have already shown that both normalized gradient layer variants are symplectic in
	\cref{cor_norm_gradient_layers_symp}.
\end{proof}

\todo{Introduce N1-LA-SympNet and N2-LA-SympNet}

\section{Relation to geometric integrators}

The symplectic Euler and Störmer-Verlet schemes are two geometric integrators. Geometric integrators
are numerical integrators for ODE systems, which preserve a geometric property. In our context,
this geometric property is symplecticity. Precisely, if we denote the numerical integrator with
$\phi^{\text{h}}_{t,H} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$, the map $\phi^{\text{h}}_{t,H}$ is
symplectic.

The two variants of the symplectic Euler scheme are given by
\begin{equation*}
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H} (p_{n+1}, q_n) \\
			q_{n+1} &= q_n + h \grad[p]{H}(p_{n+1}, q_n)	
	\end{split}
	\quad\quad \text{or} \quad\quad
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H}(p_n, q_{n+1}) \\
			q_{n+1} &= q_n + h \grad[p]{H}{p}(p_n, q_{n+1})	
	\end{split}
\end{equation*}

The $p$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	p_{n+1/2} &= p_n - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_n) \\[7pt]
	q_{n+1} &= q_n + \frac{h}{2} \lb \grad[p]{H}(p_{n+1/2}, q_n) + \grad[p]{H}(p_{n+1/2}, q_{n+1}) \rb \\[7pt]
	p_{n+1} &= p_{n+1/2} - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_{n+1})
\end{align*}
and the $q$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	q_{n+1/2} &= q_n + \frac{h}{2} \grad[p]{H}(p_n, q_{n+1/2}) \\[7pt]
	p_{n+1} &= q_n - \frac{h}{2} \lb \grad[q]{H}(p_n, q_{n+1/2}) + \grad[q]{H}(p_{n+1}, q_{n+1/2}) \rb \\[7pt]
	q_{n+1} &= q_{n+1/2} + \frac{h}{2} \grad[p]{H}(p_{n+1}, q_{n+1/2})
\end{align*}
We refer to \citet[p.~189 and p.~190]{hairer2006} for details.

If the Hamiltonian $H$ is separable, i.e. $H(q,p) = U(q) + V(p)$, the sympletic Euler and Störmer-Verlet
schemes become explicit. Then given $\grad[p]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $p \mapsto \grad[p]{H}(p)$ 
and $\grad[q]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $q \mapsto \grad[q]{H}(q)$,
the left variant of the symplectic Euler scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix} \begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix} 
\end{equation*}
and the right variant of the symplectic Euler scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

Similarly, the $p$-staggered Störmer-Verlet scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & 0 \\
		-\frac{h}{2} \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{bmatrix}
		id & 0 \\
		-\frac{h}{2} \grad[q]{H} & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
and the $q$-staggered Störmer-Verlet scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & \frac{h}{2} \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & \frac{h}{2} \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

To conclude, the explicit Euler and Störmer-Verlet schemes can be expressed with
the same unit triangular structure we use for SympNets. This means that a SympNet is able
reproduce both schemes exactly or approximately, provided appropriate layer choices.
\todo{Recurrent networks?}

Symplecticity for the explicit Euler and Störmer-Verlet schemes follows directly 
from \cref{gradient_corollary} and the fact that the composition of symplectic maps is again symplectic.

\section{Numerical experiments}

\subsection{Low-dimensional systems}

Training data uniformly sampled from phase space.

Time step $t=0.1$

\subsubsection{Harmonic Oscillator}

Given $q,p \in \R$, the Hamiltonian for the Harmonic Oscillator is given by
\begin{equation*}
	H(q,p) = \frac{p^2}{2m} + \frac{1}{2} kq^2
\end{equation*}
For the experiment, we choose $m=1, k=1$.

The flow for the Harmonic Oscillator is linear. Because of ???, a SympNet consisting of nine alternating 
upper and lower linear layers should be able to learn the flow for the Harmonic Oscillator.
We verify this with a numerical experiment.

\todo{Show why flow is linear}

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{axis}[
		ymode=log,
		no markers,
		title={Test loss},
		xlabel={Epoch},
		ylabel={Loss},
		legend entries={
			Linear SympNet
		}
	]

	\addplot[
		color=orange
	] table[x=epoch,y=loss,col sep=comma] {../data/harmonic_oscillator/l-sympnet/sigmoid/test_loss.csv};
		
	\end{axis}
\end{tikzpicture}
\caption{Test loss for linear SympNet on Harmonic Oscillator.}\label{fig_harm_osc_loss}
\end{figure}

Indeed \cref{fig_harm_osc_loss} shows that the linear SympNet successfully 
learns the flow for the Harmonic Oscillator.

\subsubsection{Simple Pendulum}

\begin{equation*}
	H(q,p) = \frac{p^2}{2ml^2} + mgl (1-cos(q))
\end{equation*}
For the experiment, we choose $m=1, g=1, l=1$.

Training data sampled from swinging domain
$(q,p) \in [-\frac{\pi}{2}, \frac{\pi}{2}] \times [-\sqrt{2}, \sqrt{2}]$

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[	
		group style={
			group size=2 by 3,
			y descriptions at=edge left,
			horizontal sep=0.1cm,
			vertical sep=2.2cm,
		},
		xlabel=Epoch, ylabel=Loss,
		width=\axisdefaultwidth, height=7cm,
		no markers,
		ymax=1e-1, ymin=5e-10, ymode=log,
		legend style={nodes={scale=0.75, transform shape}},
		legend entries={
			FNN,
			LA-SympNet,
			N1-LA-SympNet,
			N2-LA-SympNet,
			G-SympNet,
			N1-G-SympNet,
			N2-G-SympNet
		}
	]
		\nextgroupplot[title={Training loss (sigmoid activation)}]
			\addplot[
				color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/sigmoid/loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/sigmoid/loss.csv};

			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/sigmoid/loss.csv};

			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/sigmoid/loss.csv};

		\nextgroupplot[title={Test loss (sigmoid activation)},]
			\addplot[
				color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/sigmoid/test_loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/sigmoid/test_loss.csv};

		\nextgroupplot[title={Training loss (tanh activation)}]
			\addplot[
			color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/tanh/loss.csv};

			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/loss.csv};

			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/tanh/loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/tanh/loss.csv};

			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/loss.csv};

			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/tanh/loss.csv};

		\nextgroupplot[title={Test loss (tanh activation)}]
			\addplot[
			color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/test_loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/tanh/test_loss.csv};

			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/test_loss.csv};

			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/tanh/test_loss.csv};

		\nextgroupplot[title={Training loss (ELU activation)}]

			\addplot[
				color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/elu/loss.csv};
		
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/elu/loss.csv};
		
			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/elu/loss.csv};
		
			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/elu/loss.csv};
		
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/elu/loss.csv};
		
			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/elu/loss.csv};
		
			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/elu/loss.csv};

		\nextgroupplot[title={Test loss (ELU activation)}]
			\addplot[
				color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/elu/test_loss.csv};
		
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/elu/test_loss.csv};
		
	\end{groupplot}
\end{tikzpicture}
\caption{Training loss and test loss for training data from simple pendulum, swinging domain. 
(First row) sigmoid activation (Second row) tanh activation (Third row) ELU activation}
\end{figure}

% \pgfplotstabletypeset[col sep=comma,
% 	 columns/architecture/.style={string type}
%     ] {../data/simple_pendulum_swing/test_loss_summary.csv}

% \begin{tikzpicture}
% 	\begin{axis}[
% 		width = \linewidth,
% 		title={Phase plot (swinging case)},
% 		xlabel={q},
% 		ylabel={p},
% 		no markers, smooth
% 	]

% 	\addplot[
% 		color=gray,thick
% 	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/phase_plot.csv};

% 	\addplot[
% 		color=red
% 	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/swinging_case/phase_plot.csv};

% 	\addplot[
% 		color=red,dashed,
% 		select coords between index={0}{72}
% 	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/swinging_case/phase_plot.csv};

% 	\addplot[
% 		color=red,densely dotted,
% 		select coords between index={0}{72}
% 	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/swinging_case/phase_plot.csv};

% 	\addplot[
% 		color=brown
% 	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/fnn/swinging_case/phase_plot.csv};

% 	\addplot[
% 		color=blue
% 	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/swinging_case/phase_plot.csv};

% 	\addplot[
% 		color=blue,dashed,
% 		select coords between index={0}{72}
% 	] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/swinging_case/phase_plot.csv};

% 	%\addplot[
% 	%	color=blue,densely dotted,
% 	%	select coords between index={0}{72}
% 	%] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/swinging_case/phase_plot.csv};
		
% 	\end{axis}
% \end{tikzpicture}

% \begin{tikzpicture}
% 	\begin{axis}[
% 		width = \linewidth,
% 		height = \axisdefaultheight,
% 		no markers, smooth,
% 		title={Total energy},
% 		xlabel={t},
% 		ylabel={Total energy},
% 		legend style={draw=none},
% 		legend entries={
% 			Störmer-Verlet,
% 			FNN,
% 			LA-SympNet,
% 			N1-LA-SympNet,
% 			N2-LA-SympNet,
% 			G-SympNet,
% 			N1-G-SympNet,
% 			N2-G-SympNet
% 		}
% 	]

% 	\addplot[
% 		color=gray
% 	] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/total_energy.csv};

% 	\addplot[
% 		color=brown
% 	] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/fnn/swinging_case/total_energy.csv};

% 	\addplot[
% 		color=red
% 	] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/swinging_case/total_energy.csv};

% 	\addplot[
% 		color=orange
% 	] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/swinging_case/total_energy.csv};

% 	\addplot[
% 		color=violet
% 	] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/swinging_case/total_energy.csv};

% 	\addplot[
% 		color=blue
% 	] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/swinging_case/total_energy.csv};

% 	\addplot[
% 		color=cyan
% 	] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/swinging_case/total_energy.csv};
		
% 	\end{axis}
% \end{tikzpicture}

Rotating domain

$(q,p) \in [-20, 20] \times [0.5, 2.5]$

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[
		group style={
			group size=3 by 1,
			y descriptions at=edge left,
			horizontal sep=0.1cm,
			vertical sep=2.2cm,
		},
		xlabel=Epoch, ylabel={Test loss},
		width=6.3cm, height=7cm,
		no markers,
		ymax=1e-1, ymin=5e-6, ymode=log
	]

	\nextgroupplot[
		title={Sigmoid},
		legend entries={
			FNN,
			LA-SympNet,
			N1-LA-SympNet,
			N2-LA-SympNet,
			G-SympNet,
			N1-G-SympNet,
			N2-G-SympNet
		},
		legend columns=4,
		legend to name=grouplegend
	]
		\addplot[
			color=brown
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/fnn/sigmoid/test_loss.csv};

		\addplot[
			color=red
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/la-sympnet/sigmoid/test_loss.csv};

		\addplot[
			color=red, dashed
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n1-la-sympnet/sigmoid/test_loss.csv};

		\addplot[
			color=red, densely dotted
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n2-la-sympnet/sigmoid/test_loss.csv};

		\addplot[
			color=blue
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/g-sympnet/sigmoid/test_loss.csv};

		\addplot[
			color=blue, dashed
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n1-g-sympnet/sigmoid/test_loss.csv};

		\addplot[
			color=blue, densely dotted
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n2-g-sympnet/sigmoid/test_loss.csv};

	\nextgroupplot[title={tanh}]
		\addplot[
			color=brown
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/fnn/tanh/test_loss.csv};

		\addplot[
			color=red
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/la-sympnet/tanh/test_loss.csv};

		\addplot[
			color=red, dashed
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n1-la-sympnet/tanh/test_loss.csv};

		\addplot[
			color=red, densely dotted
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n2-la-sympnet/tanh/test_loss.csv};

		\addplot[
			color=blue
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/g-sympnet/tanh/test_loss.csv};

		\addplot[
			color=blue, dashed
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n1-g-sympnet/tanh/test_loss.csv};

		\addplot[
			color=blue, densely dotted
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n2-g-sympnet/tanh/test_loss.csv};

	\nextgroupplot[title={ELU}]
		\addplot[
			color=brown
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/fnn/elu/test_loss.csv};

		\addplot[
			color=red
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/la-sympnet/elu/test_loss.csv};

		\addplot[
			color=red, dashed
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n1-la-sympnet/elu/test_loss.csv};

		\addplot[
			color=red, densely dotted
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n2-la-sympnet/elu/test_loss.csv};

		\addplot[
			color=blue
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/g-sympnet/elu/test_loss.csv};

		\addplot[
			color=blue, dashed
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n1-g-sympnet/elu/test_loss.csv};

		\addplot[
			color=blue, densely dotted
		] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_rot/n2-g-sympnet/elu/test_loss.csv};
			
	\end{groupplot}
\end{tikzpicture}
\\[5pt]
\ref{grouplegend}
\caption{Simple pendulum, rotating domain.}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[
		group style={
			group size=2 by 1
		}
	]
		
	\nextgroupplot[
		title={Phase plot (swinging)},
		legend entries={
			Exact,
			FNN (tanh),
			N2-LA-SympNet (sigmoid),
			N2-G-SympNet (tanh)
		},
		legend columns=4,
		legend to name=leg_ped_rot_phase
	]
		\addplot[
			color=gray,thick
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/exact/swinging_case/phase_plot.csv};

		\addplot[
			color=brown
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/fnn/tanh/swinging_case/phase_plot.csv};

		\addplot[
			color=red,densely dotted,thick
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/n2-la-sympnet/sigmoid/swinging_case/phase_plot.csv};

		\addplot[
			color=blue,densely dotted,thick
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/n2-g-sympnet/tanh/swinging_case/phase_plot.csv};

	\nextgroupplot[title={Phase plot (rotating)}]
		\addplot[
			color=gray,thick
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/exact/rotating_case/phase_plot.csv};

		\addplot[
			color=brown
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/fnn/tanh/swinging_case/phase_plot.csv};

		\addplot[
			color=red,densely dotted,thick
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/n2-la-sympnet/sigmoid/rotating_case/phase_plot.csv};

		\addplot[
			color=blue,densely dotted,thick
		] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_rot/n2-g-sympnet/tanh/rotating_case/phase_plot.csv};

	\end{groupplot}
\end{tikzpicture}
\\[5pt]
\ref{leg_ped_rot_phase}
\caption{Phase plots for simple pendulum. Training data uniformly sampled from rotating domain.}
\end{figure}

Swinging and rotating domain

$(q,p) \in [-20, 20] \times [-2.5, 2.5]$

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[
		group style={
			group size=3 by 1,
			y descriptions at=edge left,
			horizontal sep=0.1cm,
			vertical sep=2.2cm,
		},
		xlabel=Epoch, ylabel={Test loss},
		width=6.3cm, height=7cm,
		no markers,
		ymax=1e-1, ymin=5e-6, ymode=log
	]
	
		\nextgroupplot[
			title={Sigmoid},
			legend entries={
				FNN,
				LA-SympNet,
				N1-LA-SympNet,
				N2-LA-SympNet,
				G-SympNet,
				N1-G-SympNet,
				N2-G-SympNet
			},
			legend columns=4,
			legend to name=grouplegend
		]
			\addplot[
				color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/fnn/sigmoid/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/sigmoid/test_loss.csv};
	
		\nextgroupplot[title={tanh}]
			\addplot[
				color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/fnn/tanh/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/tanh/test_loss.csv};
	
		\nextgroupplot[title={ELU}]
			\addplot[
				color=brown
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/fnn/elu/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=red, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=red, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=blue, dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=blue, densely dotted
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/elu/test_loss.csv};
				
	\end{groupplot}
\end{tikzpicture}
\\[5pt]
\ref{grouplegend}
\caption{Simple pendulum, swinging and rotating domain.}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 1
			},
			xlabel=$q$, ylabel=$p$
		]
			
		\nextgroupplot[
			title={Phase plot (swinging)},
			legend entries={
				Exact,
				FNN (tanh),
				N1-LA-SympNet (tanh),
				G-SympNet (tanh)
			},
			legend columns=4,
			legend to name=leg_ped_swing_rot_phase
		]
			\addplot[
				color=gray,thick
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/phase_plot.csv};
	
			\addplot[
				color=brown
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/fnn/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=red,densely dotted,thick
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/n1-la-sympnet/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=blue
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/tanh/swinging_case/phase_plot.csv};
	
		\nextgroupplot[title={Phase plot (rotating)}]
			\addplot[
				color=gray,thick
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/phase_plot.csv};
	
			\addplot[
				color=brown
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/fnn/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=red,densely dotted,thick
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/n1-la-sympnet/tanh/rotating_case/phase_plot.csv};
	
			\addplot[
				color=blue
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/tanh/rotating_case/phase_plot.csv};
	
		\end{groupplot}
	\end{tikzpicture}
	\\[5pt]
	\ref{leg_ped_swing_rot_phase}
	\caption{Simple pendulum, swinginig and rotating domain, phase plots.}
\end{figure}

\subsection{High-dimensional systems}

\todo{Describe Hamiltonian PDEs. A PDE describes system locally, thus it makes sense to share parameters, for
example in the form of CNNs etc.}

Assumptions:
\begin{itemize}
	\item \todo{Original PDE has constant coefficients Or rather should not depend on $x$ itself?}
	\item Equidistant grid points
\end{itemize}

\todo{Explain why we need assumptions.}

\subsubsection{Linear wave equation}

\todo{The choice of basis for the symmetric convolution kernels is critical (for fast convergence)!}

\subsubsection{Sine-Gordon}

\todo{Padding modes}

\todo{Activation layer with sin works well if $a \in \mathbb{R}$, i.e. activation function is applied
isotropic.}

%----------------------------------------------------------------------------------------
%
% Resume
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\section{R\'esum\'e}
\subsection{Summary and conclusion}

\subsection{Outlook}

%----------------------------------------------------------------------------------------
%
% Appendices
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\section{First Appendix Section}

\newpage~\newpage
\section{Declaration of authorship}

\vspace{3cm}

\begin{table}[h!]
\centering
\begin{tabular}{|p{13cm}|}
\hline\\
	I hereby certify
	\begin{enumerate}
		\item that this thesis has been composed by me and is based on my own work, unless stated otherwise,
		\item that all direct or indirect sources used are acknowledged as references and all extracts from work of others, either verbatim or in spirit, are stated as such,
		\item that neither the thesis itself nor parts of this thesis have been part of another examination procedure,
		\item that neither the thesis itself nor parts of this thesis have been published and
		\item that all copies of this thesis, either digital or printed, coincide.
	\end{enumerate}
	Therewith, this declaration of authorship is in accordance with the examination regulations from 22th July 2016 of the bachelor's program \emph{Simulation Technology} of the University of Stuttgart.\\\\
\hline
\end{tabular}
\end{table}

\vspace{4cm}
\hrulefill\\
Name
\hspace{7cm}
Date, City, Signature
\end{appendices}
%----------------------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\addcontentsline{toc}{section}{References}
\bibliographystyle{abbrvnat}
\bibliography{../../literature/references.bib}

\end{document}
