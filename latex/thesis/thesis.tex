\documentclass[twoside,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,bindingoffset=0.5cm,inner=2.5cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry}
%\usepackage[a4paper,bindingoffset=1cm,inner=2cm,outer=2cm,top=2.5cm,bottom=2.5cm]{geometry} %,showframe
\usepackage{helvet}
\usepackage[T1]{fontenc}
\renewcommand{\familydefault}{\sfdefault}
% \usepackage[german,ngerman]{babel}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amstext,amssymb,bm}
\usepackage{mathtools}
\usepackage{xcolor,color}
\usepackage{pifont}
\usepackage{array}
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{subfig}
\usepackage{tabularx}
\usepackage[numbers]{natbib}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{algpseudocode,algorithm}
\usepackage{placeins}
%%% Title page
\usepackage{common/titlePageST}
%%% Appendix in TOC
\usepackage[toc,page]{appendix}
%%% Setstrech on title page
\usepackage{setspace}
%%% Tilde in URL in literature
\usepackage{url}
%%% multiple rows
\usepackage{multirow}
%%% fancy column and row seperators
\usepackage{hhline}
%%% custom items in enumerate and itemize
\usepackage{enumitem}
%%% custom format for algorithm comments
\algrenewcommand{\algorithmiccomment}[1]{\hskip3em // #1}

\clubpenalty=5000
\widowpenalty=5000

\setlength{\emergencystretch}{2cm}
%----------------------------------------------------------------------------------------
%	abbreviation includes
%----------------------------------------------------------------------------------------
\input{common/default_abbrev}
\input{common/specific_abbrev}
%%% Clever refing
\usepackage[capitalise,noabbrev]{cleveref}

%----------------------------------------------------------------------------------------
%	References with cleveref
%----------------------------------------------------------------------------------------
\crefformat{equation}{(#2#1#3)}

%----------------------------------------------------------------------------------------
%	Theorem environments
%----------------------------------------------------------------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{example}{Example}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}

%----------------------------------------------------------------------------------------
%	fancyhdr
%----------------------------------------------------------------------------------------
\usepackage{fancyhdr}

%----------------------------------------------------------------------------------------
%	pgfplots
%----------------------------------------------------------------------------------------
\usepackage{pgfplots,pgfplotstable,booktabs}
\pgfplotsset{compat=1.16}
\usepgfplotslibrary{groupplots}
\usepgfplotslibrary{external}
\tikzexternalize
\tikzsetexternalprefix{figures_cache/}

\pgfplotsset{
	every axis/.append style={
		ymajorgrids,
		grid style={dashed,lightgray,semithick},
	}
}

\pgfplotsset{
  	every axis plot/.append style={line width=0.7pt,},
}

% Style to select only points from #1 to #2 (inclusive)
\pgfplotsset{select coords between index/.style 2 args={
    x filter/.code={
        \ifnum\coordindex<#1\def\pgfmathresult{}\fi
        \ifnum\coordindex>#2\def\pgfmathresult{}\fi
    }
}}

\pgfplotstableset{
	every head row/.style={ before row=\toprule,after row=\midrule}, 
	every last row/.style={ after row=\bottomrule}
}
%----------------------------------------------------------------------------------------

\makeatletter
\newcommand{\theauthor}{Steffen Müller} %
\makeatother

\renewcommand{\headrulewidth}{0.3pt}
\renewcommand{\footrulewidth}{0.3pt}
\fancyfoot[OL,ER]{University of Stuttgart} % inner
\fancyfoot[OR,EL]{\thepage} % outer
\fancyhead[OL,ER]{\theauthor} % inner 
\fancyhead[OR,EL]{IANS -- Institute of Applied Analysis and Numerical Simulation} % outer
\fancyfoot[C]{}

% \headsep=4mm
% \footskip=4mm
\parindent=0mm
\parskip=6pt
% \renewcommand{\footskip}{3pt}

%----------------------------------------------------------------------------------------
%	Something
%----------------------------------------------------------------------------------------
\usepackage{textpos}
\setlength{\TPHorizModule}{1mm}%
\setlength{\TPVertModule}{1mm}%
% \headsep=5mm
% \footskip=5mm
\pagestyle{fancy}
\newcommand{\articleheading}[3]{
{\large #1}\\[3mm]
{\Large\bf #2}\\[3mm]
{\large #3}
}

%----------------------------------------------------------------------------------------
%	Start Document
%----------------------------------------------------------------------------------------
\begin{document}
\pagenumbering{roman}
%----------------------------------------------------------------------------------------
%
% TITLE PAGE
%
%----------------------------------------------------------------------------------------
%------------------------------------------
\begin{titlePageST}
%------------------------------------------
\makeLogo%
{-10pt}{
\includegraphics[width=0.7\textwidth]{figures/logos/simtech.pdf}
}%
{0pt}{
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figures/logos/ians.pdf}
	\end{center}}%
{0pt}{
\begin{flushright}
	\vspace{-10pt}
	\includegraphics[width=0.9\textwidth]{figures/logos/unistuttgart_logo_englisch_cmyk.eps}
\end{flushright}}%
\vspace{35pt}%
%------------------------------------------
\makeHeader%
[Research Group: Numerical Mathematics] %
{Institute of Applied Analysis and Numerical Simulation} %
\vspace{50pt}%
%------------------------------------------
\makeTitle%
{Simulation Technology Degree Course} %
{Bachelor Thesis} %
\vspace{80pt}%
%------------------------------------------
\makeTitleThesis%
{Symplectic Neural Networks}
\vspace{90pt}%
%------------------------------------------
\begin{supervisorST}{3}%
\addSuper%
{First Reviewer}%
{Prof. Dr. B. Haasdonk}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\addSuper%
{Second Reviewer}%
{Prof. Dr. D. Pflüger}%
{Institute of Parallel and Distributed\\[-0.2cm]
Systems (Scientific Computing)}%
\addSuper%
{Advisor}%
{Patrick Buchfink, M.Sc.}%
{Institute of Applied Analysis and\\[-0.2cm]
Numerical Simulation (IANS)}%
\end{supervisorST}%
\vspace{80pt}%
%------------------------------------------
\begin{authorST}{Submitted by}%
\addAuthorInfo{Author}{Steffen Müller}
\addAuthorInfo{Student ID}{3260643}
\addAuthorInfo{SimTech ID}{119} %
\addAuthorInfo{Submission Date}{...} %FILLIN
\end{authorST}%
%------------------------------------------
\end{titlePageST}
%----------------------------------------------------------------------------------------
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ABSTRACT
%
%----------------------------------------------------------------------------------------
\section*{Abstract}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% ACKNOWLEDGEMENTS
%
%----------------------------------------------------------------------------------------
\section*{Acknowledgements}
...
\clearpage
\newpage\thispagestyle{plain}\null
\newpage\thispagestyle{plain}

%----------------------------------------------------------------------------------------
%
% TOC
%
%----------------------------------------------------------------------------------------
\setcounter{tocdepth}{2}
\tableofcontents
\clearpage
\newpage\thispagestyle{plain}\null
%----------------------------------------------------------------------------------------
%
% Begin with document content
%
%----------------------------------------------------------------------------------------
\newpage
\pagenumbering{arabic} 
%----------------------------------------------------------------------------------------
%
% Introduction
%
%----------------------------------------------------------------------------------------
\section{Introduction}

\subsection{Outline}

\subsection{Notation}

The $d$-by-$d$ identity matrix is denoted by $I_d$. If the dimension $d$ can be inferred
from context we may just write $I$.

$\onevec{n} \in \R^n$ denotes the $n$-dimensional $1$-vector.

We denote the $k$-th partial derivative of a function $f: \mathbb{R}^n \to \mathbb{R},
x \mapsto f(x)$ with $\deldel{x_k} f(x)$.

If $f: \mathbb{R}^{n_1} \to \mathbb{R}^{n_2}$ is a multi-dimensional function

\todo{"partial derivative" of a vector, Jacobian matrices}

\todo{Gradient}

\todo{Describe special meaning of dimension $d$}

\begin{equation*}
	J := \begin{pmatrix}
		0 & I_d \\
		-I_d & 0
	\end{pmatrix}
\end{equation*}

vector $v \in \R^n$
\begin{equation*}
	v = \begin{pmatrix}
		v_i
	\end{pmatrix}_{i=1}^n
\end{equation*}

We denote the $i$-th entry of a vector $v \in \R^n$ with $v_i$ or $(v)_i$.

For $q,p \in \R^{d}$ and maps $f_{11}, f_{12}, f_{21}, f_{22} : \R^d \to \R^d$
we define the block operator as
\begin{equation*}
	\begin{bmatrix}
		f_{11} & f_{12} \\
		f_{21} & f_{22}
	\end{bmatrix}
	\qpvec
	:= \begin{pmatrix}
		f_{11}(q) + f_{12}(p) \\
		f_{21}(q) + f_{22}(p)
	\end{pmatrix} \in \R^{2d}
\end{equation*}

identity map $id$

$\R^{\mathbb{Z}} := \{ f : \mathbb{Z} \to \mathbb{R} \}$

$\N$ without zero, $\N_0$ with zero.

Kronecker delta
\begin{equation*}
	\delta_{ij} := \begin{cases}
		1 &: i = j \\
		0 &: \text{else}
	\end{cases}
\end{equation*}

\begin{equation*}
	\norm{f}_{C^r(W,\R^n)} := ...
\end{equation*}


%----------------------------------------------------------------------------------------
%
% Content
%
%----------------------------------------------------------------------------------------
\newpage
\section{Problem setup}

\begin{definition}
	A matrix $A \in \mathbb{R}^{2d \times 2d}$ is called symplectic if $A^TJA=J$.
\end{definition}

\begin{definition}
	A differentiable map $\phi : U \to \mathbb{R}^{2d}$ (where $U \subset \mathbb{R}^{2d}$ is an open set)
	is called symplectic if the Jacobian matrix $\jac{\phi}{x}$ is everywhere symplectic, i.e.
	\begin{equation*}
		\lb \jac{\phi}{x} \rb^T J \lb \jac{\phi}{x} \rb = J
	\end{equation*}
\end{definition}

\begin{definition}
	A Hamiltonian (ODE) system can be written in canonical form as
	\begin{align*}
		\dot{y}(t) &= J \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
		y(t_0) &= y_0
	\end{align*}
	where $y: I \subset \mathbb{R} \to \mathbb{R}^{2d},\, t \mapsto y(t) = (q(t),p(t))$ and 
	$y_0 = (q_0, p_0) \in \mathbb{R}^{2d}$ the initial value for $t_0 \in \mathbb{R}$. 
	The function $H: \mathbb{R}^{2d} \to \mathbb{R}$ is called the Hamiltonian 
	or the total energy. $q = q(t) \in \mathbb{R}^d$ are called generalized coordinates
	and $p=p(t) \in \mathbb{R}^d$ are called conjugate momenta. 
	The phase space $\mathcal{V} = \mathbb{R}^{2d}$ has even dimension for Hamiltonian systems.
\end{definition}

Note that the Hamiltonian $H$ is a first integral, i.e. the total energy is preserved, because
\begin{equation*}
	\ddt H(y(t)) = \lsb \grad{H}(y(t)) \rsb^T \dot{y}(t) = 
	\lsb \grad{H}(y(t)) \rsb^T J \grad{H}(y(t)) = 0
\end{equation*}
since $J$ skew-symmetric.

Let $\phi_{t,H} : U \subset \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ be the flow for a 
canonical Hamiltonian system with Hamiltonian $H$ for a fixed time step $t$, i.e.
\begin{equation*}
	\phi_{t,H}\begin{pmatrix}
		q_0 \\
		p_0
	\end{pmatrix}
	= \begin{pmatrix}
		q(t; q_0, p_0) \\
		p(t; q_0, p_0)
	\end{pmatrix}
\end{equation*}
where $q(t; q_0, p_0)$ and $p(t; q_0, p_0)$ denote the solution of the Hamiltonian system
at time $t$ for initial conditions $y_0 = (q_0,p_0) \in \mathbb{R}^{2d}$. 
Poincaré has shown that the flow of a Hamiltonian system is symplectic.

\begin{theorem}(Poincaré 1899)
	Let $H: \mathbb{R}^{2d} \to \mathbb{R}$ be a twice continuously differentiable
	function on $U \subset \mathbb{R}^{2d}$. Then, for each fixed $t$, the flow
	$\phi_{t,H}$ is a symplectic map wherever it is defined.
\end{theorem}
\begin{proof}
	We refer to \citet[Theorem 2.4, p.~184]{hairer2006} 
	or to \citet[Theorem 1, p.~54]{leimkuhler_reich_2005}.
\end{proof}

Note that a Hamiltonian (ODE) system in general can be written as
\begin{align*}
	\dot{y}(t) &= S \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
	y(t_0) &= y_0
\end{align*}
with an arbitrary nondegenerate skew-symmetric matrix $S \in \mathbb{R}^{2d \times 2d}$.
However there always exists a transformation to express such a Hamiltonian ODE system in
canonical form (see \citet[Remark 3.8]{peng2016}). Thus, we restrict w.l.o.g.
to canonical systems.

Our goal is to learn the flow map $\phi_{t,H} : U \to \mathbb{R}^{2d}$ for fixed $t$ with a neural network.
To be specific, for fixed $t$, we supply the neural network with training pairs $(y_i, \phi_{t,H}(y_i))$
($y_i \in \mathbb{R}^{2d}$ and $i=1, \dots, n_{\text{train}}$) and want the neural network
to be able to predict $\phi_{t,H}(y)$ for arbitrary $y \in \mathbb{R}^{2d}$.
This leads to the idea that we embed symplecticity into the neural network itself structurally.

Symplecticity has already been successfully embedded into numerical integrators for ODEs, which
led to the development of geometric integrators, see for example \citeauthor{hairer2006}
(\cite{hairer2006}).

\section{Architecture of SympNets}

In this section we recapitulate the architecture of SympNets (Symplectic Networks) as
proposed by \citeauthor{Jin2020} in \cite{Jin2020}. We proof
symplecticity for all layer types.

First, let us briefly introduce neural network terminology we will use.
We refer to \cite{Goodfellow2016} for a comprehensive look on neural
network concepts.

\begin{definition}
	(Layer)
	A neural network layer with input dimension $n_1$ and output dimension $n_2$
	and parameters $\theta \in \R^{n_\theta}$ is a map 
	$\phi : \R^{n_1} \to \R^{n_2},\, x \mapsto \mathcal{L}(x;\theta) = \mathcal{L}(x)$.
\end{definition}

A common layer is the so-called (fully-connected) linear layer
$\phi(x) = Wx +b$ with parameters $W \in \R^{n_2 \times n_1},\, b \in \R^{n_2}$,
i.e. $\theta = (vec(W)^T, b^T)^T \in \R^{n_2 n_1 + n_2}$.

In our special case we will always have $n_1 = n_2 = 2d$.

\begin{definition}
	(Neural Network)
	A neural network $\Phi$ is a composition of one or multiple layers with
	compatible input and ouput dimensions. 
	\begin{equation*}
		\Phi(x;\Theta) = \phi_{n_L} \lb \phi_{n_L-1} \lb \cdots
		\lb \phi_2(
		\phi_1(x;\theta_1); \theta_2 ) \cdots \rb ; \theta_{n_L-1} \rb ; \theta_{n_L} \rb
	\end{equation*}
	We denote the vector of all
	layer parameters with $\Theta = (\theta_1^T, \dots, \theta_{n_L}^T)^T$.
\end{definition}

We use neural networks in the context of supervised learning. 
In supervised learning, we want to learn a function based on some training data
$\mathcal{T} = \{(x_1, y_1) \dots, (x_{n_{\text{train}}},y_{n_{\text{train}}}) \}
\subset \R^{n_x} \times \R^{n_y}$. Here, $x_i$ refers to input
and $y_i$ refers to the expected output.

If the training data $\mathcal{T}$ does not fit into memory,
we split the training data into disjoint subsets
$\{ \mathcal{B}_i \}_{i=1}^{n_{\text{b}}}$ with
$\mathcal{T} = \mathcal{B}_1 \cup \cdots \mathcal{B}_{n_{\text{b}}}$.
We call every $\mathcal{B}_i$ a mini-batch.

For a given loss function $L$, the neural network tries to minimize the loss 
function $L$ by learning locally optimal parameters $\Theta_{\text{min}}$.
Typically, the loss function $L$ is expressed as
\begin{equation*}
	L(\mathcal{B}; \Theta) = \sum_{(x_i,y_i) \in \mathcal{B}} l(\Phi(x_i; \Theta), y_i),
\end{equation*}
where $l : \R^{n_y} \times \R^{n_y} \to \R$ gives the loss for an 
individual training sample and $\mathcal{B}$ refers to a mini-batch
or to the whole training data $\mathcal{T}$.

The neural network trains a local optimum $\Theta_{\text{min}}$ via
an iterative algorithm from the gradient descent family. For example, given
initial parameters $\Theta_0$, the standard gradient descent algorithm is defined as
\begin{equation*}
	\Theta_{i+1} = \Theta_i - \gamma \grad[\Theta]{L(\mathcal{T};\Theta_{i})}
\end{equation*}
Here, $\gamma \in \R$ denotes the learning rate.
If the training data $\mathcal{T}$ is split into multiple mini-batches 
$\{ \mathcal{B}_i \}_{i=1}^{n_{\text{b}}}$, every gradient-descent step
is executed with a different mini-batch $\mathcal{B}_i$, i.e.
\begin{equation*}
	\Theta_{i+1} = \Theta_i - \gamma \grad[\Theta]{L(
		\mathcal{B}_{((i \textrm{ mod } n_{\text{b}}) + 1)};
		\Theta_{i})}
\end{equation*}
In this case, the algorithm is called stochastic gradient descent, because it estimates
the gradient of the whole training data based on smaller mini-batches.
The iteration index $i \in \N$ denotes the so-called epoch.

The algorithm for computing the gradient is called backpropagation, which
is a special case of reverse-mode automatic differentation.
Basically, the backpropagatin algorithm first computes the loss value 
in a so-called forward pass
and keeps the intermediate values in a computational graph.
Afterwards, in the so-called backward pass, the chain rule is applied programmatically
on the computational graph to compute the gradient, while reusing the intermediate values
from the forward pass.

\subsection{General architecture}

Let us recapitulate the general architecture of SympNets.
We use that the composition of symplectic maps is again symplectic. So we impose that every
layer of the neural network must be symplectic, which leads to the overall neural network
to be symplectic.

\begin{definition}
	(Unit Triangular Layer) A layer $\phi : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ 
	is called a unit triangular layer with layer transform 
	$\layertf : \mathbb{R}^d \to \mathbb{R}^d,\, p \mapsto \layertf(p)$
	and bias parameter $b \in \R^{2d}$, if $\phi$ can be expressed as
	\begin{equation*}
		\phi_\up \qpvec = \uppersympop{\layertf} + b
		= \begin{pmatrix}
			q + \layertf(p) \\
			p
		\end{pmatrix} + b \quad \text{(upper unit triangular layer)}
	\end{equation*}
	or
	\begin{equation*}
		\phi_\low \qpvec = \lowersympop{\layertf} + b
		= \begin{pmatrix}
			q \\
			\layertf(q) + p
		\end{pmatrix} + b \quad \text{(lower unit triangular layer)}
	\end{equation*}
	If we do not explicitly specify the bias parameter $b$, we assume
	that $b$ is a non-learnable constant with value $b=0$. The layer transform
	also depends on parameters $\theta \in \R^{n_\theta}$, i.e.
	$\layertf(p) = \layertf(p;\theta)$, 
	but we suppress the dependency on the parameters for shorter notation.
\end{definition}

A SympNet only consists of unit triangular layers. In order that the unit triangular layers 
$\phi_{\text{up}}$ or $\phi_{\text{low}}$ are symplectic,
we have to put additional requirements on the layer transform $\layertf$, see
\cref{jacobi_symmetric} later in this section.

This structure is a special version
of the structure proposed by \citeauthor{Deco1995} in \cite{Deco1995} 
for volume-conserving neural networks. Both SympNets and the volume-conserving neural
networks proposed by \citeauthor{Deco1995} are so-called residual neural networks:

\begin{definition}
	(Residual neural network)
	A neural network is called a residual neural network
	if it can be expressed as a composition of residual blocks
	\begin{equation*}
		R(x; \theta) = R(x) = \mathcal{F}(x; \theta) + x
	\end{equation*}
	with input $x \in \R^{n_1}$ and parameters $\theta \in \R^{n_\theta}$. 
	The map $\mathcal{F}$ represents the residual mapping.
	In other words, the neural network $\Phi$ then can be expressed as
	\begin{equation*}
		\Phi(x) = (R_n \circ \cdots \circ R_1)(x)
	\end{equation*}
	with residual blocks $R_1, \dots, R_n$.
\end{definition}

The term "residual neural network" was firstly introduced by \citeauthor{resnet2016} in
\cite{resnet2016}. A SympNet is indeed a residual neural network, because a
unit triangular layer itself forms a residual block:
\begin{equation*}
	\phi_\up \qpvec = \lb \begin{pmatrix}
		T(p) \\
		0
	\end{pmatrix} + b \rb + \qpvec
\end{equation*}

\begin{lemma}\label{jacobi_symmetric}
	An upper or lower unit triangular layer $\phi_{\text{up}}$ or $\phi_{\text{low}}$
	is symplectic if and only if the Jacobian of the layer transform
	$\layertf \in C(\mathbb{R}^d, \R^d)$ 
	is everywhere symmetric.
\end{lemma}
\begin{proof}
	We show the result for $\phi_{\text{up}}$ only. 
	The proof is analogous for $\phi_{\text{low}}$.

	\begin{equation*}
		\jac{\phi_{\text{up}}}{(q,p)} = \begin{pmatrix}
			I & \jac{\layertf}{p} \\
			0 & I
		\end{pmatrix}
	\end{equation*}
	It follows
	\begin{align*}
		\lb \jac{\phi_{\text{up}}}{(q,p)} \rb^T J \lb \jac{\phi_{\text{up}}}{(q,p)} \rb
		&= \lb \jac{\phi_{\text{up}}}{(q,p)} \rb^T \begin{pmatrix}
			0 & I \\
			-I & -\jac{\layertf}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			I & 0 \\
			\lb \jac{\layertf}{p} \rb^T & Id
		\end{pmatrix} \begin{pmatrix}
			0 & I \\
			-I & -\jac{\layertf}{p}
		\end{pmatrix} \\
		&= \begin{pmatrix}
			0 & I \\
			-I & \lb \jac{\layertf}{p} \rb ^T-\jac{\layertf}{p}
		\end{pmatrix} \overset{!}{=} J
	\end{align*}
	Thus, $\phi_{\text{up}}$ is symplectic if and only if
	$\lb \jac{\layertf}{p} \rb ^T-\jac{\layertf}{p}=0$, 
	i.e. if and only if the Jacobian $\jac{\layertf}{p}$ is everywhere symmetric.
\end{proof}

The next corollary is useful to construct symplectic unit triangular layers.
\begin{corollary}\label{gradient_corollary}
	Let $V: \mathbb{R}^d \to \mathbb{R}, \; p \mapsto V(p)$ be a function in 
	$C^2(\R^d)$. Then the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = \grad{V}(p)
	\end{equation*}
	are symplectic. We call $V$ a potential.
\end{corollary}
\begin{proof}
	The Jacobian matrix of $\grad{V}$ corresponds to the Hessian matrix of $V$,
	i.e. $\jac{(\grad{V})}{p} = HV$. \todo{Notation for Hessian}
	The Hessian is everywhere symmetric due to $V \in C^2(\R^d)$ (Lemma of Schwarz),
	thus the result follows with \cref{jacobi_symmetric}.
\end{proof}

\subsection{Linear layers}

\begin{definition}\label{def_linear_layer}
	(Linear layers)
	Given a symmetric matrix $S \in \R^{d \times d}$ and bias $b \in \R^{2d}$,
	we call the upper and lower unit triangular layers
	with layer transform $\layertf(p) = Sp$ the (symplectic) upper and lower 
	linear layers $\ell_\up$ and $\ell_\low$.
\end{definition}

The linear layers can be expressed with matrix-vector multiplication, for example
\begin{equation*}
	\ell_\up \qpvec = \begin{pmatrix}
		I & S \\
		0 & I
	\end{pmatrix} \qpvec + b
\end{equation*}

The matrix $S$ and the bias $b$ are learnable parameters.
In practice, we parametrize the symmetric matrix $S\in \mathbb{R}^{d \times d}$
with $S = A^T + A$ via another arbitrary matrix $A\in \mathbb{R}^{d \times d}$, as
most optimization methods used for learning neural networks are designed for
unconstrained optimization problems.

The linear layers are symplectic, because the Jacobian of the layer transform $\jac{\layertf}{p} = S$
is a symmetric matrix by defintion. Alternatively, we can apply \cref{gradient_corollary} by
choosing the potential $V(p) := p^TAp$.

\begin{proof}
	For $k=1, \dots, d$ we have
	\begin{align*}
		\lb \grad{V}(p) \rb_k &= \deldel{p_k} \lb \sum_{i,j=1}^d A_{ij} p_i p_j \rb
		= \sum_{i,j=1}^d A_{ij} \deldel{p_k}(p_i) p_j + \sum_{i,j=1}^d A_{ij} p_i \deldel{p_k}(p_j) \\
		&= \sum_{j=1}^d A_{kj} p_j + \sum_{i=1}^d A_{ik} p_i = ((A+A^T)p)_k \\
	\end{align*}
	Choose $A=\frac{1}{2}S \implies A+A^T=S$.
	Thus, symplecticity follows with \cref{gradient_corollary}.
\end{proof}

We may enhance expressivity of a linear layer by alternately composing multiple
$\ell_{up}$ and $\ell_{low}$. We define 
$\mathcal{L}^{n}_{up},\, \mathcal{L}^{n}_{low} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$ with

\begin{align*}
	\mathcal{L}^{n}_{up} \qpvec &:= \begin{pmatrix}
		I && 0 / S_n \\
		S_n / 0 && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && 0 \\
		S_2 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && S_1 \\
		0 && I
	\end{pmatrix} + b \\
	\mathcal{L}^{n}_{low} \qpvec &:= \begin{pmatrix}
		I && S_n / 0 \\
		0 / S_n && I
	\end{pmatrix}
	\cdots
	\begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix}
	\begin{pmatrix}
		I && 0 \\
		S_1 && I
	\end{pmatrix} + b
\end{align*}

$\mathcal{L}^{n}_{up}$ and $\mathcal{L}^{n}_{low}$ are again symplectic because they 
are a composition of the symplectic maps $\ell_{up}$ and $\ell_{low}$.

\citeauthor{jin2020unit} show in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map. In other words, 
the set of all possible $\mathcal{L}^{9}_{up}$ is equal to the set of all symplectic linear maps.

It does not make sense to put two upper linear layers or two lower linear layers after each other,
because this reduces to a single upper or lower linear layer:

\begin{proof}
	We show the statement for two upper linear layers $\ell_{up,2}$ and $\ell_{up,1}$.
	\begin{align*}
		\lb \ell_{up,2} \circ \ell_{up,1} \rb \qpvec &=
		\begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix}
		\lb
		\begin{pmatrix}
			I && S_1 \\
			0 && I
		\end{pmatrix}
		\qpvec + b_1
		\rb + b_2 \\
		&= \begin{pmatrix}
			I && S_1 + S_2 \\
			0 && I
		\end{pmatrix} \qpvec
		+ \begin{pmatrix}
			I && S_2 \\
			0 && I
		\end{pmatrix} b_1
		+ b_2 \\
		&= \begin{pmatrix}
			I && S \\
			0 && I
		\end{pmatrix} \qpvec + b
	\end{align*}
	with $S := S_1 + S_2$ and $b := \begin{pmatrix}
		I && S_2 \\
		0 && I
	\end{pmatrix} b_1
	+ b_2$.
\end{proof}

\subsection{Activation layers}

\begin{definition}
	(Activation layers)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \activation(p_i) \rb_{i=1}^d
	\end{equation*}
	the upper and lower activation layers $\mathcal{N}_\up$ and $\mathcal{N}_\low$.
\end{definition}
The coefficients $a \in \mathbb{R}^d$ are learnable parameters.

\begin{corollary}
	Given an activation function $\activation \in C^1(\R)$,
	the activation layers $\mathcal{N}_{up}$ and $\mathcal{N}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be an antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{k=1}^d a_k \mathcal{A}(p_k)
	\end{equation*}

	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $k=1, \dots d$
	\begin{equation*}
		\lb \grad{V}(p) \rb_i = \deldel{p_i} \lb \sum_{k=1}^d a_k \mathcal{A}(p_k) \rb
		= a_i \activation(p_i) = \lb \layertf(p) \rb_i
	\end{equation*}
	Symplecticity follows with \cref{gradient_corollary}.
\end{proof}

\subsection{Gradient layers}

\begin{definition}
	(Gradient layers)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \activation \lb (Kp)_j + c_j \rb \bigg)_{j=1}^n
	\end{equation*}
	the upper and lower gradient layers $\mathcal{G}_\up$ and $\mathcal{G}_\low$.
\end{definition}
$K \in \mathbb{R}^{n \times d}$ and $a,c \in \mathbb{R}^n$
are learnable parameters. 
$n \in \mathbb{N}$ denotes the width of the gradient layer and can be chosen freely.
In practice, we choose $n >> d$ for large expressivity.

\begin{corollary}
	Given an activation function $\sigma \in C^1$ the gradient layers $\mathcal{G}_{up}$
	and $\mathcal{G}_{low}$ are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. We define the potential
	\begin{equation*}
		V(p) := \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + c_j)
	\end{equation*}
	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $i=1, \dots, d$
	\begin{align*}
		(\grad{V(p)})_i &= \deldel{p_i} \lb \sum_{j=1}^n a_j \mathcal{A}((Kp)_j + c_j) \rb
		= \sum_{j=1}^n a_j \activation((Kp)_j + c_j) 
		\underbrace{\deldel{p_i} \lb (Kp)_j \rb}_{=K_{ji} = K^T_{ij}}
		= (\layertf(p))_i
	\end{align*}
	With \cref{gradient_corollary} follows that the Gradient layers
	$\mathcal{G}_{up}$ and $\mathcal{G}_{low}$ are symplectic.
\end{proof}

Note that activation layers are a subset of gradient layers. We can see this by choosing
$n=d$, $K=I_d$ and $c=0$.

\section{Univeral approximation theorems}

In \cite{Jin2020}, \citeauthor{Jin2020} show approximation theorems for SympNets.
We repeat the statements here and refer to \cite{Jin2020} for proofs.

\begin{definition}
	($r$-finite)
	Let $r \in \N_0$ be given. The function $\sigma$ is r-finite if $\sigma \in C^r(\R)$
	and $0 < \int \abs{\dd{x} \sigma} d\lambda < \infty$. $\lambda$ is the Lebesgue measure on $\R$.
\end{definition}

\begin{definition}
	($r$-uniformly dense on compacta)
	Let $m,n \in \N, \, r \in \N_0$ be given, $U \subset \R^m$ an open set,
	and $S_1 \subset C^r(U,\R^n)$. Then we say $S_2$ is $r$-uniformly dense
	on compacta in $S_1$ if $S_2 \subset S_1$ and for all $f \in S_1$, compact
	$W \subset U$ and every $\epsilon > 0$, there exists $g \in S_2$ such that
	$\norm{f-g}_{C^r(W,\R^n)} < \epsilon$.
\end{definition}

\begin{definition}\label{def_la_sympnet}
	(LA-SympNet)
\end{definition}

\begin{definition}\label{def_g_sympnet}
	(G-SympNet)
\end{definition}

\begin{theorem}
	(Approximation theorem for LA-SympNets)
\end{theorem}

\begin{theorem}
	(Approximation theorem for G-SympNets)
\end{theorem}

The approximation theorems do not give any information
about the size of the SympNet, or how long it would take to learn a SympNet.
\todo{They simply states that there exists}

\todo{sigmoid, tanh and elu are r-finite}

\section{Convolution and Normalization for SympNets}

In this section we introduce new extensions to SympNets. In particular, we bring the concept
of convolution to SympNets and we propose a possibility how to embed normalization into SympNets while
maintaining symplecticity.

\subsection{Introduction to Convolution}

\todo{Motivate convolution layers (weight sharing, sparse-connectivity)}

We introduce convolution in a rather abstract way in order to simplify the following proofs.
General convolution is defined on infinite-dimensional function spaces.
Neural networks implement a finite-dimensional version, as the operation has to be
computable.

Given two discrete functions $f,g \in \mathbb{R}^\mathbb{Z}$ convolution is defined as 
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau - a)
\end{equation*}
However, most popular neural network libraries actually do not implement real convolution
in convolution layers. Instead, they implement the so called cross-correlation, but call it convolution. 
Cross-correlation is a flipped convolution and is defined as
\begin{equation*}
	*: \mathbb{R}^{\mathbb{Z}} \times \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (f*g)(\tau) = \sum^{\infty}_{a=-\infty} f(a) g(\tau + a)
\end{equation*}
Because most popular neural network libraries implement cross correlation and call it convolution,
we stick to the same convention and mean cross-correlation if we write $*$.
Still, the choice is arbitrary, as the following statements could be constructed analogously with convolution.

We work with finite-dimensional vectors in neural networks. Therefore, we define a bijective mapping between
vectors in $\mathbb{R}^n$ and functions in $\mathbb{R}^{\mathbb{Z}}$ 
via zero-continuation on $\mathbb{Z}$:
\begin{align*}
	\mathcal{I}_n &: \mathbb{R}^n \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb\mathcal{I}_n(x)\rb(\tau) := \sum_{a=1}^{n} x_a \delta_{\tau a} \\
	%
	\mathcal{I}_n^{-1} &: \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^n,
	\quad \mathcal{I}_n^{-1}(f) := \lb f(\tau) \rb_{\tau=1}^n
\end{align*}

Additionially, we define a shift operator $\mathcal{S}_\zeta$, 
which shifts a discrete function $f \in \mathbb{R}^\mathbb{Z}$ by $\zeta \in \mathbb{Z}$ positions
in the right direction
\begin{equation*}
	\mathcal{S}_\zeta : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad \lb \mathcal{S}_\zeta(f) \rb (\tau) := f(\tau - \zeta)
\end{equation*}

\begin{definition}\label{def_cross_corr}
	(Valid cross-correlation)
	We define the valid cross-correlation of a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$ and 
	a finite-dimensional input $x \in \mathbb{R}^{n_x}$ with $n_x \geq n_k$ as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_x-n_k+1},
		\quad *_{\text{v}}(k,x) := \mathcal{I}_{n_x-n_k+1}^{-1} (
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}
In other words, the valid cross-correlation $*_{\text{v}}$ evaluates the cross-correlation 
only at positions where the finite-dimensional kernel $k$ and the 
finite-dimensional input $x$ fully overlap and puts the result into a finite-dimensional vector.

\todo{Add illustration}

The kernel $k$ will be a parameter which the neural network should learn.
Note that in this context it does not make a difference if we use convolution or cross-correlation, because
the neural network would just learn a flipped version of the kernel $k$ in the opposite case.

We define a stride operator $\mathcal{D}_s$ for a stride number $s \in \mathbb{N}$ with
\begin{equation*}
	\mathcal{D}_s : \mathbb{R}^{\mathbb{Z}} \to \mathbb{R}^{\mathbb{Z}},
	\quad (\mathcal{D}_s)(\tau) := f(s \tau)
\end{equation*}
The stride operators skips $s$ entries of $f \in \mathbb{R}^\mathbb{Z}$.

\begin{definition}\label{def_cross_corr_stride}
	(Valid cross-correlation with stride)
	Given a finite-dimensional kernel $k \in \mathbb{R}^{n_k}$, a finite-dimensional
	input $x \in \mathbb{R}^{n_x}$, stride number $s \in \mathbb{N}$ and output dimension
	\begin{equation*}
		n_{\text{out}} = \left\lfloor \frac{n_x - n_k}{s} \right\rfloor + 1
	\end{equation*}
	the valid-cross correlation with stride is defined as
	\begin{equation*}
		*_{\text{v}} : \mathbb{R}^{n_k} \times \mathbb{R}^{n_x} \to \mathbb{R}^{n_{\text{out}}},
		\quad *_{\text{v}}(k,x) := 
		(\mathcal{I}_{n_{\text{out}}}^{-1} \circ D_s)
		(
			\mathcal{S}_{-1}( \mathcal{I}_{n_k}(k)) * \mathcal{I}_{n_x}(x)
		)
	\end{equation*}
\end{definition}

\todo{Add illustration}

Note that \cref{def_cross_corr_stride} equals \cref{def_cross_corr} for stride number $s=1$.

\subsection{Convolution layers}

The valid cross correlation (without stride) has the output dimension $n_x - n_k + 1$.
Thus, the valid cross-correlation $*_{\text{v}}$ decreases the dimension of the input $x$.
However, we want to incorporate the valid cross-correlation as the layer transform $\layertf$ of an
unit triangular layer. The layer transform $\layertf$ must keep the dimension $d$.
Therefore we apply a so-called padding operation on the layer transform input $p \in \R^d$ 
before passing it to the valid
cross-correlation. The padding operation increases the dimension of $p$ in order
that the composition of padding and cross-correlation keeps the overall dimension.
Let $c \in \mathbb{N}$ denote the number by which the padding operator increases 
the input dimension, such that dimension is $n_x = d+c$. 
In order that the composition of padding and cross-correlation keeps the dimension, it must hold
\begin{equation*}
	n_x - n_k + 1 = \underbrace{(d+c)}_{\substack{
		\text{increased dim.} \\
		\text{by } c \text{ via padding}
	}} - \, n_k + 1 = d \iff c = n_k-1
\end{equation*}
For symmetry reasons, we pad the same number of values at the beginning and 
the end of the input vector $p$. This means that the number of padded values $c$ has to be even,
i.e. we can write $c=2m$ for a $m \in \mathbb{N}_0$.
Consequently, the equation above implies that the kernel size $n_k$ has to be odd, i.e. 
$n_k = 2m+1$. We define two different padding operators,
which both increase the input vector dimension by $2m$.

\begin{definition}
	(Constant padding)
	Given the padding values $l,r \in \mathbb{R}^m$, we define constant padding 
	$c_{\text{pad},m}$ as
	\begin{equation*}
		c_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad c_{\text{pad},m}(p) = c_{\text{pad},m}(p;l,r) := \lb l^T, p^T, r^T \rb^T
	\end{equation*}
\end{definition}
The constant padding $c_{\text{pad},m}$ is an affine linear map in $p$, because we can write
\begin{equation}\label{cpad_affine}
	c_{\text{pad},m}(p;l,r) = c_{\text{pad}}(p;0_m,0_m)
	+ c_{\text{pad},m}(0_d;l,r)
\end{equation}
and $c_{\text{pad},m}(p;0_m,0_m)$ is linear regarding $p$.

\begin{definition}
	(Symmetric padding)
	We define symmetric padding as
	\begin{equation*}
		s_{\text{pad},m} : \mathbb{R}^d \to \mathbb{R}^{d+2m},
		\quad s_{\text{pad},m}(p) := (
			\underbrace{p_m, p_{m-1} \dots, p_1}_{m \text{ values}}, \,
			\underbrace{p_1, p_2 \dots, p_d}_{d \text{ values}}, \,
			\underbrace{p_d, p_{d-1} \dots, p_{d-m+1}}_{m \text{ values}}
		)^T
	\end{equation*}
\end{definition}
The symmetric padding $s_{\text{pad},m}$ is a linear map. We call the operation symmetric padding,
because the outermost left and right inner vector entries are mirrored at the beginning and the end
of the vector.

Given an odd kernel $k \in \mathbb{R}^{2m+1}$, we set
\begin{equation*}
	\hat{k} = \hat{k}(k) := \mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}
Then $\hat{k}(-m) = k_1, \, \dots,\, \hat{k}(m) = k_{2m+1}$ and
$\hat{k}(\tau)=0$ for $\abs{\tau} > m$. With $\hat{k}$, we can express the valid cross-correlation as
\begin{equation*}
	*_{\text{v}}(k,x) = \mathcal{I}_{n_x-2m}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \mathcal{I}_{n_x}(x)
	)
\end{equation*}
because
\begin{equation*}
	\mathcal{S}_{m}(\hat{k}) 
	= \mathcal{S}_{m}(\mathcal{S}_{-m-1}(\mathcal{I}_{2m+1}(k)))
	= \mathcal{S}_{m+(-m-1)}(\mathcal{I}_{2m+1}(k))
	= \mathcal{S}_{-1}(\mathcal{I}_{2m+1}(k))
\end{equation*}

\begin{definition}
	(Symmetric kernel)
	We call an odd kernel $k \in \mathbb{R}^{2m+1}$ symmetric if
	\begin{equation*}
		\hat{k}(\tau) = \hat{k}(-\tau)
	\end{equation*}
	for all $\tau \in \mathbb{Z}$.
\end{definition}

Let us now define symplectic convolution layers by combining padding and valid 
cross-convolution.
\begin{definition}\label{def_conv_layer}
	(Convolution layers)
	Given a symmetric kernel $k \in \mathbb{R}^{2m+1}$, padding
	$\chi_{\text{pad},m} = c_{\text{pad},m}$ or $\chi_{\text{pad},m} = s_{\text{pad},m}$,
	we call the upper and lower unit triangular layers with bias $b \in \R^{2d}$ and layer transform
	\begin{equation*}
		\layertf(p) := *_{\text{v}}(k,\chi_{\text{pad},m}(p))
	\end{equation*}
	the (symplectic) convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$.
\end{definition}
The symmetric kernel $k \in \mathbb{R}^{2m+1}$ is a learnable parameter.
If $\chi_{\text{pad},m} = c_{\text{pad},m}$, the padding values $l,r \in \R^m$ can either
be learnable parameters or constant. The convolution layers can be used without any bias ($b=0$), a
constant non-learnable bias ($b \neq 0 =$ const), or with a learnable bias parameter 
($b$ is a learnable parameter). We stick to $b=0$ in our experiments.

With $\hat{\chi}_{\text{pad},m}(p) := \mathcal{I}_{d+2m}(\chi_{\text{pad},m}(p))$,
we can write the layer transform as
\begin{equation}\label{eq_conv_layer_transform}
	\layertf(p) =
	\mathcal{I}_{d}^{-1} (
		\mathcal{S}_{m}(\hat{k}) * \hat{\chi}_{\text{pad},m}(p)
	)
\end{equation}

\begin{lemma}\label{jac_linear_map}
	Let $f: \mathbb{R}^d \to \mathbb{R}^d$ be a linear map. Then the Jacobian matrix
	$\jac{f}{x}$ is constant, i.e.
	\begin{equation*}
		\jac{f}{x} \bigg|_{x = v} = \jac{f}{x} \bigg|_{x = w} \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
	and for the Jacobian-vector product holds
	\begin{equation*}
		\lb \jac{f}{x} \bigg|_{x = v} \rb w = f(w) \quad \forall v,w \in \mathbb{R}^d
	\end{equation*}
\end{lemma}
\begin{proof}
	$f$ is linear $\implies$ There exists a matrix representation $f(x) = Ax$ with
	$A \in \mathbb{R}^{d \times d}$ \\
	$\implies \jac{f}{x} \big|_{x=v} = A = \text{const.} \quad \forall v \in \mathbb{R}^d$
	$\implies \lb \jac{f}{x} \big|_{x=v} \rb w = Aw = f(w) \quad \forall v,w \in \mathbb{R}^d$
\end{proof}
As the Jacobian matrix for a linear map is constant, we omit the evaluation point, i.e.
$\jac{f}{x} \big|_{x=v} = \jac{f}{x}$.

\begin{theorem}\label{thm_conv_const_pad_symplectic}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $\chi_{\text{pad},m} = c_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	We have to show that the Jacobian of the layer transform $\layertf(p)$
	is symmetric (\cref{jacobi_symmetric}).
	Because of \cref{cpad_affine}, it suffices to show the case $l,r=0$ (the Jacobian
	of a constant has only zero-valued entries). For $l,r=0$, the layer transform 
	$\layertf(p)$ 
	is a linear map, because it is a composition of linear maps only.

	Define the bilinear form
	\begin{equation}\label{eq_bilinear_proof_conv}
		b : \mathbb{R}^d \times \mathbb{R}^d \to \mathbb{R},
		\quad b(v,w) := \ip{v}{\lb \jac{\layertf}{p} \rb w}
	\end{equation}
	To show symmetry of the Jacobian, we show that 
	$b(e_i, e_j) = b(e_j, e_i)$ for all $i,j=1,\dots,d$.
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{\lb \jac{\layertf}{p} \rb e_j}
		= \ip{e_i}{\activation_{\mathcal{C}}(e_j)} \quad \text{(\cref{jac_linear_map})} \\
		&\stackrel{\cref{eq_conv_layer_transform}}{=} \ip{e_i}{\mathcal{I}_{d}^{-1} (
			\mathcal{S}_{m}(\hat{k}) * \hat{c}_{\text{pad},m}(e_j)
		)} \\
		&= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				\underbrace{(\hat{c}_{\text{pad},m}(e_j))(\tau+a)}_{
					= \delta_{(\tau+a) (j+m)}
				}
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((j-i+m)-m) = \hat{k}(j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\begin{theorem}
	The convolution layers $\mathcal{C}_{up}$ and $\mathcal{C}_{low}$
	with padding $\chi_{\text{pad},m} = s_{\text{pad},m}$ are symplectic.
\end{theorem}
\begin{proof}
	With $\chi_{\text{pad},m} = s_{\text{pad},m}$, the layer transform 
	$\layertf$ is linear, because it is a composition of linear maps. Thus,
	we proof the statement the same way as \cref{thm_conv_const_pad_symplectic} above.
	For $\tau,a \in \mathbb{Z}$ we have
	\begin{equation*}
		\hat{s}_{\text{pad},m}(e_j)(\tau + a) = 
		\delta_{(\tau + a) (m-j+1)} + \delta_{(\tau + a) (j+m)} + \delta_{(\tau + a) (2d+m-j+1)}
	\end{equation*}
	Consequently, for $\chi_{\text{pad},m} = s_{\text{pad},m}$ and $i,j=1, \dots, d$, 
	the bilinear form \cref{eq_bilinear_proof_conv} becomes
	\begin{align*}
		b(e_i, e_j) &= \ip{e_i}{
			\lb \sum_{a=-\infty}^{\infty} 
				\hat{k}(a-m)
				(\hat{s}_{\text{pad},m}(e_j))(\tau+a)
			\rb_{\tau=1}^d
		} \\
		&= \hat{k}((m-j+1-i)-m) + \hat{k}((j+m-i)-m) + \hat{k}((2d+m-j+1-i)-m) \\
		&= \hat{k}(1-j-i) + \hat{k}(j-i) + \hat{k}(1+2d-j-i)
	\end{align*}
	The kernel $k$ is symmetric, thus $\hat{k}(j-i) = \hat{k}(i-j) \implies b(e_i, e_j) = b(e_j,e_i)$.
\end{proof}

\subsubsection*{Parametrization of a symmetric kernel}

Let $\{b_1, b_2, \dots b_m \} \subset \mathbb{R}^{2m+1}$ be a basis of the space 
$\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.
The symmetric kernel $k$ is then parametrized by the coefficients $\beta \in \mathbb{R}^m$ via
\begin{equation*}
	k = (b_1, b_2, \dots, b_m) \beta = \beta_1 b_1 + \beta_2 b_2 + \dots + \beta_m b_m
\end{equation*}

One choice is the canonical basis with basis vectors $b_1, \dots, b_m \in \mathbb{R}^{2m+1}$ given by
\begin{equation*}
	(b_i)_{j+m+1} = \hat{k}(b_i)(j) = \begin{cases}
		1 &: \abs{j} = i-1 \\
		0 &:else
	\end{cases} 
	\quad (j=-m, \dots, m)
\end{equation*}

Another possible basis choice inspired by finite differences is
\todo{Cite paper with similar idea}
\begin{align*}
	b_1^{FD} = (0, \dots, 0,& 1,0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_2^{FD} = (0, \dots, 0, 1,-&2,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1},\\
	b_3^{FD} = (0, 0, \dots, 0, 1,-4,& 6,-4,1, 0, \dots, 0)^T \in \mathbb{R}^{2m+1} \\
	&\vdots
\end{align*}
In general, the entries for a basis vector $b_i^{FD} \in \mathbb{R}^{2m+1}$ $(1 \leq i \leq m)$ 
originate from Pascal's triangle.
\begin{equation*}
	\lb b_i^{FD} \rb_{j+m+1} = \hat{k}(b_i^{FD})(j) = \begin{dcases}
		(-1)^j \binom{2(i-1)}{j+i-1} &: \abs{j} < i \\
		0 &: \text{else}
	\end{dcases}
	\quad (j=-m, \dots, m)
\end{equation*}

The symmetry of the kernel basis vectors $b_i^{FD}$ follows with the definition of the binomial coefficient.
\begin{align*}
	\binom{2(i-1)}{j+i-1} &= \frac{2(i-1)!}{(j+i-1)!(2(i-1)-(j+i-1))!} \\
	&= \frac{2(i-1)!}{(j+i-1)!(i-j-1)!}
\end{align*}
Thus we have
\begin{equation*}
	\binom{2(i-1)}{(-j)+i-1} = \binom{2(i-1)}{j+i-1}
\end{equation*}
and for $i=1, \dots, m$ and $j= 0, \dots, m$
\begin{align*}
	\hat{k}(b_i^{FD})(-j) &= \begin{dcases}
		(-1)^{-j} \binom{2(i-1)}{(-j)+i-1} : \abs{-j} < i \\
		0 : else
	\end{dcases} \\
	&= \begin{dcases}
		(-1)^{j} \binom{2(i-1)}{j+i-1} : \abs{j} < i \\
		0 : else
	\end{dcases} \\
	&= \hat{k}(b_i^{FD})(j) 
\end{align*}
So $\{ b_i^{FD} \}_{i=1}^m$ is a valid basis of
the space $\{ k \in \mathbb{R}^{2m+1} : k \text{ is a symmetric kernel} \}$.

In our numerical experiments, it turns out that
parametrization plays an important role how well a neural network learns.

\subsection{Convolution Gradient Layers}\label{sec_conv_gradient_layer}

For a fixed kernel $k$, valid cross-correlation is a linear map regarding the second argument. 
Therefore there exists a representation matrix $A_{k}$ with 
$*_{\text{v}}(k,p) = A_{k}p$. Transposed valid cross-correlation 
is defined as the linear map associated with the transpose $A_{k}^T$ of $A_{k}$.
We denote the transposed valid cross-correlation by $*_{\text{v}}^T$.

Again, most popular neural network libraries say convolution, but actually implement
valid cross-correlation. In this case, transposed convolution actually refers to
transposed valid cross-correlation.

For large $d \in \mathbb{N}$, it can make sense to use convolution inside Gradient layers 
instead of a full matrix $K$. Let us repeat the layer transfrom $\layertf$ for Gradient layers.

\begin{equation*}
	\layertf(p) = K^T \bigg( a_j \activation \lb (Kp)_j + c_j \rb \bigg)_{j=1}^n
\end{equation*}

A gradient layer with width $n \in \mathbb{N}$ can be implemented by 
using valid cross-correlation (with stride) for $K^T \in \mathbb{R}^{d \times n}$ and the
corresponding transposed valid cross-correlation (with stride) for $K \in \mathbb{R}^{n \times d}$.
The choice of transpose is intentional, because we want to upscale the input $p \in \mathbb{R}^d$ ($n >> d$). 
Cross-correlation without padding decreases the dimension. Therefore we first apply 
transposed cross-correlation on $p$, because the transpose increases the dimension.
A large kernel size $n_k$ and a large stride number $s$ allow for significant upscaling.
We emphasize that the kernel $k$ has not to be symmetric in this setting. 
It is reasonable that the parametrization for $a \in \R^n$ and $c \in \R^n$ also respects
the convolution in a certain way.
One possibility to parametrize $a \in \R^n$ with a parameter $\tilde{a} \in \R^{n_k}$ is
\begin{equation*}
	c = *_{\text{v}}^T(\tilde{c},1_d)
	.
\end{equation*}
The vector $c \in \R^n$ can be parametrized with a parameter $\tilde{c} \in \R^{n_k}$ analogously.
To summarize, the convolution gradient layers then can be defined as:
\begin{definition}
	(Convolution gradient layers)
	Given a kernel size $n_k \in \N$, a stride number $s \in \N$,
	kernel parameters $k,\tilde{a},\tilde{c} \in \mathbb{R}^{n_k}$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{align*}
		T(p) = *_{\text{v}} \lb k, \, \lb a_j \activation \lb \lb *_{\text{v}}^T(k,p) \rb_j + c_j \rb \rb_{j=1}^n \rb 
		\quad
		\text{with } c = *_{\text{v}}^T(\tilde{c},1_d) 
		\text{ and } a = *_{\text{v}}^T(\tilde{a},1_d)
	\end{align*}
	the upper and lower gradient convolution layers 
	$\mathcal{G}_\up^{\text{conv}}$ and $\mathcal{G}_\low^{\text{conv}}$.
	The width $n \in \N$ corresponds to the output dimension of the transposed valid cross-correlation $*_{\text{v}}^T$
	and thus depends on the kernel size $n_k$ and the stride number $s$.
\end{definition}

The convolution gradient layers are symplectic by construction,
because they are a special parametrization of gradient layers.

\todo{Cite similar idea}

\subsection{Normalization}

Batch normalization is a standard method to accelerate training initially proposed by
\citeauthor{batchnorm-ioffe15} in \cite{batchnorm-ioffe15}. 
We introduce a possibility to incorporate batch normalization
into activation and gradient layers while maintaining symplecticity.
\todo{Cite similar idea}

\todo{Sigmoid activation function saturates for large values. Vanishing gradient.
Therefore normalize input before applying activation function.}

Given a mini-batch input $\mathcal{B} = \{  x_1, x_2, \dots, x_{n_{\mathcal{B}}} \} \subset \mathbb{R}^{n}$ 
with size $n_{\mathcal{B}}$, we define the batch normalization transformation as
\begin{equation*}
	\eta_{\gamma, \beta} : \mathbb{R}^n \to \mathbb{R}^n,\quad
	\eta_{\gamma, \beta}(x) 
	:= \lb \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} 
	\lb x_j - \lb \mu_\mathcal{B} \rb_j \rb + \beta_j \rb_{j=1}^n
\end{equation*}
where $n$ is the input dimension of the input $x \in \mathbb{R}^n$ and $\gamma, \beta \in \mathbb{R}^{n}$ 
are learnable parameters.
The scalar $\epsilon \in \mathbb{R}$ is a small positive value to avoid division by zero.
$\sigma^2_\mathcal{B} \in \mathbb{R}^{n}$ refers to the mini-batch variance and
$\mu_\mathcal{B} \in \mathbb{R}^{n}$ refers to the mini-batch mean. The batch normalization results 
in zero mean and unit variance of the whole mini-batch input.

The learnable parameters $\gamma, \beta \in \mathbb{R}^{n}$ allow the neural network to modify
the normalization during training if necessary. The batch normalization transform is able to 
represent the identity transform by setting appropriate $\gamma, \beta$.

The mean $\mu_\mathcal{B} \in \R^n$ and variance $\sigma^2_\mathcal{B} \in \R^n$ are estimated by
\begin{align*}
	\mu_\mathcal{B} &= \frac{1}{n_{\mathcal{B}}} \sum_{i=1}^{n_{\mathcal{B}}} x_i \\
	\sigma^2_\mathcal{B} &= \frac{1}{n_{\mathcal{B}}} 
	\sum_{i=1}^{n_{\mathcal{B}}} (x_i - \mu_\mathcal{B})^2
\end{align*}

During training the mini-batch variance $\sigma^2_\mathcal{B} \in \mathbb{R}^{n}$ and
the mini-batch $\mu_\mathcal{B} \in \mathbb{R}^{n}$ are continously updated in the forward pass.
To be precise, for a new mini-batch input
$\mathcal{B} = \{ x_1, x_2, \dots, x_{n_{\mathcal{B}}} \} \subset \R^n$, 
the mean $\mu_\mathcal{B}$ and variance $\sigma^2_\mathcal{B}$
are updated based on $\mathcal{B}$, before $\eta_{\gamma, \beta}(x_k)$ for
$x_k \in \mathcal{B}$ is evaluated (see Algorithm 1).
When training has finished the neural network does not update $\sigma_\mathcal{B}^2$ 
and $\mu_\mathcal{B}$ anymore. Instead, the neural network remembers the last value from training.

Note that the batch normalization transformation may be incorporated into a layer
deep inside a neural network. If this is the case, the mini-batch input $\mathcal{B}$ 
refers to the collective output of the previous layer.

\begin{algorithm}\label{algo_batch_norm}
	\caption{Batch normalization transform}
	\textbf{Input:} mini batch $\mathcal{B} = \{x_1, x_2, \dots, x_{n_\mathcal{B}}\} \subset \R^n$ 
	and $x_k \in \mathcal{B}$ \\
	\textbf{Output:} $\eta_{\gamma, \beta}(x_k) \in \R^n$
	\setstretch{1.5}
	\begin{algorithmic}
		\If{training\_mode} \Comment{Update mean and variane if training, otherwise
		keep previous values}
			\State $\mu_\mathcal{B} \gets \frac{1}{n_\mathcal{B}} \sum_{i=1}^{n_\mathcal{B}} x_i$
			\State $\sigma^2_\mathcal{B} \gets \frac{1}{n_\mathcal{B}} \sum_{i=1}^{n_\mathcal{B}} 
			(x_i - \mu_\mathcal{B})^2$
		\EndIf
		\State \Return 
		$\lb \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} 
		\lb (x_k)_j - \lb \mu_\mathcal{B} \rb_j \rb + \beta_j \rb_{j=1}^n$
	\end{algorithmic}
\end{algorithm}

The batch normalization transform can be implemented in a straightforward way with modern
neural network libraries, as during training the Jacobian $\jac{\eta_{\gamma, \beta}}{(\gamma, \beta)}$ 
is obtained via automatic differentiation.

If the training data is small enough, so that splitting the data into multiple mini batches
is not necessary, the mean and variance are calculated for the whole training data set. This is the
case for our numerical experiments, as we work with very small training data sets.

\subsubsection{Normalized gradient layers}

We add normalization to gradient layers.

\begin{definition}\label{def_norm_gradient_layers_1}
	(Normalized gradient layers - Variant 1)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \activation
		\lb \overline{p}_j \rb \bigg)_{j=1}^n
		\quad \text{with } \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	the upper and lower normalized gradient layers 
	$\mathcal{G}_\up^{\eta}$ and $\mathcal{G}_\low^{\eta}$ (variant 1).
\end{definition}

\begin{definition}\label{def_norm_gradient_layers_2}
	(Normalized gradient layers - Variant 2)
	Given width $n \in \N$, $K \in \mathbb{R}^{n \times d}$, $a,c \in \mathbb{R}^n$ and
	an activation function $\activation : \R \to \R$,
	we call the upper and lower triangular layers with layer transform
	\begin{equation*}
		\layertf(p) = K^T \bigg( a_j \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon}
		\activation \lb \overline{p}_j \rb \bigg)_{j=1}^n
		\quad \text{with } \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	the upper and lower normalized gradient layers 
	$\mathcal{G}_\up^{\eta}$ and $\mathcal{G}_\low^{\eta}$ (variant 2).
	The variance $\sigma^2_\mathcal{B} \in \R^n$ refers to the mini-batch variance of $Kp+c$.
\end{definition}

$K \in \mathbb{R}^{n \times d}$ and $a,c, \gamma, \beta \in \mathbb{R}^n$
are learnable parameters.

\todo{Discuss variant 1 vs. variant 2 (numerical experiments)}

\begin{corollary}\label{cor_norm_gradient_layers_symp}
	Given an activation function $\sigma \in C^1(\R)$,
	the normalized gradient layers $\mathcal{G}^{\eta}_{up}$ and $\mathcal{G}^{\eta}_{down}$
	(variant 1 and variant 2) are symplectic.
\end{corollary}
\begin{proof}
	Let $\mathcal{A}: \mathbb{R} \to \mathbb{R}$ be the antiderivative of $\activation$, 
	i.e. $\dd{x} \mathcal{A} = \activation$. For arbitrary $\alpha \in \R^n$, we define the potential
	\begin{equation*}
		V(p) := \sum_{j=1}^n \alpha_j a_j \mathcal{A}(\overline{p}_j)
		\quad \text{with } \overline{p} = \eta_{\gamma, \beta} \lb Kp + c \rb \in \R^n
	\end{equation*}
	Then $V \in C^2(\mathbb{R}^d, \mathbb{R})$ and for $i=1, \dots, d$
	\begin{align*}
		(\grad{V(p)})_i &= \deldel{p_i} \lb \sum_{j=1}^n \alpha_j a_j \mathcal{A}(\hat{p}_j) \rb
		= \sum_{j=1}^n \alpha_j a_j \activation(\overline{p}_j) \deldel{p_i} \overline{p}_j
		= \sum_{j=1}^n \alpha_j a_j \activation(\overline{p}_j)
		\frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon} \underbrace{K_{ji}}_{=K^T_{ij}} \\
		%
		\implies
		\grad{V(p)}
		&= K^T \bigg( \alpha_j a_j \frac{\gamma_j}{(\sigma^2_\mathcal{B})_j + \epsilon}
		\activation \lb \overline{p}_j \rb \bigg)_{j=1}^n
	\end{align*}
	If we choose $\alpha_j = 1$, follows $\grad{V(p)} = \layertf(p)$ for variant 2.
	If we choose $\alpha_j = \frac{(\sigma^2_\mathcal{B})_j + \epsilon}{\gamma_j}$,
	follows $\grad{V(p)} = \layertf(p)$ for variant 1.
	With \cref{gradient_corollary} follows that both normalized Gradient layer variants
	$\mathcal{G}_{up}^\eta$ and $\mathcal{G}_{low}^\eta$ are symplectic.
\end{proof}

\subsubsection{Normalized convolution gradient layers}\label{sec_conv_gradient_layer}

Analogously to the convolution gradient layers introduced in \cref{sec_conv_gradient_layer},
we can implement a normalized convolution gradient layer (variant 1 and variant 2) 
with width $n \in \mathbb{N}$
using valid cross-correlation (with stride) for $K^T \in \mathbb{R}^{d \times n}$ and the
corresponding transposed valid cross-correlation (with stride) for $K \in \mathbb{R}^{n \times d}$.
It is reasonable that the normalization respects the convolution in a certain way.
This involves calculating the mean and variance in a way that the convolution is respected.
In particular, given a convolution kernel size $n_k \in \N$
that the normalization shares weights 

In our numerical experiments, we only use normalized convolution gradient layers with equal 
kernel size $n_k$ and stride number $s$, i.e. $n_k = s$. For this special configuration,
the output $\tilde{p} \in \R^{n_k d}$ of the transposed cross-correlation can be expressed as
\begin{equation*}
	\tilde{p}_i = \lb *_{\text{v}}^T(k,p) \rb_i = \begin{cases}
		k_i p_1 &: 1 \leq i \leq n_k \\
		k_{i - n_k} p_2 &: n_k+1 \leq i \leq 2n_k \\
		k_{i - 2n_k} p_3 &: 2n_k+1 \leq i \leq 3n_k \\
		\dots
	\end{cases} \quad (1 \leq i \leq n_k d)
	.
\end{equation*}
So the transposed cross-correlation results in a upscaled vector $\tilde{p} \in \R^{n_k d}$ with blocks of size $n_k$.
Therefore, we implement the batch normalization transform $\eta_{\gamma, \beta}$ in such a way that $\tilde{p}_i$ is
normalized the same as $\tilde{p}_{i+n_k}$ for all $i = 1, \dots, n_k (d-1)$. In particular, this means that
$\lb \mu_\mathcal{B} \rb_i = \lb \mu_\mathcal{B} \rb_{i+n_k}$, 
$\lb \sigma^2_\mathcal{B} \rb_i = \lb \sigma^2_\mathcal{B} \rb_{i+n_k}$, 
$\gamma_i = \gamma_{i+n_k}$ and $\beta_i = \beta_{i+n_k}$ for $i = 1, \dots, n_k (d-1)$
and that the parameters $\gamma$ and $\beta$ can be parametrized with vectors
$\tilde{\gamma} \in \R^{n_k}$ and $\tilde{\beta} \in \R^{n_k}$. All respective entries of $\tilde{p} \in \R^{n_k d}$ 
belonging together are used to compute the mean and variance.
As before, the mean and variance are computed over all training samples in the mini-batch $\mathcal{B}$.

Similar normalization schemes, which respect the convolution, can be constructed for arbitrary 
kernel size $n_k$ and stride value $s$.

\subsubsection{Normalized activation layers}

\begin{definition}\label{def_norm_activation_layer_1}
	(Normalized activation layers - Variant 1)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \activation(\hat{p}_i) \rb_{i=1}^d
		\quad \text{with } \hat{p} = \eta_{\gamma, \beta} \lb p \rb \in \R^d
	\end{equation*}
	the normalized upper and lower activation layers $\mathcal{N}_\up^\eta$ and $\mathcal{N}_\low^\eta$
	(variant 1).
\end{definition}

\begin{definition}\label{def_norm_activation_layer_2}
	(Normalized activation layers - Variant 2)
	Given an activation function $\activation : \R \to \R$ and coefficients $a \in \mathbb{R}^d$, 
	we call the upper and lower unit triangular layers with bias $b=0$ and layer transform
	\begin{equation*}
		\layertf(p) = \lb a_i \frac{\gamma_i}{(\sigma^2_\mathcal{B})_i + \epsilon} 
		\activation(\hat{p}_i) \rb_{i=1}^d
		\quad \text{with } \hat{p} = \eta_{\gamma, \beta} \lb p \rb \in \R^d
	\end{equation*}
	the normalized upper and lower activation layers $\mathcal{N}_\up^\eta$ and $\mathcal{N}_\low^\eta$
	(variant 2).
	The variance $\sigma^2_\mathcal{B} \in \R^d$ refers to the mini-batch variance of $p$.
\end{definition}

The coefficients $a \in \mathbb{R}^d$ are learnable parameters.

\begin{corollary}
	Given an activation function $\sigma \in C^1(\R)$,
	the normalized activation layers $\mathcal{N}^{\eta}_{up}$ and $\mathcal{N}^{\eta}_{low}$
	(variant 1 and variant 2) are symplectic.
\end{corollary}
\begin{proof}
	Symplecticity follows because the normalized activation layers are a special case
	of normalized gradient layers (choose $n=d$, $K=I_d$ and $c=0$ for normalized gradient layers). 
	We have already shown that both normalized gradient layer variants are symplectic in
	\cref{cor_norm_gradient_layers_symp}.
\end{proof}

\section{Relation to geometric integrators}

The symplectic Euler and Störmer-Verlet schemes are two geometric integrators. Geometric integrators
are numerical integrators for ODE systems, which preserve a geometric property. In our context,
this geometric property is symplecticity. Precisely, if we denote the numerical integrator with
$\phi^{\text{h}}_{t,H} : \mathbb{R}^{2d} \to \mathbb{R}^{2d}$, the map $\phi^{\text{h}}_{t,H}$ is
symplectic.

The two variants of the symplectic Euler scheme are given by
\begin{equation*}
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H} (p_{n+1}, q_n) \\
			q_{n+1} &= q_n + h \grad[p]{H}(p_{n+1}, q_n)	
	\end{split}
	\quad\quad \text{or} \quad\quad
	\begin{split}
			p_{n+1} &= p_n - h \grad[q]{H}(p_n, q_{n+1}) \\
			q_{n+1} &= q_n + h \grad[p]{H}{p}(p_n, q_{n+1})	
	\end{split}
\end{equation*}

The $p$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	p_{n+1/2} &= p_n - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_n) \\[7pt]
	q_{n+1} &= q_n + \frac{h}{2} \lb \grad[p]{H}(p_{n+1/2}, q_n) + \grad[p]{H}(p_{n+1/2}, q_{n+1}) \rb \\[7pt]
	p_{n+1} &= p_{n+1/2} - \frac{h}{2} \grad[q]{H}(p_{n+1/2}, q_{n+1})
\end{align*}
and the $q$-staggered variant of the Störmer-Verlet scheme is given by
\begin{align*}
	q_{n+1/2} &= q_n + \frac{h}{2} \grad[p]{H}(p_n, q_{n+1/2}) \\[7pt]
	p_{n+1} &= q_n - \frac{h}{2} \lb \grad[q]{H}(p_n, q_{n+1/2}) + \grad[q]{H}(p_{n+1}, q_{n+1/2}) \rb \\[7pt]
	q_{n+1} &= q_{n+1/2} + \frac{h}{2} \grad[p]{H}(p_{n+1}, q_{n+1/2})
\end{align*}
We refer to \citet[p.~189 and p.~190]{hairer2006} for details.

If the Hamiltonian $H$ is separable, i.e. $H(q,p) = U(q) + V(p)$, the sympletic Euler and Störmer-Verlet
schemes become explicit. Then given $\grad[p]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $p \mapsto \grad[p]{H}(p)$ 
and $\grad[q]{H} : \mathbb{R}^d \to \mathbb{R}^d$, $q \mapsto \grad[q]{H}(q)$,
the left variant of the symplectic Euler scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix} \begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix} 
\end{equation*}
and the right variant of the symplectic Euler scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

Similarly, the $p$-staggered Störmer-Verlet scheme can be expressed as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & 0 \\
		-\frac{h}{2} \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & h \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{bmatrix}
		id & 0 \\
		-\frac{h}{2} \grad[q]{H} & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}
and the $q$-staggered Störmer-Verlet scheme as
\begin{equation*}
	\begin{pmatrix}
		q_{n+1} \\
		p_{n+1}
	\end{pmatrix} =
	\begin{bmatrix}
		id & \frac{h}{2} \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{bmatrix}
		id & 0 \\
		-h \grad[q]{H} & id
	\end{bmatrix}
	\begin{bmatrix}
		id & \frac{h}{2} \grad[p]{H} \\
		0 & id
	\end{bmatrix}
	\begin{pmatrix}
		q_{n} \\
		p_{n}
	\end{pmatrix}
\end{equation*}

To conclude, the explicit Euler and Störmer-Verlet schemes can be expressed with
the same unit triangular structure we use for SympNets. This means that a SympNet is able
reproduce both schemes exactly or approximately, provided appropriate layer choices.

Symplecticity for the explicit Euler and Störmer-Verlet schemes follows directly 
from \cref{gradient_corollary} and the fact that the composition of symplectic maps is again symplectic.

\section{Numerical experiments}

In this section, we compare the performance of different SympNet architectures.
Additionially, we compare SympNets to network architectures which do not intrinsically
conserve the symplectic property.

In all our experiments, we use the mean-square error (MSE) loss defined by
\begin{equation*}
	L(\mathcal{B}, \Theta) = \sum_{(x_i, y_i) \in \mathcal{B}} \norm{\Phi(x_i; \Theta) - y_i}^2_2
	,
\end{equation*}
where $\mathcal{B}$ refers to a mini-batch and $\Theta$ refers to the neural network parameters.
We work with very small training data. Thus, we do not split the training data into mini-batches.
We initialize all bias parameters with $0$. All other parameters are initialized randomly from
the normal distribution with mean $0$ and standard deviation $0.01$.
For training, we use the AMSgrad variant of the Adam optimizer \cite{amsgrad2018}.
The standard Adam optimizer led to large loss spikes at later epochs and often
failed to converge to stable parameter values. This phenomenon
was greatly reduced by switching to the AMSgrad variant. The Adam alogrithm (and the AMSgrad variant)
computes individual adaptive learning rates for different parameters, based on a baseline
learning rate hyperparameter $\gamma \in \R$.

In all our experiments the training and test data are subsets of $\mathcal{V} \times \mathcal{V}$,
where $\mathcal{V} = \R^{2d}$ denotes the phase space of the Hamiltonian system. In particular,
given some phase space samples $x_1, \dots, x_n \in \mathcal{V}$, we generate data samples 
$\mathcal{T} \subset \mathcal{V} \times \mathcal{V}$
with the $q$-staggered Störmer-Verlet integrator $\phi^{\text{h}}_{t,H}$
for a fixed time $t \in \R$ and the respective Hamiltonian $H$:
\begin{equation*}
	\mathcal{T} = \{ (x_i, \phi^{\text{h}}_{t,H}(x_i)) \}_{i=1}^{n_{\text{train}}}
\end{equation*}
The data samples $\mathcal{T}$ are split into a training data set $\mathcal{T}_{\text{train}}$
and $\mathcal{T}_{\text{test}}$, i.e. $\mathcal{T}_{\text{train}} \cup \mathcal{T}_{\text{test}} = \mathcal{T}$
and $\mathcal{T}_{\text{train}} \cap \mathcal{T}_{\text{test}} = \emptyset$. 
We denote the size of the training data set with $n_{\text{train}} \in \N$ and the size of
the test data set with $n_{\text{test}} \in \N$.
The training data
$\mathcal{T}_{\text{train}}$ and test data $\mathcal{T}_{\text{test}}$ are used to 
calculate the training and test loss.

\subsection{Low-dimensional systems}

In this section, we deal with low-dimensional Hamiltonian systems with $d=1$.
The neural network architectures used in the experiments with low-dimensional systems
are listed in \cref{table_low_dim_arch}.
\begin{table}
	\centering
	\begin{tabular}{lp{8cm}c}
		\toprule Architecture & Layers & Parameters \\
		\midrule L-SympNet & Nine alternating upper and lower linear layers,
		with bias in last layer, see \cref{def_linear_layer}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $11$ \\
		%
		FNN & 
		A fully-connected network with overall input and output dimension $2$. The network has one hidden
		(fully-connected) linear layers with input and output dimension $50$.
		Nonlinear activation layers are placed between all linear layers.
		& $2802$ \\
		%
		LA-SympNet & LA-SympNet with depth $5$ and $4$ sublayers, see \cref{def_la_sympnet}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $41$ \\
		%
		N1-LA-SympNet & 
		Same as LA-SympNet, but with normalized activation layers (variant 1, 
		see \cref{def_norm_activation_layer_1}) instead of activation layers.
		& $51$ \\
		%
		N2-LA-SympNet & 
		Same as LA-SympNet, but with normalized activation layers (variant 2, 
		see \cref{def_norm_activation_layer_2}) instead of activation layers.
		& $51$ \\
		%
		G-SympNet & G-SympNet with depth $4$ and width $30$, see \cref{def_g_sympnet}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $360$ \\
		%
		N1-G-SympNet & 
		Same as G-SympNet, but with normalized Gradient layers 
		(variant 1, see \cref{def_norm_gradient_layers_1}) instead of Gradient layers.
		& $600$ \\
		%
		N2-G-SympNet & 
		Same as G-SympNet, but with normalized Gradient layers 
		(variant 2, see \cref{def_norm_gradient_layers_2}) instead of Gradient layers. 
		& $600$ \\
		%
		LARGE-FNN &
		A fully-connected network with overall input and output dimension $2$. The network has two hidden
		(fully-connected) linear layers with input and output dimension $70$.
		Nonlinear activation layers are placed between all linear layers.
		& $10292$ \\
		%
		LARGE-LA-SympNet & LA-SympNet with depth $40$ and $9$ sublayers, see \cref{def_la_sympnet}.
		Initially proposed by \citeauthor{Jin2020} in \cite{Jin2020}.
		& $491$ \\
		%
		LARGE-N1-LA-SympNet & 
		Same as LARGE-LA-SympNet, but with normalized activation layers (variant 1, 
		see \cref{def_norm_activation_layer_1}) instead of activation layers.
		& $571$ \\
		%
		LARGE-N2-LA-SympNet & 
		Same as LARGE-LA-SympNet, but with normalized activation layers (variant 2, 
		see \cref{def_norm_activation_layer_2}) instead of activation layers.
		& $571$ \\ \bottomrule
	\end{tabular}
	\caption{
	Table of all neural network architectures for the
	low-dimensional experiments. The activation function used inside the activation
	and gradient layers is either sigmoid, tanh or ELU and is explicitly stated.
	}\label{table_low_dim_arch}
\end{table}

For all low-dimensional experiments, the training and test data are generated
with the fixed time $t = 0.1$
from phase space samples uniformly sampled from a compact set $\mathcal{D} \subset \R^2$.

\subsubsection{Harmonic Oscillator}

For $q,p \in \R$, the Hamiltonian for the Harmonic Oscillator is given by
\begin{equation*}
	H(q,p) = \frac{p^2}{2m} + \frac{1}{2} kq^2
\end{equation*}
For the experiment, we choose $m=1$ and $k=1$.

The flow for the Harmonic Oscillator is linear, because the resulting ODE
is linear. Thus, the ODE can be solved via the variation of constants formula,
which results in a linear flow.
As we have mentioned,
\citeauthor{jin2020unit} have shown in \cite{jin2020unit} that $\mathcal{L}^{9}_{up}$
can parametrize every symplectic linear map.
Consequently, a SympNet consisting out of nine alternating 
upper and lower linear layers should be able to learn the flow for the Harmonic Oscillator.
Indeed, \cref{fig_harm_osc_loss} shows that such a linear SympNet successfully 
learns the flow for the Harmonic Oscillator with a very low test loss.

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{axis}[
		ymode=log,
		no markers,
		title={Test loss (Harmonic Oscillator)},
		xlabel={Epoch}, xmin=0, xmax=500,
		ylabel={Test loss},
		legend entries={
			L-SympNet
		}
	]

	\addplot[
		color=orange
	] table[x=epoch,y=loss,col sep=comma] {../data/harmonic_oscillator/l-sympnet/sigmoid/test_loss.csv};
		
	\end{axis}
\end{tikzpicture}
\caption{Test loss for Harmonic Oscillator ($500$ epochs). The L-SympNet consists out of nine alternating upper and 
lower linear layers. The training and test data are generated from random phase space samples in 
$\mathcal{D} = [-2,2] \times [-2,2]$. The training data size is $n_{\text{train}} = 40$
and the test data size is $n_{\text{test}}=400$.
The learning rate is set to $\gamma = 0.01$.}\label{fig_harm_osc_loss}
\end{figure}

\subsubsection{Simple Pendulum}

For $q,p \in \R$, the Hamiltonian for the Simple Pendulum is given by
\begin{equation*}
	H(q,p) = \frac{p^2}{2ml^2} + mgl (1-cos(q))
\end{equation*}
For the experiment, we choose $m=1, g=1$ and $l=1$. The flow of the Simple Pendulum is nonlinear.
Therefore, the flow cannot be represented by linear models, which makes the Simple Pendulum
a good experiment to look into. The generalized coordinate $q$ denotes the angle of the pendulum.

We conduct two different experiments with different
phase space samples originating from two different sets $\mathcal{D}$:
\begin{align*}
	\mathcal{D}_{\text{swing}} &= [-\frac{\pi}{2}, \frac{\pi}{2}] \times [-\sqrt{2}, \sqrt{2}]
	\quad (\text{swinging}) \\
	\mathcal{D}_{\text{swing+rotate}} &= [-20, 20] \times [-2.5, 2.5]
	\quad (\text{swinging and rotating})
\end{align*}
Physically, for $(q,p) \in \mathcal{D}_{\text{swing}}$ only swinging is possible,
because $\abs{q} \leq \pi/2$ for all $t$ and too low momentum $p$.
We choose $n_{\text{train}} = 40$ and $n_{\text{test}} = 400$ for all experiments involving
the Simple Pendulum. The test data is sampled from the same compact set $\mathcal{D}$ as the training data.

\cref{fig_pend_exp1_loss} and \cref{{table_pend_exp1_loss}} show the training and test loss for the different architectures
and activation functions with training and test data generated from $\mathcal{D}_{\text{swing}}$ and learning rate $\gamma = 0.01$.
Note that incorporating normalization results in better training and test losses, and may also stabilize
the learning process. The SympNets do not tend to overfit.
In contrast, the fully-connected networks (FNN) tend to overfit, because they have a high test loss (right column) 
compared to the training loss (left column).
\cref{fig_pend_exp1_phase_plots} and \cref{fig_pend_exp1_time} show phase plots, the total energy over time
and the predicted position over time for the two best-performing architectures and the FNN.
Note that no architecture is able to successfully predict the phase flow for a initial value outside the training data,
i.e. $(q_0, p_0) \notin \mathcal{D}_{\text{swing}}$.

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[	
		group style={
			group size=2 by 3,
			y descriptions at=edge left,
			horizontal sep=0.1cm,
			vertical sep=2.2cm,
		},
		xlabel=Epoch, ylabel=Loss,
		width=\axisdefaultwidth, height=7cm,
		no markers,
		ymax=1e-1, ymin=5e-10, ymode=log,
		legend style={nodes={scale=0.75, transform shape}},
		legend entries={
			FNN,
			LA-SympNet,
			N1-LA-SympNet,
			N2-LA-SympNet,
			G-SympNet,
			N1-G-SympNet,
			N2-G-SympNet
		}
	]
		\nextgroupplot[title={Training loss (sigmoid activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/sigmoid/loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=magenta, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/sigmoid/loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/sigmoid/loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/sigmoid/loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/sigmoid/loss.csv};

		\nextgroupplot[title={Test loss (sigmoid activation)},]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/sigmoid/test_loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=magenta, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/sigmoid/test_loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/sigmoid/test_loss.csv};

		\nextgroupplot[title={Training loss (tanh activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/tanh/loss.csv};

			\addplot[
				color=magenta, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/tanh/loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/tanh/loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/tanh/loss.csv};

		\nextgroupplot[title={Test loss (tanh activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/test_loss.csv};

			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=magenta, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/tanh/test_loss.csv};

			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/tanh/test_loss.csv};

			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/test_loss.csv};

			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/tanh/test_loss.csv};

		\nextgroupplot[title={Training loss (ELU activation)}]

			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/elu/loss.csv};
		
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/elu/loss.csv};
		
			\addplot[
				color=magenta, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/elu/loss.csv};
		
			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/elu/loss.csv};
		
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/elu/loss.csv};
		
			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/elu/loss.csv};
		
			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/elu/loss.csv};

		\nextgroupplot[title={Test loss (ELU activation)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/fnn/elu/test_loss.csv};
		
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=magenta, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=orange, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-la-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/g-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=olive, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/elu/test_loss.csv};
		
			\addplot[
				color=cyan, densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing/n2-g-sympnet/elu/test_loss.csv};
		
	\end{groupplot}
\end{tikzpicture}
\caption{Training and test loss for simple pendulum with training and test data 
generated from $\mathcal{D}_{\text{swing}}$.
(First row) sigmoid activation (Second row) tanh activation (Third row) ELU activation.
Normalization leads to improved performance.}\label{fig_pend_exp1_loss}
\end{figure}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss (learning rate $\gamma = 0.01$)}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data/simple_pendulum_swing/test_loss_summary.csv}

	\caption{Test losses for Simple Pendulum 
	with training and test data generated from $\mathcal{D}_{\text{swing}}$ after $10^5$ epochs. 
	}\label{table_pend_exp1_loss}
\end{table}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 1,
				horizontal sep=2cm
			},
			width=7.7cm,
			xlabel=$q$, ylabel=$p$,
			xmajorgrids
		]
			
		\nextgroupplot[
			title={Phase plot for $q_0 = \frac{\pi}{2},\, p_0 = 0$},
			legend entries={
				FNN (tanh),
				N2-LA-SympNet (tanh),
				N2-G-SympNet (tanh),
				Exact
			},
			legend columns=4,
			legend to name=leg_ped_swing_phase
		]
			\addplot[
				color=green
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=magenta
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=olive
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/swinging_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/phase_plot.csv};
	
		\nextgroupplot[title={Phase plot for $q_0 = \pi,\, p_0=1$}, ymax=6]
			\addplot[
				color=green
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/rotating_case/phase_plot.csv};

			\addplot[
				color=magenta
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/sigmoid/rotating_case/phase_plot.csv};
	
			\addplot[
				color=olive
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/rotating_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing/exact/rotating_case/phase_plot.csv};
	
		\end{groupplot}
	\end{tikzpicture}
	\\[5pt]
	\ref{leg_ped_swing_phase}
	\caption{Phase plots for simple pendulum. Training and test data generated from $\mathcal{D}_{\text{swing}}$.
	(Left) The phase plots of the SympNets and the exact solution are indistinguishable.
	(Right) The neural networks fails to generalize for a $(q_0, p_0) \notin \mathcal{D}_{\text{swing}}$.}
	\label{fig_pend_exp1_phase_plots}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 1,
				horizontal sep=2cm
			},
			no markers,
			xlabel={t},
			width=7.7cm,
			legend style={nodes={scale=0.75, transform shape}},
			legend entries={
				Störmer-Verlet,
				FNN (tanh),
				N1-LA-SympNet (tanh),
				N1-G-SympNet (tanh)
			}
		]

		\nextgroupplot[title={Total energy $H$}, ylabel={$H(t)$}, xmin=0, xmax=100]
			\addplot[
				color=darkgray
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/total_energy.csv};

			\addplot[
				color=green
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/swinging_case/total_energy.csv};

			\addplot[
				color=magenta,densely dashed
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/swinging_case/total_energy.csv};

			\addplot[
				color=olive,densely dashed
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/swinging_case/total_energy.csv};

		\nextgroupplot[title={Predicted position $q$}, ylabel={$q(t)$}, xmin=91, xmax=97, legend pos=north west]
			\addplot[
				color=darkgray
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/exact/swinging_case/q.csv};

			\addplot[
				color=green
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/fnn/tanh/swinging_case/q.csv};

			\addplot[
				color=magenta,densely dashed
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/n1-la-sympnet/tanh/swinging_case/q.csv};

			\addplot[
				color=olive,densely dashed
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing/n1-g-sympnet/tanh/swinging_case/q.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{Total energy and position over time for simple pendulum for $q_0 = \frac{\pi}{2},\, p_0 = 0$.
	Training and test data generated from $\mathcal{D}_{\text{swing}}$.
	The total energy is maintained with the SympNets, in contrast to the FNN.
	The prediction of the position over long time is better with SympNets than with the FNN.}
	\label{fig_pend_exp1_time}
\end{figure}

\cref{fig_pend_exp2_loss} shows the test losses if the training and test data
is generated from $\mathcal{D}_{\text{swing+rotate}}$ instead.
We perform the experiment with learning rate $\gamma = 0.01$ and faster learning rate $\gamma = 1.0$.
\cref{table_pend_exp2_loss} shows the final test loss for the first learning rate $\gamma = 0.01$.
\cref{table_pend_exp2_loss_fast_lr} shows the final test loss for the faster learning rate $\gamma = 1.0$.

The LARGE-LA-SympNet and the normalized variants LARGE-N1-LA-SympNet and LARGE-N2-LA-SympNet completely fail to learn the
flow if the training data is generated from $\mathcal{D}_{\text{swing+rotate}}$, 
although their depth has been increased such that they have the same amount of parameters 
as the G-SympNets (see \cref{table_low_dim_arch}).

For training and test data generated from $\mathcal{D}_{\text{swing+rotate}}$, 
normalization does not lead to a significant change in prediction performance, compared to
what we observed with $\mathcal{D}_{\text{swing}}$. For $\mathcal{D}_{\text{swing+rotate}}$,
the G-SympNet with sigmoid activation performs best overall
(see left plot in first row of \cref{fig_pend_exp2_loss}). Still, normalization slightly improves the
performance for the tanh and ELU activation (see 
mid and right plot in first row of \cref{fig_pend_exp2_loss}).
Furthermore, normalization shows more robustness with respect to faster learning rates
(see second row in \cref{fig_pend_exp2_loss}). The G-SympNet without normalization fails to converge
for the faster learning rate $\gamma = 1.0$, in contrast to the N1-G-SympNet and N2-G-SympNet.

The generalized coordinate $q$ is an angular coordinate for the simple pendulum. This is the reason why 
$\lim_{t \to \infty} q(t) = \infty$
in the rotating case. This circumstance makes learning the neural network hard, especially if the
goal is to have a good prediction over a long time. 
Furthermore, intuitively, the large values of $q$ could mask the swinging case, where $\abs{q} \leq \pi/2$.
This might be an explanation for why normalization does not lead to a 
similar performance improvement for $\mathcal{D}_{\text{swing+rotate}}$
as with $\mathcal{D}_{\text{swing}}$. This issue needs further investigation.
We comment this issue in the Outlook (\cref{sec_outlook}).

For almost all experiments, the first variants of the normalization layers (N1) perform better or equally good
to the second variants of the normalization layers (N2). There are only two cases where the N2 normalization variant
has a slightly better final test loss than the N1 normalization variant 
(see \cref{fig_pend_exp2_loss}, right plot in first row and mid plot in second row).

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{groupplot}[
		group style={
			group size=3 by 2,
			y descriptions at=edge left,
			horizontal sep=0.1cm,
			vertical sep=2.2cm,
		},
		xlabel=Epoch, ylabel={Test loss},
		width=6.3cm, height=7cm,
		no markers,
		ymax=1e-2, ymin=5e-7, ymode=log
	]
	
		\nextgroupplot[
			title={Sigmoid ($\gamma = 0.01$)},
			legend entries={
				LARGE-FNN,
				LARGE-LA-SympNet,
				LARGE-N1-LA-SympNet,
				LARGE-N2-LA-SympNet,
				G-SympNet,
				N1-G-SympNet,
				N2-G-SympNet
			},
			legend columns=4,
			legend to name=grouplegend
		]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=magenta,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n1-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=orange,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n2-la-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/sigmoid/test_loss.csv};
	
		\nextgroupplot[title={tanh ($\gamma = 0.01$)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/tanh/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=magenta,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n1-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=orange,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n2-la-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/tanh/test_loss.csv};
	
		\nextgroupplot[title={ELU ($\gamma = 0.01$)}]
			\addplot[
				color=green
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/elu/test_loss.csv};
	
			\addplot[
				color=red
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=magenta,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n1-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=orange,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/large-n2-la-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=blue
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=olive,,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n1-g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot/n2-g-sympnet/elu/test_loss.csv};

		%%% FROM HERE: fast learning rate
		\nextgroupplot[title={Sigmoid ($\gamma = 1.0$)}]
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n1-g-sympnet/sigmoid/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n2-g-sympnet/sigmoid/test_loss.csv};
	
		\nextgroupplot[title={tanh ($\gamma = 1.0$)}]	
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n1-g-sympnet/tanh/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n2-g-sympnet/tanh/test_loss.csv};
	
		\nextgroupplot[title={ELU ($\gamma = 1.0$)}]
			\addplot[
				color=olive,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n1-g-sympnet/elu/test_loss.csv};
	
			\addplot[
				color=cyan,densely dashed
			] table[x=epoch,y=loss,col sep=comma] {../data/simple_pendulum_swing_rot_fast_lr/n2-g-sympnet/elu/test_loss.csv};
				
	\end{groupplot}
\end{tikzpicture}
\\[5pt]
\ref{grouplegend}
\caption{Test loss for simple pendulum with training and test data from $\mathcal{D}_{\text{swing+rotate}}$.
(First row) Learning rate $\gamma = 0.01$. (Second row) Faster learning rate $\gamma = 1.0$.
Test losses bigger than $10^{-2}$ and architectures that diverged during training are not displayed.
}\label{fig_pend_exp2_loss}
\end{figure}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss (learning rate $\gamma = 0.01$)}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data/simple_pendulum_swing_rot/test_loss_summary.csv}

	\caption{Test losses for Simple Pendulum with training and test data
	generated from $\mathcal{D}_{\text{swing+rotate}}$ after $3 \cdot 10^5$ epochs.}
	\label{table_pend_exp2_loss}
\end{table}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss (learning rate $\gamma = 1.0$)}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data/simple_pendulum_swing_rot_fast_lr/test_loss_summary.csv}

	\caption{Test losses for simple pendulum after $3 \cdot 10^5$ epochs
	with training and test data generated from $\mathcal{D}_{\text{swing+rotate}}$
	and faster learning rate $\gamma = 1.0$.
	NaN means that the test loss diverged. Only G-SympNets with normalization
	are able to handle the faster learning rate.}
	\label{table_pend_exp2_loss_fast_lr}
\end{table}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 1,
				horizontal sep=2cm
			},
			width=7.7cm,
			xlabel=$q$, ylabel=$p$,
			xmajorgrids
		]
			
		\nextgroupplot[
			title={Phase plot for $q_0 = \frac{\pi}{2},\, p_0 = 0$},
			legend entries={
				LARGE-FNN (sigmoid),
				LARGE-LA-SympNet (tanh),
				G-SympNet (sigmoid),
				Störmer-Verlet
			},
			legend columns=4,
			legend to name=leg_ped_swing_phase
		]
			\addplot[
				color=green,thin
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/swinging_case/phase_plot.csv};
	
			\addplot[
				color=red
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/tanh/swinging_case/phase_plot.csv};
	
			\addplot[
				color=blue
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/swinging_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/phase_plot.csv};
	
		\nextgroupplot[title={Phase plot for $q_0 = \pi,\, p_0=1$}]
			\addplot[
				color=green
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/rotating_case/phase_plot.csv};

			\addplot[
				color=red
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/large-la-sympnet/tanh/rotating_case/phase_plot.csv};
	
			\addplot[
				color=blue
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/rotating_case/phase_plot.csv};

			\addplot[
				color=gray
			] table[x=q,y=p,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/phase_plot.csv};
	
		\end{groupplot}
	\end{tikzpicture}
	\\[5pt]
	\ref{leg_ped_swing_phase}
	\caption{Phase plots for simple pendulum. Training and test data uniformly sampled from $\mathcal{D}_{\text{swing+rotate}}$.
	(Left) Only the G-SympNet is indistinguishable from the exact phase plot. The FNN does not have a closed orbit.
	(Right) The LA-SympNet fails to predict the phase plot for $q_0 = \pi,\, p_0=1$.}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=2 by 2,
				horizontal sep=2cm,
				vertical sep=2cm,
			},
			no markers,
			xlabel={t},
			width=7cm,
			legend style={nodes={scale=0.75, transform shape}},
			legend entries={
				Störmer-Verlet,
				LARGE-FNN (sigmoid),
				G-SympNet (sigmoid)
			}
		]

		%% Swinging case
		\nextgroupplot[title={Total energy $H$ for $q_0 = \frac{\pi}{2},\, p_0 = 0$}, ylabel={$H(t)$}, xmin=0, xmax=100]
			\addplot[
				color=darkgray
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/total_energy.csv};

			\addplot[
				color=green
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/swinging_case/total_energy.csv};

			\addplot[
				color=blue
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/swinging_case/total_energy.csv};

		\nextgroupplot[title={Predicted position $q$ for $q_0 = \frac{\pi}{2},\, p_0 = 0$}, ylabel={$q(t)$}, xmin=91, xmax=97, legend pos=north west]
			\addplot[
				color=darkgray
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/exact/swinging_case/q.csv};

			\addplot[
				color=green
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/swinging_case/q.csv};

			\addplot[
				color=blue
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/swinging_case/q.csv};

		%% Rotating case	
		\nextgroupplot[title={Total energy $H$ for $q_0 = \pi,\, p_0=1$}, ylabel={$H(t)$}, xmin=0, xmax=10]
			\addplot[
				color=darkgray
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/total_energy.csv};

			\addplot[
				color=green
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/rotating_case/total_energy.csv};

			\addplot[
				color=blue
			] table[x=t,y=E,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/rotating_case/total_energy.csv};

		\nextgroupplot[title={Predicted position $q$ for $q_0 = \pi,\, p_0=1$}, ylabel={$q(t)$}, xmin=8, xmax=9, legend pos=north west]
			\addplot[
				color=darkgray
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/exact/rotating_case/q.csv};

			\addplot[
				color=green
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/large-fnn/sigmoid/rotating_case/q.csv};

			\addplot[
				color=blue
			] table[x=t,y=q,col sep=comma] {../data/simple_pendulum_swing_rot/g-sympnet/sigmoid/rotating_case/q.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{
	Training and test data generated from $\mathcal{D}_{\text{swing+rotate}}$, 
	learning rate $\gamma = 0.01$.
	Total energy and position over time for simple pendulum for $q_0 = \frac{\pi}{2},\, p_0 = 0$.
	(first row) and $q_0 = \pi,\, p_0=1$ (second row).
	The total energy is better maintained with the SympNet.
	The prediction of the position over long time is better with the SympNet than with the FNN.}
\end{figure}

\FloatBarrier
\subsection{High-dimensional systems}

\todo{A PDE describes system locally, thus it makes sense to share parameters, for
example in the form of CNNs etc.}

Assumptions:
\begin{itemize}
	\item \todo{Original PDE has constant coefficients Or rather should not depend on $x$ itself?}
	\item Equidistant grid points
\end{itemize}

\todo{Explain why we need assumptions.}

The one-dimensional semi-linear wave equation with constant speed $c \in \R$, nonlinear term 
$g : \R \to \R$ and domain length $l \in \R$
\begin{equation*}
	\frac{\partial^2}{\partial t^2} u(t,x) = 
	c^2 \frac{\partial^2}{\partial x^2} u(t,x) - g(u(t,x)) 
	\quad \forall x \in \lsb -\frac{l}{2},\frac{l}{2} \rsb, 
	\, t \in I \subset \R
\end{equation*}
\todo{double-check domain}
with initial conditions
\begin{align*}
	u(t_0,x) &= u_0(x) \\
	\deldelt u(t_0,x) &= w_0(x) \quad \forall x \in \lsb -\frac{l}{2},\frac{l}{2} \rsb
\end{align*}
and Dirichlet boundary conditions
\begin{equation*}
	u(t,-l/2) = u_{\text{left}}, \, u(t,l/2) = u_{\text{right}} \quad \forall t \in I \subset \R
\end{equation*}
can be transformed into a Hamiltonian ODE system upon discretization with finite differences
(see \cite{2006ham_pde} and \cite{peng2016} for details). 
Here, $u(x,t) \in \R$ denotes the unknown time-dependent displacement field on the interval $[-l/2, l/2]$.

Given an equally-spaced grid $\{ x_i \}_{n=0}^{n+1} \subset \R$, $x_0 = -l/2$, $x_{n+1} = l/2$,
with grid width $\Delta x$,
initial values $y_0 = (\bar{q}_0^T, \bar{p}_0^T)^T \in \R^{2n}$, $\bar{q}_0 = \lb u_0(x_i) \rb_{i=1}^n \in \R^n$, 
$\bar{p}_0 = \lb w_0(x_i) \rb_{i=1}^n \in \R^n$,
the resulting Hamiltonian system is given by
\begin{align*}
	\dot{y}(t) &= \frac{J_{2n}}{\Delta x} \grad{H}(y(t)) \quad \text{for } t \in I \subset \mathbb{R} \\
	y(t_0) &= y_0
\end{align*}
with Hamiltonian
\begin{equation*}
	H(q,p) = \sum_{i=1}^n \Delta x \lb 
	\frac{1}{2} p_i^2 + \frac{c^2 (q_{i+1} - q_i)^2}{4 \Delta x^2} 
	+ \frac{c^2 (q_i - q_{i-1})^2}{4 \Delta x^2} + G(q_i)
	\rb
	\quad (q,p \in \R^n)
\end{equation*}
where $G'(u) = \int_0^u g(v) dv$, $q_0 := u_{\text{left}}$ and $q_{n+1} := u_{\text{right}}$.
The generalized coordinates $q_i(t)$ then refer to $u(t,x_i)$ and 
the conjugate momenta $p_i(t)$ then refer to $\deldelt u(t,x_i)$.

The neural network architectures used in the experiments with high-dimensional systems
are listed in \cref{table_high_dim_arch}.

\begin{table}
	\centering
	\begin{tabular}{lp{8cm}c}
		\toprule Architecture & Layers & Parameters \\
		\midrule CNN & 
		A standard purely linear convolutional neural network with four $1$-dimensional
		convolution layers with kernel size $3$ and symmetric padding to keep the input dimension. 
		& $12$ \\
		%
		C-SympNet & A purely linear SympNet consisting out of four alternating upper and lower
		(symplectic) convolution layers with kernel size $3$ and constant zero-padding,
		see \cref{def_conv_layer}. 
		& $8$ \\
		%
		LARGE-CNN &
		A standard convolutional neural network consisting of four consecutive blocks of
		the following layers: $1$-dimensional convolution layer with kernel size $3$
		and symmetric padding, $1$-dimensional transposed convolution layer
		with kernel size $100$ and stride $100$, nonlinear activation layer
		and $1$-dimensional convolution layer with kernel size $100$ and stride $100$. 
		& $809$ \\
		%
		CG-SympNet & 
		A SympNet consisting out of four consecutive blocks of
		the following two layers: upper (or lower) $1$-dimensional symplectic convolution layer with
		kernel size $3$ and symmetric padding (see \cref{def_conv_layer}) 
		followed by a upper (or lower) convolution gradient layer
		with kernel size $100$ and stride $100$ (see \cref{sec_conv_gradient_layer}). 
		The four blocks as a whole alternate between
		the upper and lower variants, i.e. a single block in itself contains either only the upper or only
		the lower variants. 
		& $1208$ \\
		%
		N1-CG-SympNet & Same as CG-SympNet, but with normalized convolution gradient layers
		(variant 1, see \cref{sec_conv_gradient_layer}) instead of convolution gradient layers.
		& $2008$ \\
		%
		N2-CG-SympNet & Same as CG-SympNet, but with normalized convolution gradient layers
		(variant 2, see \cref{sec_conv_gradient_layer}) instead of convolution gradient layers.
		& $2008$ \\ \bottomrule
	\end{tabular}
	\caption{Table of all neural network architectures for the
	high-dimensional experiments. The activation function used inside the nonlinear activation layers
	and the convolution gradient layers is either sigmoid, tanh or ELU and is explicitly stated.
	If not stated otherwise, we use the FD basis to parametrize the symmetric kernels 
	inside the symplectic convolution layers.}\label{table_high_dim_arch}
\end{table}

\subsubsection{Linear wave equation}

Transport problem

$G(u) = g(u) =0$, $u_{\text{left}} = u_{\text{right}} = 0$

$c=1$

initial values bump function
\begin{align*}
	u_0(x) &= \begin{cases}
	\frac{1}{2} \lb 
	exp \lb - \lb \frac{6x}{w} \rb^2 \rb - exp \lb - \lb \frac{6w/2}{w} \rb^2 \rb 
	\rb & : \abs{x} \leq w \\
	0 & : \text{else}
	\end{cases} \\
	w_0(x) &= 0
\end{align*}
with $w = l/4$.

\begin{figure}
\centering
\begin{tikzpicture}
	\begin{axis}[
		ymode=log,
		no markers,
		title={Test loss (Linear wave equation)},
		xlabel={Epoch}, xmin=0, xmax=500,
		ylabel={Test loss},
		legend style={nodes={scale=0.75, transform shape}},
		legend cell align=left,
		legend entries={
			CNN,
			C-SympNet (canonical basis),
			C-SympNet (FD basis)
		}
	]

	\addplot[
		color=green
	] table[x=epoch,y=loss,col sep=comma] {../data-wave/transport/linear_cnn/sigmoid/test_loss.csv};

	\addplot[
		color=blue
	] table[x=epoch,y=loss,col sep=comma] {../data-wave/transport/linear_canonical/sigmoid/test_loss.csv};

	\addplot[
		color=magenta
	] table[x=epoch,y=loss,col sep=comma] {../data-wave/transport/linear_fd/sigmoid/test_loss.csv};
		
	\end{axis}
\end{tikzpicture}
\caption{\todo{todo}}
\end{figure}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{axis}[
			no markers,
			width=0.7*\linewidth, height=\axisdefaultheight,
			title={Displacement $q(t=9,x)$ (Linear wave equation)},
			xlabel={$x$}, xmin=-0.5, xmax=0.5,
			ylabel={$q(t=9,x)$}, ymin=-0.3, ymax=0.2, restrict y to domain=-2:2,
			legend style={
				nodes={scale=0.75, transform shape},
				legend cell align=left,
				%legend pos=outer north east
			},
			legend entries={
				CNN,
				C-SympNet (canonical basis),
				C-SympNet (FD basis),
				Störmer-Verlet,
			}
		]	
	
		\addplot[
			color=green
		] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_cnn/sigmoid/q_t9.csv};
	
		\addplot[
			color=blue
		] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_canonical/sigmoid/q_t9.csv};
	
		\addplot[
			color=magenta
		] table[x=x,y=q,col sep=comma] {../data-wave/transport/linear_fd/sigmoid/q_t9.csv};

		\addplot[
			color=darkgray
		] table[x=x,y=q,col sep=comma] {../data-wave/transport/exact/q_t9.csv};
			
		\end{axis}
	\end{tikzpicture}
	\caption{\todo{todo}}
\end{figure}

\todo{The choice of basis for the symmetric convolution kernels is critical (for fast convergence)!}

\subsubsection{Sine-Gordon}

The Sine-Gordon equation is a 
semi-linear wave equation with $G(u) = 1 - cos(u)$, $g(u) = sin(u)$ and $c=1$.

One can show that the Sine-Gordon equation has the solution
\begin{equation*}
	u_{\text{sol}}(t,x) = 4 arctan \lsb exp \lb \frac{x-x_0-vt}{\sqrt{1-v^2}} \rb \rsb 
	.
\end{equation*}

For our experiment, we set the initial values
\begin{align*}
	u_0(x) &= u_{\text{sol}}(0,x) \\
	w_0(x) &= \deldelt u_{\text{sol}}(0,x)
\end{align*}
with $v=0.2$ and the boundary conditions $u_{\text{left}} = 0, \, u_{\text{right}} = 2\pi$.

\todo{Padding modes}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[	
			group style={
				group size=2 by 3,
				y descriptions at=edge left,
				horizontal sep=0.7cm,
				vertical sep=2.2cm,
			},
			xlabel=Epoch, ylabel=Loss,
			width=0.95*\axisdefaultwidth, height=7cm,
			no markers,
			ymax=1e-1, ymin=5e-10, ymode=log,
			xmin=0, xmax=2000,
			legend style={nodes={scale=0.75, transform shape}},
			legend entries={
				LARGE-CNN,
				CG-SympNet,
				N1-CG-SympNet,
				N2-CG-SympNet
			}
		]
			\nextgroupplot[title={Training loss (sigmoid activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/sigmoid/loss.csv};
	
				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/sigmoid/loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/sigmoid/loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/sigmoid/loss.csv};
	
	
			\nextgroupplot[title={Test loss (sigmoid activation)},]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/sigmoid/test_loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/sigmoid/test_loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/sigmoid/test_loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/sigmoid/test_loss.csv};
	
			\nextgroupplot[title={Training loss (tanh activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/tanh/loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/tanh/loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/tanh/loss.csv};
	
			\nextgroupplot[title={Test loss (tanh activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/tanh/test_loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/test_loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/tanh/test_loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/tanh/test_loss.csv};
	
			\nextgroupplot[title={Training loss (ELU activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/elu/loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/elu/loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/elu/loss.csv};
	
			\nextgroupplot[title={Test loss (ELU activation)}]
				\addplot[
					color=green
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/cnn/elu/test_loss.csv};

				\addplot[
					color=red
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/gradient/elu/test_loss.csv};

				\addplot[
					color=blue
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/test_loss.csv};

				\addplot[
					color=olive
				] table[x=epoch,y=loss,col sep=comma] {../data-wave/sine_gordon/n2-gradient/elu/test_loss.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{\todo{Sine Gordon}}
\end{figure}

\begin{table}
	\centering
	\pgfplotstabletypeset[
		every head row/.style={
			before row={
				\toprule & \multicolumn{3}{c}{Test loss}\\
			},
			after row=\midrule
		},
		col sep=comma,
		columns/architecture/.style={string type, column name={Architecture}},
		columns/test_loss_sigmoid/.style={column name={Sigmoid}},
		columns/test_loss_tanh/.style={column name={Tanh}},
		columns/test_loss_elu/.style={column name={ELU}}
	] {../data-wave/sine_gordon/test_loss_summary.csv}

	\caption{\todo{Sine Gordon}}
\end{table}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\begin{groupplot}[
			group style={
				group size=1 by 2,
				vertical sep=2.2cm,
			},
			no markers,
			width=0.9*\linewidth, height=\axisdefaultheight,
			title={Displacement $q(t=9,x)$ (Sine Gordon1)},
			xlabel={$x$}, xmin=-25, xmax=25,
			ymin=-1, ymax=10, restrict y to domain=-5:10,
			legend style={
				nodes={scale=0.75, transform shape},
				legend cell align=left,
				%legend pos=outer north east
			},
			legend entries={
				LARGE-CNN,
				CG-SympNet (tanh),
				CG-SympNet (ELU),
				Störmer-Verlet,
			}
		]

		\nextgroupplot[title={Displacement $q(t=3,x)$ (Sine Gordon)}, ylabel={$q(t=3,x)$}]
			\addplot[
				color=green
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/cnn/elu/q_t3.csv};
		
			\addplot[
				color=red
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/q_t3.csv};
		
			\addplot[
				color=blue
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/q_t3.csv};

			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/exact/q_t3.csv};
		
		\nextgroupplot[title={Displacement $q(t=9,x)$ (Sine Gordon)}, ylabel={$q(t=9,x)$}]
			\addplot[
				color=green
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/cnn/elu/q_t9.csv};
		
			\addplot[
				color=red
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/gradient/tanh/q_t9.csv};
		
			\addplot[
				color=blue
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/n1-gradient/elu/q_t9.csv};

			\addplot[
				color=darkgray
			] table[x=x,y=q,col sep=comma] {../data-wave/sine_gordon/exact/q_t9.csv};
			
		\end{groupplot}
	\end{tikzpicture}
	\caption{\todo{Sine Gordon}}
\end{figure}

%----------------------------------------------------------------------------------------
%
% Resume
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\section{R\'esum\'e}
\subsection{Summary and conclusion}

\todo{normalization improves performance in a lot of cases}

\todo{normalization leads to better stability}

\todo{small training data}

\todo{preserve total energy}

\todo{better long-time prediction performance}

\subsection{Outlook}\label{sec_outlook}

\todo{Simple pendulum, rotating case}

\todo{Recurrent networks}

\todo{Convolution: non-equidistant grids, 2D, 3D}

%----------------------------------------------------------------------------------------
%
% Appendices
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\begin{appendices}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}

\section{Declaration of authorship}

\vspace{3cm}

\begin{table}[h!]
\centering
\begin{tabular}{|p{13cm}|}
\hline\\
	I hereby certify
	\begin{enumerate}
		\item that this thesis has been composed by me and is based on my own work, unless stated otherwise,
		\item that all direct or indirect sources used are acknowledged as references and all extracts from work of others, either verbatim or in spirit, are stated as such,
		\item that neither the thesis itself nor parts of this thesis have been part of another examination procedure,
		\item that neither the thesis itself nor parts of this thesis have been published and
		\item that all copies of this thesis, either digital or printed, coincide.
	\end{enumerate}
	Therewith, this declaration of authorship is in accordance with the examination regulations from 22th July 2016 of the bachelor's program \emph{Simulation Technology} of the University of Stuttgart.\\\\
\hline
\end{tabular}
\end{table}

\vspace{4cm}
\hrulefill\\
Name
\hspace{7cm}
Date, City, Signature
\end{appendices}
%----------------------------------------------------------------------------------------
%
% BIBLIOGRAPHY
%
%----------------------------------------------------------------------------------------
\clearpage\newpage\null %empty page
\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\addcontentsline{toc}{section}{References}
\bibliographystyle{abbrvnat}
\bibliography{../../literature/references.bib}

\end{document}
