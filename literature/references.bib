% Encoding: UTF-8

@Unpublished{Unterthiner2016,
  author   = {Thomas Unterthiner, Sepp Hochreiter},
  note     = {ICLR 2016 workshop submission},
  title    = {Understanding Very Deep Networks via Volume Conservation},
  year     = {2016},
  comment  = {Paper which explains the relation to ResNets},
  file     = {:Understanding_Very_Deep_Networks_via_Volume_Conservation_Unterthiner_Hochreiter.pdf:PDF},
  groups   = {Hamiltonian ML, Volume-Preserving NNs, Symplectic Nets},
  keywords = {rank5},
  ranking  = {rank5},
}

@Article{Deco1995,
  author   = {Gustavo Deco and Wilfried Brauer},
  journal  = {Neural Networks},
  title    = {Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures},
  year     = {1995},
  issn     = {0893-6080},
  number   = {4},
  pages    = {525 - 535},
  volume   = {8},
  abstract = {A neural network learning paradigm based on information theory is proposed as a way to perform, in an unsupervised fashion, redundancy reduction among the elements of the output layer without loss of information from the sensory input. The model developed performs nonlinear decorrelation up to higher orders of the cumulant tensors and results in probabilistically independent components of the output layer. This means that we don't need to assume Gaussian distribution at either the input or the output. The theory presented is related to the unsupervised learning theory of Barlow, which proposes redundancy reduction as the goal of cognition. When nonlinear units are used (sigmoid or higher-order pi-neurons), nonlinear principal component analysis is obtained. In this case, nonlinear manifolds can be reduced to minimum dimension manifolds. If such units are used the network performs a generalized principal component analysis in the sense that non-Gaussian distributions can be linearly decorrelated and higher orders of the correlation tensors are also taken into account. The basic structure of the architecture involves a general transformation that is volume conserving and therefore the entropy, yielding a map without loss of information. Minimization of the mutual information among the output neurons eliminates the redundancy between the outputs and results in statistical decorrelation of the extracted features. This is known as factorial learning. To sum up, this paper presents a model of factorial learning for general nonlinear transformations of an arbitrary non-Gaussian (or Gaussian) environment with statistically nonlinearly correlated input. Simulations demonstrate the effectiveness of this method.},
  comment  = {Original source of the symplectic ANN structure based on the block matrix [[I, W],[0, I]] with W symmetric.
Interesting equations: (7),(8),(9)},
  doi      = {https://doi.org/10.1016/0893-6080(94)00108-X},
  file     = {:Nonlinear_higher-order_statistical_decorrelation_by_volume-conserving_neural_architectures_Deco_Brauer.pdf:PDF},
  groups   = {Volume-Preserving NNs, Symplectic Decorrelation, Symplectic Nets},
  keywords = {Nonlinear decorrelation, Volume-conserving architectures, Factorial learning},
  url      = {http://www.sciencedirect.com/science/article/pii/089360809400108X},
}

@Article{Jin2020,
  author   = {Pengzhan Jin and Zhen Zhang and Aiqing Zhu and Yifa Tang and George Em Karniadakis},
  journal  = {Neural Networks},
  title    = {SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems},
  year     = {2020},
  issn     = {0893-6080},
  pages    = {166 - 179},
  volume   = {132},
  abstract = {We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions. We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models, and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.017},
  keywords = {Deep learning, Physics-informed, Dynamical systems, Hamiltonian systems, Symplectic maps, Symplectic integrators},
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608020303063},
}

@Article{jin2020unit,
  author        = {{Jin}, Pengzhan and {Tang}, Yifa and {Zhu}, Aiqing},
  journal       = {arXiv e-prints},
  title         = {{Unit triangular factorization of the matrix symplectic group}},
  year          = {2019},
  month         = dec,
  pages         = {arXiv:1912.10926},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191210926J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1912.10926},
  eprint        = {1912.10926},
  keywords      = {Mathematics - Symplectic Geometry, 15A23, 15A24, 15B57, 65F15, 65F40},
  primaryclass  = {math.SG},
}

@InProceedings{batchnorm-ioffe15,
  author    = {Sergey Ioffe and Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Francis Bach and David Blei},
  month     = {07--09 Jul},
  pages     = {448--456},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layerâ€™s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  pdf       = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url       = {http://proceedings.mlr.press/v37/ioffe15.html},
}

@Book{hairer2006,
  author    = {Ernst Hairer and Christian Lubich and Gerhard Wanner},
  publisher = {Springer-Verlag},
  title     = {Geometric Numerical Integration},
  year      = {2006},
  doi       = {10.1007/3-540-30666-8},
  url       = {https://doi.org/10.1007%2F3-540-30666-8},
}

@Article{Peng2016,
  author    = {Liqian Peng and Kamran Mohseni},
  journal   = {{SIAM} Journal on Scientific Computing},
  title     = {Symplectic Model Reduction of Hamiltonian Systems},
  year      = {2016},
  month     = {jan},
  number    = {1},
  pages     = {A1--A27},
  volume    = {38},
  doi       = {10.1137/140978922},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Book{leimkuhler_reich_2005,
  author     = {Leimkuhler, Benedict and Reich, Sebastian},
  publisher  = {Cambridge University Press},
  title      = {Simulating Hamiltonian Dynamics},
  year       = {2005},
  series     = {Cambridge Monographs on Applied and Computational Mathematics},
  collection = {Cambridge Monographs on Applied and Computational Mathematics},
  doi        = {10.1017/CBO9780511614118},
  place      = {Cambridge},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:.;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Symplectic Nets\;0\;1\;\;\;\;;
}
