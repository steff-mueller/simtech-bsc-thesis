% Encoding: UTF-8

@Unpublished{Unterthiner2016,
  author   = {Thomas Unterthiner, Sepp Hochreiter},
  note     = {ICLR 2016 workshop submission},
  title    = {Understanding Very Deep Networks via Volume Conservation},
  year     = {2016},
  comment  = {Paper which explains the relation to ResNets},
  file     = {:Understanding_Very_Deep_Networks_via_Volume_Conservation_Unterthiner_Hochreiter.pdf:PDF},
  groups   = {Hamiltonian ML, Volume-Preserving NNs, Symplectic Nets},
  keywords = {rank5},
  ranking  = {rank5},
}

@Article{Deco1995,
  author   = {Gustavo Deco and Wilfried Brauer},
  journal  = {Neural Networks},
  title    = {Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures},
  year     = {1995},
  issn     = {0893-6080},
  number   = {4},
  pages    = {525 - 535},
  volume   = {8},
  abstract = {A neural network learning paradigm based on information theory is proposed as a way to perform, in an unsupervised fashion, redundancy reduction among the elements of the output layer without loss of information from the sensory input. The model developed performs nonlinear decorrelation up to higher orders of the cumulant tensors and results in probabilistically independent components of the output layer. This means that we don't need to assume Gaussian distribution at either the input or the output. The theory presented is related to the unsupervised learning theory of Barlow, which proposes redundancy reduction as the goal of cognition. When nonlinear units are used (sigmoid or higher-order pi-neurons), nonlinear principal component analysis is obtained. In this case, nonlinear manifolds can be reduced to minimum dimension manifolds. If such units are used the network performs a generalized principal component analysis in the sense that non-Gaussian distributions can be linearly decorrelated and higher orders of the correlation tensors are also taken into account. The basic structure of the architecture involves a general transformation that is volume conserving and therefore the entropy, yielding a map without loss of information. Minimization of the mutual information among the output neurons eliminates the redundancy between the outputs and results in statistical decorrelation of the extracted features. This is known as factorial learning. To sum up, this paper presents a model of factorial learning for general nonlinear transformations of an arbitrary non-Gaussian (or Gaussian) environment with statistically nonlinearly correlated input. Simulations demonstrate the effectiveness of this method.},
  comment  = {Original source of the symplectic ANN structure based on the block matrix [[I, W],[0, I]] with W symmetric.
Interesting equations: (7),(8),(9)},
  doi      = {https://doi.org/10.1016/0893-6080(94)00108-X},
  file     = {:Nonlinear_higher-order_statistical_decorrelation_by_volume-conserving_neural_architectures_Deco_Brauer.pdf:PDF},
  groups   = {Volume-Preserving NNs, Symplectic Decorrelation, Symplectic Nets},
  keywords = {Nonlinear decorrelation, Volume-conserving architectures, Factorial learning},
  url      = {http://www.sciencedirect.com/science/article/pii/089360809400108X},
}

@Article{Jin2020,
  author   = {Pengzhan Jin and Zhen Zhang and Aiqing Zhu and Yifa Tang and George Em Karniadakis},
  journal  = {Neural Networks},
  title    = {SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems},
  year     = {2020},
  issn     = {0893-6080},
  pages    = {166 - 179},
  volume   = {132},
  abstract = {We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions. We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models, and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.017},
  keywords = {Deep learning, Physics-informed, Dynamical systems, Hamiltonian systems, Symplectic maps, Symplectic integrators},
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608020303063},
}

@Article{jin2020unit,
  author        = {{Jin}, Pengzhan and {Tang}, Yifa and {Zhu}, Aiqing},
  journal       = {arXiv e-prints},
  title         = {{Unit triangular factorization of the matrix symplectic group}},
  year          = {2019},
  month         = dec,
  pages         = {arXiv:1912.10926},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191210926J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1912.10926},
  eprint        = {1912.10926},
  keywords      = {Mathematics - Symplectic Geometry, 15A23, 15A24, 15B57, 65F15, 65F40},
  primaryclass  = {math.SG},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:.;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Symplectic Nets\;0\;1\;\;\;\;;
}
