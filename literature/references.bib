% Encoding: UTF-8

@Unpublished{Unterthiner2016,
  author   = {Thomas Unterthiner, Sepp Hochreiter},
  note     = {ICLR 2016 workshop submission},
  title    = {Understanding Very Deep Networks via Volume Conservation},
  year     = {2016},
  comment  = {Paper which explains the relation to ResNets},
  file     = {:Understanding_Very_Deep_Networks_via_Volume_Conservation_Unterthiner_Hochreiter.pdf:PDF},
  groups   = {Hamiltonian ML, Volume-Preserving NNs, Symplectic Nets},
  keywords = {rank5},
  ranking  = {rank5},
}

@Article{Deco1995,
  author   = {Gustavo Deco and Wilfried Brauer},
  journal  = {Neural Networks},
  title    = {Nonlinear higher-order statistical decorrelation by volume-conserving neural architectures},
  year     = {1995},
  issn     = {0893-6080},
  number   = {4},
  pages    = {525 - 535},
  volume   = {8},
  abstract = {A neural network learning paradigm based on information theory is proposed as a way to perform, in an unsupervised fashion, redundancy reduction among the elements of the output layer without loss of information from the sensory input. The model developed performs nonlinear decorrelation up to higher orders of the cumulant tensors and results in probabilistically independent components of the output layer. This means that we don't need to assume Gaussian distribution at either the input or the output. The theory presented is related to the unsupervised learning theory of Barlow, which proposes redundancy reduction as the goal of cognition. When nonlinear units are used (sigmoid or higher-order pi-neurons), nonlinear principal component analysis is obtained. In this case, nonlinear manifolds can be reduced to minimum dimension manifolds. If such units are used the network performs a generalized principal component analysis in the sense that non-Gaussian distributions can be linearly decorrelated and higher orders of the correlation tensors are also taken into account. The basic structure of the architecture involves a general transformation that is volume conserving and therefore the entropy, yielding a map without loss of information. Minimization of the mutual information among the output neurons eliminates the redundancy between the outputs and results in statistical decorrelation of the extracted features. This is known as factorial learning. To sum up, this paper presents a model of factorial learning for general nonlinear transformations of an arbitrary non-Gaussian (or Gaussian) environment with statistically nonlinearly correlated input. Simulations demonstrate the effectiveness of this method.},
  comment  = {Original source of the symplectic ANN structure based on the block matrix [[I, W],[0, I]] with W symmetric.
Interesting equations: (7),(8),(9)},
  doi      = {https://doi.org/10.1016/0893-6080(94)00108-X},
  file     = {:Nonlinear_higher-order_statistical_decorrelation_by_volume-conserving_neural_architectures_Deco_Brauer.pdf:PDF},
  groups   = {Volume-Preserving NNs, Symplectic Decorrelation, Symplectic Nets},
  keywords = {Nonlinear decorrelation, Volume-conserving architectures, Factorial learning},
  url      = {http://www.sciencedirect.com/science/article/pii/089360809400108X},
}

@Article{Jin2020,
  author   = {Pengzhan Jin and Zhen Zhang and Aiqing Zhu and Yifa Tang and George Em Karniadakis},
  journal  = {Neural Networks},
  title    = "{SympNets: Intrinsic structure-preserving symplectic networks for identifying Hamiltonian systems}",
  year     = {2020},
  issn     = {0893-6080},
  pages    = {166 - 179},
  volume   = {132},
  abstract = {We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions. We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models, and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.017},
  keywords = {Deep learning, Physics-informed, Dynamical systems, Hamiltonian systems, Symplectic maps, Symplectic integrators},
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608020303063},
}

@Article{jin2020unit,
  author        = {{Jin}, Pengzhan and {Tang}, Yifa and {Zhu}, Aiqing},
  journal       = {arXiv e-prints},
  title         = {{Unit triangular factorization of the matrix symplectic group}},
  year          = {2019},
  month         = dec,
  pages         = {arXiv:1912.10926},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191210926J},
  archiveprefix = {arXiv},
  eid           = {arXiv:1912.10926},
  eprint        = {1912.10926},
  keywords      = {Mathematics - Symplectic Geometry, 15A23, 15A24, 15B57, 65F15, 65F40},
  primaryclass  = {math.SG},
}

@InProceedings{batchnorm-ioffe15,
  author    = {Sergey Ioffe and Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  year      = {2015},
  address   = {Lille, France},
  editor    = {Francis Bach and David Blei},
  month     = {07--09 Jul},
  pages     = {448--456},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {37},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layerâ€™s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  pdf       = {http://proceedings.mlr.press/v37/ioffe15.pdf},
  url       = {http://proceedings.mlr.press/v37/ioffe15.html},
}

@Book{hairer2006,
  author    = {Ernst Hairer and Christian Lubich and Gerhard Wanner},
  publisher = {Springer-Verlag},
  title     = {Geometric Numerical Integration},
  year      = {2006},
  doi       = {10.1007/3-540-30666-8},
  url       = {https://doi.org/10.1007%2F3-540-30666-8},
}

@Article{Peng2016,
  author    = {Liqian Peng and Kamran Mohseni},
  journal   = {{SIAM} Journal on Scientific Computing},
  title     = "{Symplectic model reduction of Hamiltonian systems}",
  year      = {2016},
  month     = {jan},
  number    = {1},
  pages     = {A1--A27},
  volume    = {38},
  doi       = {10.1137/140978922},
  publisher = {Society for Industrial {\&} Applied Mathematics ({SIAM})},
}

@Book{leimkuhler_reich_2005,
  author     = {Leimkuhler, Benedict and Reich, Sebastian},
  publisher  = {Cambridge University Press},
  title      = {Simulating Hamiltonian Dynamics},
  year       = {2005},
  series     = {Cambridge Monographs on Applied and Computational Mathematics},
  collection = {Cambridge Monographs on Applied and Computational Mathematics},
  doi        = {10.1017/CBO9780511614118},
  place      = {Cambridge},
}

@InProceedings{resnet2016,
  author    = {K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
  booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2016},
  pages     = {770-778},
  doi       = {10.1109/CVPR.2016.90},
}

@Book{Goodfellow2016,
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher = {The MIT Press},
  title     = {Deep Learning},
  year      = {2016},
  isbn      = {0262035618},
  abstract  = {"Written by three experts in the field, Deep Learning is the only comprehensive book on the subject." -- Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.},
}

@Article{2006ham_pde,
  author  = {Bridges, Thomas},
  journal = {Journal of Physics A: Mathematical and General},
  title   = "{Numerical methods for Hamiltonian PDEs}",
  year    = {2006},
  month   = {04},
  pages   = {5287},
  volume  = {39},
  doi     = {10.1088/0305-4470/39/19/S02},
}

@InProceedings{amsgrad2018,
  author    = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  title     = "{On the convergence of Adam and beyond}",
  year      = {2018},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/ReddiKK18.bib},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200},
  url       = {https://openreview.net/forum?id=ryQu7f-RZ},
}

@Article{Ziyin2020,
       author = {{Ziyin}, Liu and {Hartwig}, Tilman and {Ueda}, Masahito},
        title = "{Neural Networks Fail to Learn Periodic Functions and How to Fix It}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2020,
        month = jun,
          eid = {arXiv:2006.08195},
        pages = {arXiv:2006.08195},
archivePrefix = {arXiv},
       eprint = {2006.08195},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200608195Z},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: fileDirectory:.;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Symplectic Nets\;0\;1\;\;\;\;;
}
